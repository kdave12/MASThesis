{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e947b7",
   "metadata": {},
   "source": [
    "# MAS Thesis Notebook\n",
    "## Krishna Dave (UID: 905636874)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a16f151",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook contains example code for privacy auditing using the TAPAS toolbox of synthetic data generated from the Breast Cancer Wisconsin Diagnostic Data (https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic). This toolbox is used for evaluating the privacy of synthetic data using adversarial techniques. The code for the TAPAS toolbox is from: https://github.com/alan-turing-institute/tapas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd24a565",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "1. Install poetry (system-wide) from https://python-poetry.org/docs/ \n",
    "2. It can also be installed using pip: `pip install git+https://github.com/alan-turing-institute/privacy-sdg-toolbox`\n",
    "3. Activate virtual environment inside the Jupyter notebook using `poetry shell`\n",
    "4. Add the virtual environment to the available kernels for the notebook.\n",
    "5. Make sure that this notebook is located in the correct directory inside the wider directory to ensure the correctness of the relative imports and file paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b157cb5",
   "metadata": {},
   "source": [
    "### Setup: Installation of other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5309b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.python.org/simple, https://pypi.apple.com/simple\n",
      "Requirement already satisfied: be-great in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (0.0.7)\n",
      "Requirement already satisfied: datasets>=2.5.2 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from be-great) (2.16.0)\n",
      "Requirement already satisfied: numpy>=1.23.1 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from be-great) (1.23.4)\n",
      "Requirement already satisfied: pandas>=1.4.4 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from be-great) (1.5.0)\n",
      "Requirement already satisfied: scikit-learn>=1.1.1 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from be-great) (1.1.2)\n",
      "Requirement already satisfied: torch>=1.10.2 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from be-great) (1.12.1)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from be-great) (4.64.1)\n",
      "Requirement already satisfied: transformers>=4.22.1 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from be-great) (4.36.2)\n",
      "Requirement already satisfied: accelerate>=0.20.1 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from be-great) (0.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from accelerate>=0.20.1->be-great) (21.3)\n",
      "Requirement already satisfied: psutil in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from accelerate>=0.20.1->be-great) (5.9.2)\n",
      "Requirement already satisfied: pyyaml in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from accelerate>=0.20.1->be-great) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from accelerate>=0.20.1->be-great) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from accelerate>=0.20.1->be-great) (0.4.1)\n",
      "Requirement already satisfied: filelock in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from datasets>=2.5.2->be-great) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from datasets>=2.5.2->be-great) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from datasets>=2.5.2->be-great) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from datasets>=2.5.2->be-great) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from datasets>=2.5.2->be-great) (2.28.1)\n",
      "Requirement already satisfied: xxhash in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from datasets>=2.5.2->be-great) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from datasets>=2.5.2->be-great) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets>=2.5.2->be-great) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from datasets>=2.5.2->be-great) (3.9.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from pandas>=1.4.4->be-great) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from pandas>=1.4.4->be-great) (2022.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from scikit-learn>=1.1.1->be-great) (1.9.2)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from scikit-learn>=1.1.1->be-great) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from scikit-learn>=1.1.1->be-great) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from torch>=1.10.2->be-great) (4.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from transformers>=4.22.1->be-great) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from transformers>=4.22.1->be-great) (0.15.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from aiohttp->datasets>=2.5.2->be-great) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from aiohttp->datasets>=2.5.2->be-great) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from aiohttp->datasets>=2.5.2->be-great) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from aiohttp->datasets>=2.5.2->be-great) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from aiohttp->datasets>=2.5.2->be-great) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from aiohttp->datasets>=2.5.2->be-great) (4.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from packaging>=20.0->accelerate>=0.20.1->be-great) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.4.4->be-great) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=2.5.2->be-great) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=2.5.2->be-great) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=2.5.2->be-great) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=2.5.2->be-great) (2022.9.24)\n"
     ]
    }
   ],
   "source": [
    "!pip install be-great\n",
    "!pip install --upgrade pip --quiet\n",
    "!pip install --upgrade ctgan --quiet\n",
    "!pip install sdv opacus autodp smartnoise-synth --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e5e53d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.python.org/simple, https://pypi.apple.com/simple\n",
      "Collecting openai\n",
      "  Downloading openai-1.6.1-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from openai) (3.6.2)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.5.3-py3-none-any.whl.metadata (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from openai) (4.64.1)\n",
      "Collecting typing-extensions<5,>=4.7 (from openai)\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2022.9.24)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.6 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.14.6-cp39-cp39-macosx_10_7_x86_64.whl.metadata (6.5 kB)\n",
      "Downloading openai-1.6.1-py3-none-any.whl (225 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.14.6-cp39-cp39-macosx_10_7_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: typing-extensions, h11, distro, annotated-types, pydantic-core, httpcore, pydantic, httpx, openai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "Successfully installed annotated-types-0.6.0 distro-1.9.0 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 openai-1.6.1 pydantic-2.5.3 pydantic-core-2.14.6 typing-extensions-4.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f823005a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                       Version     Editable project location\n",
      "----------------------------- ----------- ----------------------------------------------------------------------------------------------------------------\n",
      "absl-py                       1.3.0\n",
      "accelerate                    0.25.0\n",
      "aiohttp                       3.9.1\n",
      "aiosignal                     1.3.1\n",
      "alabaster                     0.7.12\n",
      "altgraph                      0.17.3\n",
      "annotated-types               0.6.0\n",
      "antlr4-python3-runtime        4.9.3\n",
      "anyio                         3.6.2\n",
      "appnope                       0.1.3\n",
      "argon2-cffi                   21.3.0\n",
      "argon2-cffi-bindings          21.2.0\n",
      "asttokens                     2.2.0\n",
      "astunparse                    1.6.3\n",
      "async-timeout                 4.0.3\n",
      "attrs                         22.1.0\n",
      "autodp                        0.2.3.1\n",
      "Babel                         2.10.3\n",
      "backcall                      0.2.0\n",
      "be-great                      0.0.7\n",
      "beautifulsoup4                4.11.1\n",
      "black                         22.10.0\n",
      "bleach                        5.0.1\n",
      "boto3                         1.34.2\n",
      "botocore                      1.34.2\n",
      "cachetools                    5.2.0\n",
      "certifi                       2022.9.24\n",
      "cffi                          1.15.1\n",
      "charset-normalizer            2.1.1\n",
      "click                         8.1.3\n",
      "cloudpickle                   2.2.0\n",
      "contourpy                     1.0.5\n",
      "copulas                       0.9.2\n",
      "ctgan                         0.8.0\n",
      "cycler                        0.11.0\n",
      "Cython                        0.29.32\n",
      "datasets                      2.16.0\n",
      "DataSynthesizer               0.1.11\n",
      "debugpy                       1.6.3\n",
      "decorator                     5.1.1\n",
      "deepecho                      0.5.0\n",
      "defusedxml                    0.7.1\n",
      "dill                          0.3.7\n",
      "disjoint-set                  0.7.3\n",
      "distro                        1.9.0\n",
      "docutils                      0.17.1\n",
      "ektelo                        0.1.0\n",
      "entrypoints                   0.4\n",
      "executing                     1.2.0\n",
      "Faker                         19.13.0\n",
      "fastjsonschema                2.16.2\n",
      "filelock                      3.13.1\n",
      "flake8                        4.0.1\n",
      "flatbuffers                   22.9.24\n",
      "fonttools                     4.37.4\n",
      "frozenlist                    1.4.1\n",
      "fsspec                        2023.10.0\n",
      "future                        0.18.2\n",
      "gast                          0.4.0\n",
      "google-auth                   2.12.0\n",
      "google-auth-oauthlib          0.4.6\n",
      "google-pasta                  0.2.0\n",
      "graphviz                      0.17\n",
      "greenlet                      3.0.2\n",
      "grpcio                        1.49.1\n",
      "h11                           0.14.0\n",
      "h5py                          3.7.0\n",
      "httpcore                      1.0.2\n",
      "httpx                         0.26.0\n",
      "huggingface-hub               0.20.1\n",
      "idna                          3.4\n",
      "imagesize                     1.4.1\n",
      "importlib-metadata            5.0.0\n",
      "iniconfig                     1.1.1\n",
      "ipykernel                     6.16.0\n",
      "ipython                       8.7.0\n",
      "ipython-genutils              0.2.0\n",
      "isort                         5.10.1\n",
      "jedi                          0.18.1\n",
      "Jinja2                        3.1.2\n",
      "jmespath                      1.0.1\n",
      "joblib                        1.2.0\n",
      "json5                         0.9.10\n",
      "jsonschema                    4.17.3\n",
      "jupyter_client                7.4.2\n",
      "jupyter-core                  4.11.1\n",
      "jupyter-server                1.23.3\n",
      "jupyterlab                    3.5.0\n",
      "jupyterlab-pygments           0.2.2\n",
      "jupyterlab_server             2.16.3\n",
      "keras                         2.10.0\n",
      "Keras-Preprocessing           1.1.2\n",
      "kiwisolver                    1.4.4\n",
      "libclang                      14.0.6\n",
      "macholib                      1.16.2\n",
      "Markdown                      3.4.1\n",
      "MarkupSafe                    2.1.1\n",
      "matplotlib                    3.6.1\n",
      "matplotlib-inline             0.1.6\n",
      "mccabe                        0.6.1\n",
      "mistune                       2.0.4\n",
      "multidict                     6.0.4\n",
      "multiprocess                  0.70.15\n",
      "mypy-extensions               0.4.3\n",
      "nbclassic                     0.4.8\n",
      "nbclient                      0.5.13\n",
      "nbconvert                     7.2.5\n",
      "nbformat                      5.7.0\n",
      "nest-asyncio                  1.5.6\n",
      "networkx                      2.8.7\n",
      "nose                          1.3.7\n",
      "notebook                      6.5.2\n",
      "notebook_shim                 0.2.2\n",
      "numpy                         1.23.4\n",
      "oauthlib                      3.2.1\n",
      "opacus                        0.14.0\n",
      "openai                        1.6.1\n",
      "opendp                        0.6.2\n",
      "opt-einsum                    3.3.0\n",
      "pac-synth                     0.0.6\n",
      "packaging                     21.3\n",
      "palettable                    3.3.0\n",
      "pandas                        1.5.0\n",
      "pandasql                      0.7.3\n",
      "pandocfilters                 1.5.0\n",
      "parso                         0.8.3\n",
      "pathspec                      0.10.1\n",
      "pexpect                       4.8.0\n",
      "pickleshare                   0.7.5\n",
      "Pillow                        9.2.0\n",
      "pip                           23.3.2\n",
      "platformdirs                  2.5.2\n",
      "plotly                        5.18.0\n",
      "pluggy                        1.0.0\n",
      "private-pgm                   0.0.1\n",
      "prometheus-client             0.15.0\n",
      "prompt-toolkit                3.0.31\n",
      "protobuf                      3.19.6\n",
      "psutil                        5.9.2\n",
      "psycopg2-binary               2.9.4\n",
      "ptyprocess                    0.7.0\n",
      "pure-eval                     0.2.2\n",
      "py                            1.11.0\n",
      "py-synthpop                   0.1.2\n",
      "pyarrow                       14.0.2\n",
      "pyarrow-hotfix                0.6\n",
      "pyasn1                        0.4.8\n",
      "pyasn1-modules                0.2.8\n",
      "pycodestyle                   2.8.0\n",
      "pycparser                     2.21\n",
      "pydantic                      2.5.3\n",
      "pydantic_core                 2.14.6\n",
      "pyflakes                      2.4.0\n",
      "Pygments                      2.13.0\n",
      "pyinstaller                   5.5\n",
      "pyinstaller-hooks-contrib     2022.10\n",
      "pyparsing                     3.0.9\n",
      "pyrsistent                    0.19.2\n",
      "pytest                        7.1.3\n",
      "python-dateutil               2.8.2\n",
      "pytz                          2022.4\n",
      "PyYAML                        5.4.1\n",
      "pyzmq                         24.0.1\n",
      "rdt                           1.9.0\n",
      "regex                         2023.10.3\n",
      "reprosyn                      0.1.0\n",
      "requests                      2.28.1\n",
      "requests-oauthlib             1.3.1\n",
      "rsa                           4.9\n",
      "s3transfer                    0.9.0\n",
      "safetensors                   0.4.1\n",
      "scikit-learn                  1.1.2\n",
      "scipy                         1.9.2\n",
      "sdmetrics                     0.13.0\n",
      "sdv                           1.8.0\n",
      "seaborn                       0.11.2\n",
      "Send2Trash                    1.8.0\n",
      "setuptools                    65.5.0\n",
      "setuptools-scm                7.0.5\n",
      "six                           1.16.0\n",
      "sklearn                       0.0\n",
      "smartnoise-sql                0.2.12\n",
      "smartnoise-synth              0.3.3\n",
      "sniffio                       1.3.0\n",
      "snowballstemmer               2.2.0\n",
      "soupsieve                     2.3.2.post1\n",
      "Sphinx                        4.5.0\n",
      "sphinx-rtd-theme              1.0.0\n",
      "sphinxcontrib-applehelp       1.0.2\n",
      "sphinxcontrib-devhelp         1.0.2\n",
      "sphinxcontrib-htmlhelp        2.0.0\n",
      "sphinxcontrib-jsmath          1.0.1\n",
      "sphinxcontrib-qthelp          1.0.3\n",
      "sphinxcontrib-serializinghtml 1.1.5\n",
      "spyder-kernels                2.2.0\n",
      "SQLAlchemy                    1.4.50\n",
      "stack-data                    0.6.2\n",
      "tapas                         1.0.0       /Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main\n",
      "tenacity                      8.2.3\n",
      "tensorboard                   2.10.1\n",
      "tensorboard-data-server       0.6.1\n",
      "tensorboard-plugin-wit        1.8.1\n",
      "tensorflow                    2.10.0\n",
      "tensorflow-estimator          2.10.0\n",
      "tensorflow-io-gcs-filesystem  0.27.0\n",
      "termcolor                     2.0.1\n",
      "terminado                     0.17.0\n",
      "threadpoolctl                 3.1.0\n",
      "tinycss2                      1.2.1\n",
      "tokenizers                    0.15.0\n",
      "tomli                         2.0.1\n",
      "torch                         1.12.1\n",
      "torchvision                   0.13.1\n",
      "tornado                       6.2\n",
      "tqdm                          4.64.1\n",
      "traitlets                     5.4.0\n",
      "transformers                  4.36.2\n",
      "typing_extensions             4.9.0\n",
      "urllib3                       1.26.12\n",
      "validators                    0.20.0\n",
      "wcwidth                       0.2.5\n",
      "webencodings                  0.5.1\n",
      "websocket-client              1.4.2\n",
      "Werkzeug                      2.2.2\n",
      "wheel                         0.37.1\n",
      "wrapt                         1.14.1\n",
      "wurlitzer                     3.0.2\n",
      "xxhash                        3.4.1\n",
      "yarl                          1.9.4\n",
      "zipp                          3.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee6109d",
   "metadata": {},
   "source": [
    "## Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa06868a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/mbi/__init__.py:15: UserWarning: MixtureInference disabled, please install jax and jaxlib\n",
      "  warnings.warn('MixtureInference disabled, please install jax and jaxlib')\n",
      "2023-12-27 17:24:17.514818: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Importing standard packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import itertools\n",
    "from enum import Enum\n",
    "import os\n",
    "\n",
    "# Importing from tapas\n",
    "import tapas.datasets\n",
    "import tapas.generators\n",
    "import tapas.attacks\n",
    "import tapas.threat_models\n",
    "import tapas.report\n",
    "from tapas.generators import Generator, ReprosynGenerator, Raw\n",
    "import tqdm\n",
    "from modules.myctgan import CTGAN\n",
    "from modules.myctgan import DPCTGAN\n",
    "from modules.myctgan import PATEGAN\n",
    "\n",
    "# Importing from sklearn\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Importing from mbi and be_great\n",
    "from mbi import Dataset, Domain\n",
    "from be_great import GReaT\n",
    "\n",
    "# Importing openai to call GPT4 API from OpenAI\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc3b619",
   "metadata": {},
   "source": [
    "## Using the GReaT framework API\n",
    "### Description\n",
    "Generates Realistic Synthetic Tabular data with pretrained Transformer-based language models. Here is an example that uses their API to generate synthetic data for the breast cancer dataset.\n",
    "\n",
    "### Debugging Tips/Links\n",
    "LLM model in the GReaT API has to be run on a GPU, it doesn't work on a Mac CPU. \n",
    "\n",
    "Otherwise, you will encounter the following errors: \n",
    "`RuntimeError: CUDA Out of memory` \n",
    "`RuntimeError: CUDA error: device-side assert triggered\n",
    "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
    "For debugging consider passing CUDA_LAUNCH_BLOCKING=1`\n",
    "\n",
    "To solve this, run this on Google Colab, navigate to Edit > Notebook settings > select Runtime type to be Python 3 > Hardware accelerator to T4 GPU or paid A100 GPU/V100 GPU.\n",
    "\n",
    "For the error: \n",
    "`An error has occurred: Breaking the generation loop!`\n",
    "`To address this issue, consider fine-tuning the GReaT model for an longer period. This can be achieved by increasing the number of epochs.\n",
    "Alternatively, you might consider increasing the max_length parameter within the sample function. For example: model.sample(n_samples=10, max_length=2000)\n",
    "If the problem persists despite these adjustments, feel free to raise an issue on our GitHub page at: https://github.com/kathrinse/be_great/issues`\n",
    "\n",
    "To solve this error, try passing in `fp16=true` in the model definition, increasing the `max_length` value and increasing the number of `epochs`.\n",
    "\n",
    "Here are some links that helped me with debugging: \n",
    "- https://stackoverflow.com/questions/68166721/pytorch-fails-with-cuda-error-device-side-assert-triggered-on-colab\n",
    "- https://discuss.pytorch.org/t/cuda-error-device-side-assert-triggered-cuda-kernel-errors-might-be-asynchronously-reported-at-some-other-api-call-so-the-stacktrace-below-might-be-incorrect-for-debugging-consider-passing-cuda-launch-blocking-1/160825/5\n",
    "- https://github.com/pytorch/pytorch/issues/75534\n",
    "- https://stackoverflow.com/questions/70340812/how-to-install-pytorch-with-cuda-support-with-pip-in-visual-studio\n",
    "- https://medium.com/@snk.nitin/how-to-solve-cuda-out-of-memory-error-850bb247cfb2\n",
    "- https://stackoverflow.com/questions/64589421/packagesnotfounderror-cudatoolkit-11-1-0-when-installing-pytorch\n",
    "- https://github.com/pytorch/pytorch/issues/30664\n",
    "- https://github.com/kathrinse/be_great/issues/42\n",
    "- https://github.com/kathrinse/be_great/issues/40\n",
    "- https://research.google.com/colaboratory/faq.html#gpu-availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "612c577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates synethetic data using the GReaT framework's API\n",
    "def generateSyntheticDataFromGReaT(batchSize: int, epochs: int, nSamples: int):\n",
    "    # Loading the breast cancer dataset as a frame\n",
    "    breastCancerData = load_breast_cancer(as_frame=True).frame\n",
    "    \n",
    "    # Defining an LLM model using the GReaT API\n",
    "    model = GReaT(llm='distilgpt2', batch_size=batchSize, epochs=epochs)\n",
    "    \n",
    "    # Fitting the generated model to the breastCancerDataSet\n",
    "    model.fit(breastCancerData)\n",
    "    \n",
    "    # Sampling the model to generate the synethic data\n",
    "    synthetic_data = model.sample(n_samples=nSamples)\n",
    "    \n",
    "    # Returning the produced and sampled synthetic data\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2f0d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function call to generate the synthetic data using GReaT; need to run this only once to generate the data, takes a while to generate \n",
    "# synthetic_data = generateSyntheticDataFromGReaT(batch_size=32, epochs=100, nSamples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a3c61e",
   "metadata": {},
   "source": [
    "## Using OpenAI's GPT4 to generate synthetic data from real dataset\n",
    "### Description\n",
    "We can use OpenAI's GPT4 to generate synthetic data from real dataset using the API or chat. Following are examples of both.\n",
    "\n",
    "### Useful Links\n",
    "- https://github.com/openai/openai-cookbook\n",
    "- https://github.com/openai/openai-python\n",
    "- https://www.reddit.com/r/OpenAI/comments/161cygf/gpt4_api_access/?rdt=53543\n",
    "- https://platform.openai.com/docs/introduction\n",
    "- https://platform.openai.com/playground\n",
    "- https://platform.openai.com/docs/api-reference\n",
    "- https://platform.openai.com/api-keys\n",
    "- https://platform.openai.com/docs/api-reference/authentication\n",
    "- https://community.openai.com\n",
    "- https://platform.openai.com/usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2765fea4",
   "metadata": {},
   "source": [
    "### GPT4 API Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24f0de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# While you can provide an api_key keyword argument, we recommend using python-dotenv to add OPENAI_API_KEY=\"My API Key\" to your .env file so that your API Key is not stored in source control.\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"), # Unique API Keys for paid GPT4 API usage can be created here to track : https://platform.openai.com/api-keys\n",
    ")\n",
    "\n",
    "# Request parameters that correspond to file uploads can be passed as bytes, a PathLike instance or a tuple of (filename, contents, media type).\n",
    "client.files.create(\n",
    "    file=Path(\"../data/BreastCancerRealDataset.csv\"),\n",
    "    purpose=\"synthetic data creation\",\n",
    ") \n",
    "# #The async client uses the exact same interface. If you pass a PathLike instance, the file contents will be read asynchronously automatically.\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Can you create synthetic data that mimics this real dataset, and output a csv file with that data.\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6719a74a",
   "metadata": {},
   "source": [
    "### ChatGPT 4 Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7244a88",
   "metadata": {},
   "source": [
    "<img src=\"../data/gpt4.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27dc96f",
   "metadata": {},
   "source": [
    "## Data Pre-processing: Labeling the Data Files\n",
    "\n",
    "### Description\n",
    "For data pre-processing, I had to label the raw and the generated data files with column names and indices since those were not already present in the raw data downloaded from the source.\n",
    "\n",
    "The column names for the data files include:\n",
    "1. ID (Patient ID)\n",
    "2. DIAGNOSIS (M: Malignant, B: Benign)\n",
    "3. RADIUS1\n",
    "4. TEXTURE1\n",
    "5. PERIMETER1\n",
    "6. AREA1\n",
    "7. SMOOTHNESS1\n",
    "8. COMPACTNESS1\n",
    "9. CONCAVITY1\n",
    "10. CONCAVE_POINTS1\n",
    "11. SYMMETRY1\n",
    "12. FRACTAL_DIMENSION1\n",
    "13. RADIUS2\n",
    "14. TEXTURE2\n",
    "15. PERIMETER2\n",
    "16. AREA2\n",
    "17. SMOOTHNESS2\n",
    "18. COMPACTNESS2\n",
    "19. CONCAVITY2\n",
    "20. CONCAVE_POINTS2\n",
    "21. SYMMETRY2\n",
    "22. FRACTAL_DIMENSION2\n",
    "23. RADIUS3\n",
    "24. TEXTURE3\n",
    "25. PERIMETER3\n",
    "26. AREA3\n",
    "27. SMOOTHNESS3\n",
    "28. COMPACTNESS3\n",
    "29. CONCAVITY3\n",
    "30. CONCAVE_POINTS3\n",
    "31. SYMMETRY3\n",
    "32. FRACTAL_DIMENSION3\n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus in the dataset:\n",
    "\ta) radius (mean of distances from center to points on the perimeter)\n",
    "\tb) texture (standard deviation of gray-scale values)\n",
    "\tc) perimeter\n",
    "\td) area\n",
    "\te) smoothness (local variation in radius lengths)\n",
    "\tf) compactness (perimeter^2 / area - 1.0)\n",
    "\tg) concavity (severity of concave portions of the contour)\n",
    "\th) concave points (number of concave portions of the contour)\n",
    "\ti) symmetry \n",
    "\tj) fractal dimension (\"coastline approximation\" - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2915ef2",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "### Description\n",
    "Loading data from the input csvPath, assigning it to the appropriate data types and returning a dataframe as a data type dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd89ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads data from the input csvPath, assigns it to appropriate data types and returns a dataframe as data type dictionary\n",
    "def loadBreastCancerData(csvPath: str):\n",
    "    # Reads the csv file using the input csvPath\n",
    "    df = pd.read_csv(csvPath, index_col=None)\n",
    "    \n",
    "    # Stores the column names of the DataFrame df in the variable cols\n",
    "    cols = df.columns\n",
    "    \n",
    "    # Uses dictionary comprehension to set the data type of the columns to data types of str or float64\n",
    "    dtype_dict = {\"ID\": \"str\", **{col: 'str' for col in cols[1:3]}, **{col: 'float64' for col in cols[3:]}}\n",
    "    \n",
    "    # Applies the data type conversions specified in dtype_dict to the DataFrame df\n",
    "    # The astype() function is used to cast the columns of the DataFrame to specified data types\n",
    "    df = df.astype(dtype_dict)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04799544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the synthetic data files\n",
    "breastCancerData_real = \"../data/breastcancer_real_labeled2.csv\"\n",
    "breastCancerData_synthetic = \"../data/breastcancer_synthetic_labeled2.csv\"\n",
    "\n",
    "# Callings loadBreastCancerData to load breast cancer data into a data frame\n",
    "df = loadBreastCancerData(breastCancerData_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cce9069f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DIAGNOSIS</th>\n",
       "      <th>RADIUS1</th>\n",
       "      <th>TEXTURE1</th>\n",
       "      <th>PERIMETER1</th>\n",
       "      <th>AREA1</th>\n",
       "      <th>SMOOTHNESS1</th>\n",
       "      <th>COMPACTNESS1</th>\n",
       "      <th>CONCAVITY1</th>\n",
       "      <th>CONCAVE_POINTS1</th>\n",
       "      <th>...</th>\n",
       "      <th>RADIUS3</th>\n",
       "      <th>TEXTURE3</th>\n",
       "      <th>PERIMETER3</th>\n",
       "      <th>AREA3</th>\n",
       "      <th>SMOOTHNESS3</th>\n",
       "      <th>COMPACTNESS3</th>\n",
       "      <th>CONCAVITY3</th>\n",
       "      <th>CONCAVE_POINTS3</th>\n",
       "      <th>SYMMETRY3</th>\n",
       "      <th>FRACTAL_DIMENSION3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID DIAGNOSIS RADIUS1  TEXTURE1  PERIMETER1   AREA1  SMOOTHNESS1  \\\n",
       "0    842302         M   17.99     10.38      122.80  1001.0      0.11840   \n",
       "1    842517         M   20.57     17.77      132.90  1326.0      0.08474   \n",
       "2  84300903         M   19.69     21.25      130.00  1203.0      0.10960   \n",
       "3  84348301         M   11.42     20.38       77.58   386.1      0.14250   \n",
       "4  84358402         M   20.29     14.34      135.10  1297.0      0.10030   \n",
       "\n",
       "   COMPACTNESS1  CONCAVITY1  CONCAVE_POINTS1  ...  RADIUS3  TEXTURE3  \\\n",
       "0       0.27760      0.3001          0.14710  ...    25.38     17.33   \n",
       "1       0.07864      0.0869          0.07017  ...    24.99     23.41   \n",
       "2       0.15990      0.1974          0.12790  ...    23.57     25.53   \n",
       "3       0.28390      0.2414          0.10520  ...    14.91     26.50   \n",
       "4       0.13280      0.1980          0.10430  ...    22.54     16.67   \n",
       "\n",
       "   PERIMETER3   AREA3  SMOOTHNESS3  COMPACTNESS3  CONCAVITY3  CONCAVE_POINTS3  \\\n",
       "0      184.60  2019.0       0.1622        0.6656      0.7119           0.2654   \n",
       "1      158.80  1956.0       0.1238        0.1866      0.2416           0.1860   \n",
       "2      152.50  1709.0       0.1444        0.4245      0.4504           0.2430   \n",
       "3       98.87   567.7       0.2098        0.8663      0.6869           0.2575   \n",
       "4      152.20  1575.0       0.1374        0.2050      0.4000           0.1625   \n",
       "\n",
       "   SYMMETRY3  FRACTAL_DIMENSION3  \n",
       "0     0.4601             0.11890  \n",
       "1     0.2750             0.08902  \n",
       "2     0.3613             0.08758  \n",
       "3     0.6638             0.17300  \n",
       "4     0.2364             0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79fee7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXTURE1</th>\n",
       "      <th>PERIMETER1</th>\n",
       "      <th>AREA1</th>\n",
       "      <th>SMOOTHNESS1</th>\n",
       "      <th>COMPACTNESS1</th>\n",
       "      <th>CONCAVITY1</th>\n",
       "      <th>CONCAVE_POINTS1</th>\n",
       "      <th>SYMMETRY1</th>\n",
       "      <th>FRACTAL_DIMENSION1</th>\n",
       "      <th>RADIUS2</th>\n",
       "      <th>...</th>\n",
       "      <th>RADIUS3</th>\n",
       "      <th>TEXTURE3</th>\n",
       "      <th>PERIMETER3</th>\n",
       "      <th>AREA3</th>\n",
       "      <th>SMOOTHNESS3</th>\n",
       "      <th>COMPACTNESS3</th>\n",
       "      <th>CONCAVITY3</th>\n",
       "      <th>CONCAVE_POINTS3</th>\n",
       "      <th>SYMMETRY3</th>\n",
       "      <th>FRACTAL_DIMENSION3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>0.405172</td>\n",
       "      <td>...</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.277313</td>\n",
       "      <td>...</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>...</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>...</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>...</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>0.478900</td>\n",
       "      <td>...</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>2.873000</td>\n",
       "      <td>...</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TEXTURE1  PERIMETER1        AREA1  SMOOTHNESS1  COMPACTNESS1  \\\n",
       "count  569.000000  569.000000   569.000000   569.000000    569.000000   \n",
       "mean    19.289649   91.969033   654.889104     0.096360      0.104341   \n",
       "std      4.301036   24.298981   351.914129     0.014064      0.052813   \n",
       "min      9.710000   43.790000   143.500000     0.052630      0.019380   \n",
       "25%     16.170000   75.170000   420.300000     0.086370      0.064920   \n",
       "50%     18.840000   86.240000   551.100000     0.095870      0.092630   \n",
       "75%     21.800000  104.100000   782.700000     0.105300      0.130400   \n",
       "max     39.280000  188.500000  2501.000000     0.163400      0.345400   \n",
       "\n",
       "       CONCAVITY1  CONCAVE_POINTS1   SYMMETRY1  FRACTAL_DIMENSION1  \\\n",
       "count  569.000000       569.000000  569.000000          569.000000   \n",
       "mean     0.088799         0.048919    0.181162            0.062798   \n",
       "std      0.079720         0.038803    0.027414            0.007060   \n",
       "min      0.000000         0.000000    0.106000            0.049960   \n",
       "25%      0.029560         0.020310    0.161900            0.057700   \n",
       "50%      0.061540         0.033500    0.179200            0.061540   \n",
       "75%      0.130700         0.074000    0.195700            0.066120   \n",
       "max      0.426800         0.201200    0.304000            0.097440   \n",
       "\n",
       "          RADIUS2  ...     RADIUS3    TEXTURE3  PERIMETER3        AREA3  \\\n",
       "count  569.000000  ...  569.000000  569.000000  569.000000   569.000000   \n",
       "mean     0.405172  ...   16.269190   25.677223  107.261213   880.583128   \n",
       "std      0.277313  ...    4.833242    6.146258   33.602542   569.356993   \n",
       "min      0.111500  ...    7.930000   12.020000   50.410000   185.200000   \n",
       "25%      0.232400  ...   13.010000   21.080000   84.110000   515.300000   \n",
       "50%      0.324200  ...   14.970000   25.410000   97.660000   686.500000   \n",
       "75%      0.478900  ...   18.790000   29.720000  125.400000  1084.000000   \n",
       "max      2.873000  ...   36.040000   49.540000  251.200000  4254.000000   \n",
       "\n",
       "       SMOOTHNESS3  COMPACTNESS3  CONCAVITY3  CONCAVE_POINTS3   SYMMETRY3  \\\n",
       "count   569.000000    569.000000  569.000000       569.000000  569.000000   \n",
       "mean      0.132369      0.254265    0.272188         0.114606    0.290076   \n",
       "std       0.022832      0.157336    0.208624         0.065732    0.061867   \n",
       "min       0.071170      0.027290    0.000000         0.000000    0.156500   \n",
       "25%       0.116600      0.147200    0.114500         0.064930    0.250400   \n",
       "50%       0.131300      0.211900    0.226700         0.099930    0.282200   \n",
       "75%       0.146000      0.339100    0.382900         0.161400    0.317900   \n",
       "max       0.222600      1.058000    1.252000         0.291000    0.663800   \n",
       "\n",
       "       FRACTAL_DIMENSION3  \n",
       "count          569.000000  \n",
       "mean             0.083946  \n",
       "std              0.018061  \n",
       "min              0.055040  \n",
       "25%              0.071460  \n",
       "50%              0.080040  \n",
       "75%              0.092080  \n",
       "max              0.207500  \n",
       "\n",
       "[8 rows x 29 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "922b3df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating TabularDataset object\n",
    "data = tapas.datasets.TabularDataset(data=df, \n",
    "                                     description=tapas.datasets.DataDescription(json.load(open(\"../data/cancer.json\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fee0e94",
   "metadata": {},
   "source": [
    "## Attacks, Threat Models and Privacy Auditing\n",
    "\n",
    "### Description\n",
    "Contains helper functions and methods for defining attacker knowledge on data, attacker knowledge on generator, threat models, different attack types provided in the TAPAS toolbox, generators for synthetic data, creation of target indices, functions that define the attack, train/test the model and generate metrics and reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38c5208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enum for attacker knowledge on data\n",
    "class AttackerKnowledgeOnData(Enum):\n",
    "    # This attacker knowledge assumes access to some auxiliary dataset from which training datasets are sampled, as random subset of this auxiliary data.\n",
    "    # A distinct testing dataset, sampled from the same distribution, is also used to generate testing samples.\n",
    "    AuxiliaryDataKnowledge = 1\n",
    "    \n",
    "    # Also called worst-case attack, this assumes that the attacker knows the exact dataset used to generate \n",
    "    ExactDataKnowledge = 2\n",
    "    \n",
    "# Helper function that returns attacker knowledge on data\n",
    "def getAttackerKnowledgeOnData(dataset, attackerKnowledgeOnData):\n",
    "    if attackerKnowledgeOnData == AttackerKnowledgeOnData.AuxiliaryDataKnowledge:\n",
    "        return tapas.threat_models.AuxiliaryDataKnowledge(dataset,\n",
    "                        auxiliary_split=0.5, num_training_records=100, )\n",
    "    elif attackerKnowledgeOnData == AttackerKnowledgeOnData.ExactDataKnowledge:\n",
    "        return tapas.threat_models.ExactDataKnowledge(dataset, num_training_records=100, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1035c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enum for attacker knowledge on generator\n",
    "class AttackerKnowledgeOnGenerator(Enum):\n",
    "    # BlackBoxKnowledge: The attacker has access to the generator method with access to the generator has an exact black-box.\n",
    "    # The attacker can call the generator with the same parameters as were used to produce the real dataset. This is the recommended assumption on attacker knowledge.\n",
    "    BlackBoxKnowledge = 1\n",
    "    \n",
    "    # The attacker does not have access to the generator. The attacker cannot call the generator, and the .generate method thus fails in training mode.\n",
    "    # A generator is still needed to generate evaluation samples.\n",
    "    NoBoxKnowledge = 2\n",
    "    \n",
    "    # The attacker has uncertain knowledge of the generator: they have access to the code, but not to some \"parameters\" of the code. \n",
    "    # Instead, the attacker has a prior (distribution) of acceptable parameters.\n",
    "    UncertainBoxKnowledge = 3\n",
    "    \n",
    "# Helper function that returns attacker knowledge on generator\n",
    "def getAttackerKnowledgeOnGenerator(generator, attackerKnowledgeOnGenerator):\n",
    "    if attackerKnowledgeOnGenerator == AttackerKnowledgeOnGenerator.BlackBoxKnowledge:\n",
    "        return tapas.threat_models.BlackBoxKnowledge(generator, num_synthetic_records=100, )\n",
    "    elif attackerKnowledgeOnGenerator == AttackerKnowledgeOnGenerator.NoBoxKnowledge:\n",
    "        return tapas.threat_models.NoBoxKnowledge(generator, num_synthetic_records=100, )\n",
    "    elif attackerKnowledgeOnGenerator == AttackerKnowledgeOnGenerator.UncertainBoxKnowledge:\n",
    "        return tapas.threat_models.UncertainBoxKnowledge(generator, num_synthetic_records=100, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69285fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enum for threat models\n",
    "class LabelInferenceTrainableThreatModel(Enum):\n",
    "    # This threat model implements a MIA (membership inference attacks) with arbitrary attacker knowledge on data and generator.\n",
    "    TargetedMIA = 1\n",
    "    \n",
    "    # This threat model implements a AIA (attribute inference attacks) with arbitrary attacker knowledge on data and generator.\n",
    "    TargetedAIA = 2\n",
    "\n",
    "# Helper function that returns threat model from the input parameters\n",
    "def getThreatModel(data_knowledge, target_index, sdg_knowledge, threatModel):\n",
    "    if threatModel == LabelInferenceTrainableThreatModel.TargetedMIA:\n",
    "        return tapas.threat_models.TargetedMIA(attacker_knowledge_data=data_knowledge,\n",
    "                        target_record=dataset.get_records([target_index]),\n",
    "                        attacker_knowledge_generator=sdg_knowledge,\n",
    "                        generate_pairs=True,\n",
    "                        replace_target=True,\n",
    "                        iterator_tracker=tqdm.tqdm)\n",
    "    elif threatModel == LabelInferenceTrainableThreatModel.TargetedAIA:\n",
    "        return tapas.threat_models.TargetedAIA(attacker_knowledge_data=data_knowledge,\n",
    "                        target_record=dataset.get_records([target_index]),\n",
    "                        attacker_knowledge_generator=sdg_knowledge,\n",
    "                        generate_pairs=True,\n",
    "                        replace_target=True,\n",
    "                        iterator_tracker=tqdm.tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "585f7e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enum for attacks\n",
    "class Attack(Enum):\n",
    "    # Shadow-modelling Attack: Simulates the dataset generation process, using auxiliary info available to the attacker, and train a classifier to predict a property of the training dataset from the synethic dataset\n",
    "    ShadowModellingAttack = 1\n",
    "    \n",
    "    # Groundhog Attack: The attack introduced by Stadler et al.\n",
    "    GroundhogAttack = 2\n",
    "    \n",
    "    # ProbabilityEstimationAttack: Membership Inference Attack that first estimates a statistical model p_x of the distribution of records in the synthetic data, and then uses p_x(target_record) as score. The intuition is that the distribution of the synthetic data, which is defined by the generator trained on the real data, is more likely to be high for records in the real data.\n",
    "    ProbabilityEstimationAttack = 3\n",
    "    \n",
    "    # SyntheticPredictorAttack: Attribute inference attack that first trains a classifier C on the synthetic data to predict the sensitive value v of a record x, then uses C(target_record) as prediction for the target record.\n",
    "    SyntheticPredictorAttack = 4\n",
    "    \n",
    "    # ClosestDistanceAIA: Attack that finds the closest-record to the target record, and uses the value of the sensitive attribute of that closest-record as answer to the attribute-inference attack\n",
    "    ClosestDistanceAIA = 5\n",
    "    \n",
    "    # ClosestDistanceMIA: Attack that looks for the closest record to a given target in the synthetic data to determine whether the target was in the training set\n",
    "    ClosestDistanceMIA = 6\n",
    "    \n",
    "    # LocalNeighbourhoodAttack: Attack that makes a decision based on records similar to the target record, specifically all records within a sphere of a given radius, for a specific choice of distance\n",
    "    LocalNeighbourhoodAttack = 7\n",
    "    \n",
    "    # DirectLinkage: Attack that checks only whether or not the target is in the generated synthetic dataset or not\n",
    "    DirectLinkage = 8\n",
    "\n",
    "# Returns attack for the input args\n",
    "def getAttack(attack, feature_classifier):\n",
    "    if attack == Attack.ShadowModellingAttack:\n",
    "        return tapas.attacks.ShadowModellingAttack(feature_classifier)\n",
    "    elif attack == Attack.GroundhogAttack:\n",
    "        return tapas.attacks.GroundhogAttack(feature_classifier)\n",
    "    elif attack == Attack.ProbabilityEstimationAttack:\n",
    "        return tapas.attacks.ProbabilityEstimationAttack()\n",
    "    elif attack == Attack.SyntheticPredictorAttack:\n",
    "        return tapas.attacks.SyntheticPredictorAttack()\n",
    "    elif attack == Attack.ClosestDistanceAIA:\n",
    "        return tapas.attacks.ClosestDistanceAIA()\n",
    "    elif attack == Attack.ClosestDistanceMIA:\n",
    "        return tapas.attacks.ClosestDistanceMIA()\n",
    "    elif attack == Attack.LocalNeighbourhoodAttack:\n",
    "        return tapas.attacks.LocalNeighbourhoodAttack()\n",
    "    elif attack == Attack.DirectLinkage:\n",
    "        return tapas.attacks.DirectLinkage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9e79fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enum for synthetic data generators\n",
    "class Generator(Enum):\n",
    "    # Raw data\n",
    "    Raw = 1\n",
    "    # Conditional Tabular Generative Adversarial Network models single table - tabular data distribution for synthetic data generators\n",
    "    CTGAN = 2\n",
    "    # Differentially Private Conditional Tabular GAN: Incorporates differential privacy guarantees into conditional tabular generative model\n",
    "    DPCTGAN = 3\n",
    "    # Private Aggregation of Teacher Ensembles (PATE) framework applied to GANs; modified framework (which is called PATE-GAN) allows us to tightly bound the influence of any individual sample on the model, resulting in tight differential privacy guarantees and thus an improved performance over models with the same guarantees. \n",
    "    PATEGAN = 4\n",
    "    \n",
    "# Helper function for getting the generator with the input specs\n",
    "def getGenerator(generator, epsilon, batch_size, epochs):\n",
    "    if generator == Generator.Raw:\n",
    "        return Raw()\n",
    "    elif generator == Generator.CTGAN:\n",
    "        return CTGAN(epochs=1)\n",
    "    elif generator == Generator.DPCTGAN:\n",
    "        return DPCTGAN(epsilon=epsilon, batch_size=batch_size, epochs=epochs)\n",
    "    elif generator == Generator.PATEGAN:\n",
    "        return PATEGAN(epsilon=epsilon, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02e736c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of records to target for random and outlier\n",
    "num_targets = 5\n",
    "    \n",
    "# Selecting random target record indices\n",
    "random_index = list(np.random.randint(1, 100, num_targets))\n",
    "\n",
    "# Creates target indices to target for random and outlier\n",
    "def createTargetIndices():\n",
    "    # Selecting outlier target record indices\n",
    "    model_isoforest = IsolationForest()\n",
    "    preds = model_isoforest.fit_predict(data.data.iloc[:, 3:])\n",
    "    outlier_index = list(np.random.choice(np.where(preds == -1)[0], num_targets))\n",
    "\n",
    "    # Listing out target indices by combining random and outlier indicies\n",
    "    targets = random_index + outlier_index\n",
    "\n",
    "    return targets\n",
    "\n",
    "targets = createTargetIndices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66341900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[84, 70, 70, 90, 66, 491, 73, 57, 491, 550]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb23f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that defines the attack by consolidating data knowledge, knowledge on the generator as well as training and testing the attacks by calling the helper functions\n",
    "def attack(dataset, target_index, generator):\n",
    "    # Getting attacker knowledge on data\n",
    "    data_knowledge = getAttackerKnowledgeOnData(dataset, AttackerKnowledgeOnData.AuxiliaryDataKnowledge)\n",
    "    \n",
    "    # Getting knowledge on generator\n",
    "    sdg_knowledge = getAttackerKnowledgeOnGenerator(generator, AttackerKnowledgeOnGenerator.BlackBoxKnowledge)\n",
    "    \n",
    "    # Defining the threat model membership inference attack on a random record with attacker goal\n",
    "    threat_model = tapas.threat_models.TargetedMIA(attacker_knowledge_data=data_knowledge,\n",
    "                        target_record=dataset.get_records([target_index]),\n",
    "                        attacker_knowledge_generator=sdg_knowledge,\n",
    "                        generate_pairs=True,\n",
    "                        replace_target=True,\n",
    "                        iterator_tracker=tqdm.tqdm)\n",
    "\n",
    "    # Initializing an attacker of Groundhog attack with standard parameters\n",
    "    random_forest = RandomForestClassifier(n_estimators=100)\n",
    "    feature_set = tapas.attacks.NaiveSetFeature() + tapas.attacks.HistSetFeature() + tapas.attacks.CorrSetFeature()\n",
    "    feature_classifier = tapas.attacks.FeatureBasedSetClassifier(feature_set, random_forest)\n",
    "    attacker = getAttack(Attack.GroundhogAttack, feature_classifier)\n",
    "    \n",
    "    # Training the attack\n",
    "    start = time.time()\n",
    "    try:\n",
    "        attacker.train(threat_model, num_samples=10)\n",
    "    except:\n",
    "        print(\"An exception occurred while training the threat model.\")\n",
    "    end = time.time()\n",
    "    print(\"Training time delta for the attacker: {}\".format(end-start))\n",
    "\n",
    "    # Testing the attack\n",
    "    start = time.time()\n",
    "    try:\n",
    "        summary = threat_model.test(attacker, num_samples=10)\n",
    "    except:\n",
    "        print(\"An exception occurred while testing the threat model.\")\n",
    "    end = time.time()\n",
    "    print(\"Testing time delta for the attacker: {}\".format(end-start))\n",
    "    \n",
    "    try:\n",
    "        metrics = summary.get_metrics()\n",
    "    except:\n",
    "        print(\"An exception occurred while getting metrics for the summary.\")\n",
    "        \n",
    "    # Defining metrics for data set of Breast Cancer\n",
    "    metrics[\"dataset\"] = \"Breast Cancer\"\n",
    "    \n",
    "    print(\"Metrics: \")\n",
    "    print(metrics)\n",
    "    \n",
    "    return summary, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9935ed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function that collects metrics and summaries from the attacks conducted on generators and targets\n",
    "def main():\n",
    "    # Initializing an empty data frame and all_summaries\n",
    "    all_metrics = pd.DataFrame()\n",
    "    all_summaries = []\n",
    "    \n",
    "    # Defining an array of generators\n",
    "    generators = [getGenerator(Generator.Raw, 0, 0, 0),\n",
    "                  getGenerator(Generator.CTGAN, 0, 0, 1), \n",
    "                  getGenerator(Generator.DPCTGAN, 1, 64, 1)]\n",
    "\n",
    "    # Looping through generators\n",
    "    for generator in generators: \n",
    "        # Looping through targets\n",
    "        for target in targets: \n",
    "            try:\n",
    "                summ, metr = attack(dataset=data, target_index=target, generator=generator)\n",
    "                all_summaries.append(summ)\n",
    "                all_metrics = pd.concat([all_metrics, metr], axis=0, ignore_index=True)\n",
    "                print(metr.head())\n",
    "            except Exception:\n",
    "                continue\n",
    "    return all_metrics, all_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb098720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 825.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 0.23276209831237793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1673.37it/s]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 0.14879107475280762\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        84       Raw  Groundhog       0.6                 0.6   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.4            0.2           0.8  0.72                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        84       Raw  Groundhog       0.6                 0.6   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.4            0.2           0.8  0.72                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 2007.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 0.17675089836120605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1856.38it/s]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 0.395251989364624\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        70       Raw  Groundhog       0.6                 0.6   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.4            0.2           0.8  0.68                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        70       Raw  Groundhog       0.6                 0.6   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.4            0.2           0.8  0.68                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1873.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 0.2001509666442871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1787.02it/s]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 0.13936686515808105\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        70       Raw  Groundhog       0.5                 0.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            0.0           1.0  0.6                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        70       Raw  Groundhog       0.5                 0.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            0.0           1.0  0.6                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1998.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 0.1834869384765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1829.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 0.16707897186279297\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        90       Raw  Groundhog       0.7                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.6            0.4           0.6  0.76                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        90       Raw  Groundhog       0.7                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.6            0.4           0.6  0.76                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1211.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 0.21870207786560059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 2366.32it/s]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 0.1372051239013672\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        66       Raw  Groundhog       0.5                 0.2   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.2            0.0           1.0  0.66           1.386294  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        66       Raw  Groundhog       0.5                 0.2   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.2            0.0           1.0  0.66           1.386294  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 2246.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 0.18952393531799316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 2244.62it/s]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 0.1678481101989746\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       491       Raw  Groundhog       0.5                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  1.0            0.0           1.0  0.92                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       491       Raw  Groundhog       0.5                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  1.0            0.0           1.0  0.92                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1735.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 0.21111416816711426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 2295.61it/s]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 0.1347339153289795\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        73       Raw  Groundhog       0.7                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.6            0.4           0.6  0.88                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        73       Raw  Groundhog       0.7                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.6            0.4           0.6  0.88                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1817.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 0.23004603385925293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1112.99it/s]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 0.16250991821289062\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        57       Raw  Groundhog       0.7                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.6            0.4           0.6  0.86           1.609438  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        57       Raw  Groundhog       0.7                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.6            0.4           0.6  0.86           1.609438  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1956.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 0.18204641342163086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 2033.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 0.1876697540283203\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       491       Raw  Groundhog       0.9                 0.8   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.0            0.8           0.2  0.94                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       491       Raw  Groundhog       0.9                 0.8   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.0            0.8           0.2  0.94                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1997.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 0.21445298194885254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 2186.24it/s]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 0.12266707420349121\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       550       Raw  Groundhog       0.6                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.8            0.2           0.8  0.84                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       550       Raw  Groundhog       0.6                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.8            0.2           0.8  0.84                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:40<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 40.3291437625885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:40<00:00,  4.04s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 40.49183368682861\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        84     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        84     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:37<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 37.57262897491455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:34<00:00,  3.41s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 34.23529386520386\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        70     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        70     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:34<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 35.14689612388611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:38<00:00,  3.83s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 38.41364097595215\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        70     CTGAN  Groundhog       0.9                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.2            0.8           0.2  1.0                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        70     CTGAN  Groundhog       0.9                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.2            0.8           0.2  1.0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:35<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 35.206493854522705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:34<00:00,  3.45s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 34.59374403953552\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        90     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        90     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:36<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 36.311142921447754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:34<00:00,  3.44s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 34.57537579536438\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        66     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        66     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:34<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 35.00323700904846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:34<00:00,  3.41s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 34.19427418708801\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       491     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       491     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:34<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 34.87544322013855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:32<00:00,  3.28s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 32.93224096298218\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        73     CTGAN  Groundhog       0.8                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.4            0.6           0.4  1.0                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        73     CTGAN  Groundhog       0.8                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.4            0.6           0.4  1.0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:36<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 36.338988065719604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:37<00:00,  3.78s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 37.94095587730408\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        57     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        57     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:34<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 35.06590819358826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:35<00:00,  3.57s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 35.86284279823303\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       491     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       491     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:34<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 34.2682991027832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:33<00:00,  3.40s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 34.08963680267334\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       550     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       550     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0209, Loss D: 0.0001\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0192, Loss D: 0.0003\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0205, Loss D: 0.0037\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0213, Loss D: -0.0030\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0159, Loss D: -0.0010\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0139, Loss D: -0.0018\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0218, Loss D: -0.0036\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:19<02:51, 19.02s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0196, Loss D: 0.0002\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0150, Loss D: 0.0023\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0186, Loss D: 0.0002\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0112, Loss D: -0.0018\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0155, Loss D: 0.0092\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0151, Loss D: -0.0003\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0184, Loss D: -0.0104\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:36<02:25, 18.16s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0670, Loss D: -0.0035\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0665, Loss D: -0.0030\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0627, Loss D: 0.0045\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0671, Loss D: 0.0055\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0672, Loss D: 0.0021\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0622, Loss D: 0.0095\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0627, Loss D: -0.0020\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:54<02:06, 18.14s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0619, Loss D: -0.0045\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0592, Loss D: 0.0012\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0632, Loss D: 0.0057\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0659, Loss D: -0.0006\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0630, Loss D: 0.0074\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0621, Loss D: 0.0039\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0685, Loss D: 0.0029\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:12<01:47, 17.89s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0698, Loss D: 0.0019\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0741, Loss D: -0.0006\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0681, Loss D: -0.0032\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0690, Loss D: 0.0047\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0673, Loss D: -0.0042\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0713, Loss D: 0.0036\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0662, Loss D: 0.0032\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:29<01:29, 17.84s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0439, Loss D: 0.0050\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0427, Loss D: -0.0016\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0482, Loss D: -0.0002\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0427, Loss D: -0.0024\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0491, Loss D: 0.0025\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0456, Loss D: 0.0028\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0470, Loss D: 0.0027\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:48<01:12, 18.12s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0307, Loss D: 0.0025\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0326, Loss D: -0.0019\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0291, Loss D: 0.0044\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0267, Loss D: -0.0030\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0294, Loss D: 0.0052\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0307, Loss D: 0.0057\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0365, Loss D: 0.0029\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [02:13<01:00, 20.26s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0504, Loss D: 0.0054\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0498, Loss D: 0.0034\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0535, Loss D: 0.0066\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0502, Loss D: -0.0006\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0523, Loss D: 0.0017\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0516, Loss D: -0.0013\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0499, Loss D: -0.0062\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:33<00:40, 20.15s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0337, Loss D: 0.0068\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0368, Loss D: 0.0006\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0380, Loss D: 0.0015\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0396, Loss D: -0.0027\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0360, Loss D: -0.0017\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0338, Loss D: 0.0019\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0402, Loss D: -0.0048\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:51<00:19, 19.53s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0308, Loss D: -0.0049\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0252, Loss D: 0.0033\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0239, Loss D: 0.0027\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0266, Loss D: -0.0017\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0239, Loss D: 0.0013\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0251, Loss D: 0.0020\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0285, Loss D: 0.0027\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [03:12<00:00, 19.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 193.16021609306335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0398, Loss D: -0.0067\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0472, Loss D: 0.0034\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0473, Loss D: -0.0012\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0444, Loss D: 0.0007\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0400, Loss D: 0.0006\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0388, Loss D: 0.0017\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0451, Loss D: -0.0020\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:19<02:55, 19.48s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0561, Loss D: 0.0002\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0535, Loss D: -0.0047\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0530, Loss D: 0.0051\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0526, Loss D: 0.0082\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0511, Loss D: -0.0007\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0597, Loss D: 0.0010\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0556, Loss D: 0.0062\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:38<02:31, 18.93s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0140, Loss D: -0.0014\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0133, Loss D: -0.0052\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0190, Loss D: 0.0000\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0133, Loss D: 0.0061\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0141, Loss D: 0.0059\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0172, Loss D: -0.0002\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0160, Loss D: 0.0009\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:55<02:08, 18.37s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0131, Loss D: 0.0032\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0076, Loss D: 0.0012\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0104, Loss D: -0.0014\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0118, Loss D: 0.0065\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0119, Loss D: -0.0016\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0106, Loss D: -0.0006\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0148, Loss D: 0.0045\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:13<01:48, 18.07s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0698, Loss D: -0.0077\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0728, Loss D: 0.0005\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0673, Loss D: 0.0053\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0670, Loss D: 0.0019\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0718, Loss D: 0.0000\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0758, Loss D: 0.0008\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0730, Loss D: 0.0001\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:30<01:29, 17.86s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0314, Loss D: 0.0026\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0298, Loss D: -0.0013\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0283, Loss D: -0.0003\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0294, Loss D: 0.0028\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0299, Loss D: 0.0020\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0282, Loss D: 0.0026\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0306, Loss D: 0.0006\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:47<01:10, 17.53s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0476, Loss D: -0.0045\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0419, Loss D: -0.0022\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0491, Loss D: -0.0054\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0425, Loss D: -0.0010\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0466, Loss D: 0.0007\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0411, Loss D: 0.0027\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0410, Loss D: 0.0023\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [02:04<00:51, 17.20s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0149, Loss D: 0.0000\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0163, Loss D: -0.0010\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0185, Loss D: 0.0005\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0158, Loss D: -0.0022\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0207, Loss D: -0.0021\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0167, Loss D: 0.0037\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0107, Loss D: 0.0004\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:20<00:33, 16.96s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0299, Loss D: 0.0027\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0253, Loss D: 0.0024\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0257, Loss D: -0.0017\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0246, Loss D: 0.0049\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0238, Loss D: 0.0096\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0250, Loss D: 0.0103\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0240, Loss D: 0.0042\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:36<00:16, 16.75s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0423, Loss D: 0.0053\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0404, Loss D: 0.0002\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0428, Loss D: 0.0038\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0417, Loss D: -0.0006\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0444, Loss D: -0.0006\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0424, Loss D: 0.0026\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0420, Loss D: -0.0003\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:53<00:00, 17.35s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 173.61134910583496\n",
      "Metrics: \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        84  DP-CTGAN (eps=1)  Groundhog       0.7   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.6                  0.2            0.4           0.6  0.74   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        84  DP-CTGAN (eps=1)  Groundhog       0.7   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.6                  0.2            0.4           0.6  0.74   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0331, Loss D: 0.0060\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0346, Loss D: 0.0068\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0353, Loss D: 0.0013\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0312, Loss D: 0.0004\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0308, Loss D: -0.0024\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0337, Loss D: 0.0015\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0330, Loss D: 0.0013\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:16<02:29, 16.59s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0627, Loss D: -0.0015\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0608, Loss D: -0.0065\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0654, Loss D: -0.0037\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0673, Loss D: -0.0028\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0610, Loss D: 0.0026\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0628, Loss D: -0.0019\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0650, Loss D: 0.0029\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:33<02:12, 16.60s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0359, Loss D: -0.0006\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0379, Loss D: 0.0057\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0353, Loss D: 0.0007\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0348, Loss D: 0.0036\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0404, Loss D: -0.0033\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0382, Loss D: 0.0029\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0367, Loss D: 0.0023\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:49<01:56, 16.60s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0147, Loss D: -0.0030\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0147, Loss D: 0.0042\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0143, Loss D: -0.0030\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0137, Loss D: 0.0010\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0120, Loss D: 0.0001\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0194, Loss D: -0.0033\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0140, Loss D: 0.0063\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:08<01:43, 17.31s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0388, Loss D: -0.0068\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0329, Loss D: 0.0004\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0383, Loss D: 0.0006\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0384, Loss D: -0.0008\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0396, Loss D: 0.0076\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0370, Loss D: -0.0039\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0411, Loss D: 0.0052\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:24<01:25, 17.07s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0338, Loss D: -0.0014\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0385, Loss D: 0.0024\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0350, Loss D: -0.0001\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0336, Loss D: -0.0000\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0316, Loss D: -0.0024\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0335, Loss D: 0.0055\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0368, Loss D: -0.0004\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:41<01:07, 16.94s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0454, Loss D: 0.0012\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0440, Loss D: 0.0016\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0431, Loss D: 0.0037\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0451, Loss D: 0.0029\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0410, Loss D: -0.0036\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0435, Loss D: 0.0082\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0451, Loss D: 0.0016\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:58<00:50, 16.88s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0389, Loss D: 0.0046\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0354, Loss D: -0.0014\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0358, Loss D: 0.0062\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0374, Loss D: 0.0071\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0377, Loss D: -0.0040\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0368, Loss D: 0.0065\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0332, Loss D: 0.0092\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:14<00:33, 16.80s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0337, Loss D: -0.0049\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0349, Loss D: 0.0003\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0357, Loss D: 0.0048\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0348, Loss D: 0.0047\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0347, Loss D: 0.0015\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0301, Loss D: 0.0020\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0315, Loss D: 0.0011\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:31<00:16, 16.77s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0633, Loss D: 0.0002\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0662, Loss D: 0.0043\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0653, Loss D: -0.0020\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0652, Loss D: -0.0025\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0658, Loss D: -0.0019\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0626, Loss D: 0.0038\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0679, Loss D: 0.0001\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:48<00:00, 16.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 168.50265979766846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0557, Loss D: -0.0020\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0564, Loss D: 0.0028\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0566, Loss D: 0.0029\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0536, Loss D: -0.0050\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0510, Loss D: 0.0006\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0581, Loss D: -0.0076\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0583, Loss D: 0.0049\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:16<02:30, 16.75s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0477, Loss D: -0.0006\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0467, Loss D: 0.0008\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0521, Loss D: 0.0035\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0437, Loss D: -0.0008\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0444, Loss D: -0.0079\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0447, Loss D: 0.0051\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0456, Loss D: 0.0045\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:33<02:14, 16.87s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0171, Loss D: 0.0036\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0152, Loss D: 0.0080\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0173, Loss D: 0.0020\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0148, Loss D: 0.0050\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0139, Loss D: -0.0049\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0139, Loss D: 0.0061\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0163, Loss D: 0.0022\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:52<02:02, 17.55s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0127, Loss D: -0.0008\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0178, Loss D: -0.0086\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0143, Loss D: -0.0034\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0185, Loss D: -0.0075\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0145, Loss D: 0.0024\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0194, Loss D: -0.0012\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0120, Loss D: 0.0056\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:08<01:43, 17.18s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0427, Loss D: -0.0054\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0387, Loss D: 0.0004\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0362, Loss D: 0.0065\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0384, Loss D: 0.0101\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0360, Loss D: 0.0018\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0389, Loss D: -0.0040\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0386, Loss D: 0.0050\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:25<01:24, 16.92s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0351, Loss D: -0.0038\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0367, Loss D: 0.0007\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0376, Loss D: 0.0014\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0395, Loss D: -0.0013\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0373, Loss D: -0.0004\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0376, Loss D: 0.0091\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0397, Loss D: 0.0009\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:42<01:08, 17.12s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0483, Loss D: -0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0468, Loss D: -0.0001\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0470, Loss D: -0.0010\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0524, Loss D: -0.0001\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0476, Loss D: 0.0049\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0467, Loss D: 0.0025\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0465, Loss D: 0.0091\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:59<00:51, 17.12s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0204, Loss D: 0.0016\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0195, Loss D: -0.0003\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0218, Loss D: 0.0071\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0227, Loss D: 0.0060\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0229, Loss D: -0.0012\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0291, Loss D: 0.0018\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0241, Loss D: 0.0011\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:16<00:33, 16.93s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0043, Loss D: 0.0006\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0029, Loss D: 0.0030\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0060, Loss D: 0.0043\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0037, Loss D: 0.0003\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0008, Loss D: 0.0023\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0023, Loss D: 0.0014\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0007, Loss D: 0.0003\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:32<00:16, 16.84s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0462, Loss D: 0.0023\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0440, Loss D: 0.0066\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0517, Loss D: 0.0039\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0485, Loss D: -0.0017\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0426, Loss D: 0.0041\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0466, Loss D: -0.0001\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0507, Loss D: 0.0025\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:49<00:00, 16.99s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 169.99311089515686\n",
      "Metrics: \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        70  DP-CTGAN (eps=1)  Groundhog       0.6   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.6                  0.4            0.2           0.8  0.78   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        70  DP-CTGAN (eps=1)  Groundhog       0.6   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.6                  0.4            0.2           0.8  0.78   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0388, Loss D: 0.0032\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0396, Loss D: -0.0037\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0347, Loss D: 0.0004\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0388, Loss D: -0.0026\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0403, Loss D: 0.0052\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0373, Loss D: -0.0016\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0315, Loss D: 0.0003\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:16<02:30, 16.67s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0662, Loss D: 0.0019\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0676, Loss D: -0.0002\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0649, Loss D: 0.0027\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0615, Loss D: 0.0042\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0607, Loss D: 0.0037\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0653, Loss D: 0.0011\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0626, Loss D: 0.0021\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:33<02:13, 16.73s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0369, Loss D: -0.0038\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0354, Loss D: 0.0037\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0340, Loss D: 0.0052\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0344, Loss D: -0.0027\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0334, Loss D: -0.0045\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0316, Loss D: 0.0030\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0366, Loss D: -0.0038\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:50<01:57, 16.78s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0368, Loss D: -0.0077\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0337, Loss D: -0.0019\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0348, Loss D: 0.0036\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0358, Loss D: 0.0007\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0363, Loss D: -0.0005\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0349, Loss D: -0.0066\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0320, Loss D: -0.0013\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:14<01:57, 19.62s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0124, Loss D: 0.0019\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0099, Loss D: 0.0052\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0087, Loss D: -0.0004\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0155, Loss D: 0.0051\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0100, Loss D: 0.0008\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0123, Loss D: 0.0034\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0096, Loss D: -0.0030\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:32<01:35, 19.08s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0101, Loss D: 0.0013\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0136, Loss D: 0.0036\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0110, Loss D: -0.0002\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0098, Loss D: 0.0064\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0143, Loss D: 0.0079\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0144, Loss D: 0.0011\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0128, Loss D: 0.0022\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:49<01:14, 18.55s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0465, Loss D: -0.0026\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0505, Loss D: -0.0012\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0506, Loss D: 0.0006\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0460, Loss D: 0.0004\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0507, Loss D: -0.0001\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0524, Loss D: 0.0019\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0460, Loss D: 0.0043\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [02:06<00:53, 17.99s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0331, Loss D: 0.0012\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0357, Loss D: -0.0001\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0335, Loss D: 0.0070\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0406, Loss D: 0.0072\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0376, Loss D: 0.0064\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0338, Loss D: 0.0041\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0331, Loss D: -0.0008\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:24<00:35, 17.78s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0320, Loss D: 0.0044\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0396, Loss D: 0.0059\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0306, Loss D: -0.0011\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0408, Loss D: 0.0062\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0399, Loss D: 0.0030\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0352, Loss D: 0.0049\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0329, Loss D: -0.0005\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:41<00:17, 17.57s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0340, Loss D: -0.0046\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0386, Loss D: 0.0029\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0401, Loss D: 0.0008\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0401, Loss D: -0.0003\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0376, Loss D: 0.0051\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0376, Loss D: -0.0028\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0388, Loss D: 0.0060\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:59<00:00, 17.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 179.28112196922302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0430, Loss D: 0.0054\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0441, Loss D: -0.0020\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0424, Loss D: -0.0034\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0477, Loss D: -0.0011\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0415, Loss D: 0.0058\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0442, Loss D: 0.0033\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0380, Loss D: 0.0017\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:18<02:46, 18.54s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0382, Loss D: 0.0010\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0376, Loss D: 0.0026\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0345, Loss D: 0.0035\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0360, Loss D: -0.0018\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0423, Loss D: 0.0007\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0435, Loss D: 0.0019\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0338, Loss D: -0.0013\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:36<02:23, 17.95s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0633, Loss D: 0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0664, Loss D: 0.0029\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0642, Loss D: 0.0065\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0669, Loss D: -0.0050\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0659, Loss D: 0.0063\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0667, Loss D: -0.0001\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0643, Loss D: 0.0012\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:52<02:00, 17.23s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0427, Loss D: -0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0363, Loss D: -0.0011\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0366, Loss D: -0.0003\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0379, Loss D: 0.0072\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0386, Loss D: 0.0037\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0334, Loss D: 0.0015\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0385, Loss D: -0.0022\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:08<01:39, 16.63s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0460, Loss D: 0.0038\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0489, Loss D: -0.0012\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0493, Loss D: -0.0017\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0484, Loss D: 0.0025\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0482, Loss D: 0.0029\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0488, Loss D: -0.0030\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0427, Loss D: 0.0002\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:24<01:22, 16.50s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0133, Loss D: -0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0164, Loss D: 0.0018\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0130, Loss D: -0.0017\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0144, Loss D: 0.0049\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0152, Loss D: -0.0017\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0104, Loss D: 0.0035\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0159, Loss D: 0.0035\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:40<01:05, 16.33s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0121, Loss D: -0.0019\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0099, Loss D: -0.0005\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0104, Loss D: 0.0013\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0133, Loss D: -0.0017\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0061, Loss D: -0.0004\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0112, Loss D: 0.0036\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0087, Loss D: 0.0043\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [02:00<00:52, 17.64s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0315, Loss D: -0.0066\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0302, Loss D: -0.0017\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0283, Loss D: 0.0045\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0297, Loss D: 0.0002\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0317, Loss D: 0.0002\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0336, Loss D: 0.0000\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0303, Loss D: 0.0017\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:19<00:35, 17.83s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0366, Loss D: 0.0071\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0310, Loss D: -0.0008\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0338, Loss D: 0.0074\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0346, Loss D: 0.0010\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0362, Loss D: 0.0067\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0348, Loss D: 0.0073\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0343, Loss D: -0.0001\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:34<00:16, 16.94s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0475, Loss D: -0.0048\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0452, Loss D: -0.0035\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0470, Loss D: -0.0026\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0505, Loss D: 0.0013\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0494, Loss D: 0.0053\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0477, Loss D: -0.0002\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0478, Loss D: -0.0014\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:49<00:00, 16.93s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 169.41314888000488\n",
      "Metrics: \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        70  DP-CTGAN (eps=1)  Groundhog       0.5   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.4                  0.4            0.0           1.0  0.58   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        70  DP-CTGAN (eps=1)  Groundhog       0.5   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.4                  0.4            0.0           1.0  0.58   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0720, Loss D: 0.0026\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0727, Loss D: 0.0004\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0671, Loss D: 0.0014\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0699, Loss D: 0.0008\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0760, Loss D: 0.0047\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0716, Loss D: 0.0021\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0729, Loss D: 0.0011\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:15<02:15, 15.10s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0511, Loss D: -0.0050\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0569, Loss D: 0.0019\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0567, Loss D: -0.0073\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0514, Loss D: -0.0013\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0536, Loss D: 0.0006\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0517, Loss D: -0.0024\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0561, Loss D: 0.0095\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:30<02:00, 15.10s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0725, Loss D: -0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0700, Loss D: 0.0038\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0675, Loss D: -0.0016\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0696, Loss D: 0.0004\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0721, Loss D: 0.0036\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0727, Loss D: 0.0067\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0719, Loss D: -0.0012\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:45<01:47, 15.33s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0422, Loss D: 0.0006\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0402, Loss D: -0.0045\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0356, Loss D: 0.0014\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0378, Loss D: -0.0012\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0380, Loss D: 0.0035\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0351, Loss D: 0.0034\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0354, Loss D: 0.0041\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:01<01:33, 15.50s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0341, Loss D: 0.0011\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0367, Loss D: 0.0045\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0319, Loss D: 0.0020\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0326, Loss D: -0.0007\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0383, Loss D: 0.0002\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0330, Loss D: 0.0011\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0346, Loss D: 0.0006\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:16<01:17, 15.42s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0468, Loss D: 0.0041\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0490, Loss D: 0.0001\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0494, Loss D: 0.0020\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0536, Loss D: 0.0010\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0464, Loss D: 0.0058\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0446, Loss D: -0.0043\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0480, Loss D: -0.0004\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:32<01:02, 15.56s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0221, Loss D: -0.0062\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0219, Loss D: 0.0039\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0176, Loss D: -0.0030\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0151, Loss D: -0.0027\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0114, Loss D: 0.0050\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0128, Loss D: -0.0018\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0103, Loss D: 0.0039\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:47<00:46, 15.39s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0374, Loss D: -0.0044\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0395, Loss D: 0.0011\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0426, Loss D: 0.0011\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0390, Loss D: 0.0030\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0363, Loss D: 0.0087\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0392, Loss D: -0.0036\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0385, Loss D: -0.0002\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:03<00:31, 15.50s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0367, Loss D: 0.0026\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0371, Loss D: 0.0040\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0387, Loss D: -0.0053\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0418, Loss D: -0.0014\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0399, Loss D: 0.0031\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0362, Loss D: 0.0048\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0390, Loss D: 0.0024\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:19<00:15, 15.78s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0408, Loss D: 0.0004\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0405, Loss D: -0.0002\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0397, Loss D: -0.0002\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0380, Loss D: -0.0028\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0355, Loss D: 0.0040\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0396, Loss D: -0.0038\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0416, Loss D: 0.0056\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:36<00:00, 15.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 156.2742838859558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0034, Loss D: -0.0020\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0019, Loss D: -0.0018\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0041, Loss D: 0.0006\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0057, Loss D: 0.0042\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0081, Loss D: 0.0065\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0092, Loss D: -0.0014\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0082, Loss D: -0.0009\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:15<02:16, 15.17s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0292, Loss D: -0.0011\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0270, Loss D: 0.0000\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0242, Loss D: -0.0032\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0293, Loss D: 0.0035\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0264, Loss D: 0.0103\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0234, Loss D: 0.0016\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0297, Loss D: 0.0032\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:31<02:06, 15.76s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0336, Loss D: 0.0016\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0388, Loss D: -0.0022\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0363, Loss D: -0.0065\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0381, Loss D: -0.0015\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0406, Loss D: 0.0003\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0376, Loss D: 0.0063\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0343, Loss D: 0.0045\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:48<01:55, 16.48s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0342, Loss D: -0.0066\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0374, Loss D: 0.0034\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0368, Loss D: 0.0046\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0357, Loss D: -0.0058\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0364, Loss D: 0.0036\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0397, Loss D: 0.0036\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0432, Loss D: 0.0028\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:06<01:41, 16.90s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0428, Loss D: -0.0038\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0390, Loss D: -0.0030\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0408, Loss D: -0.0031\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0362, Loss D: 0.0045\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0371, Loss D: 0.0067\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0381, Loss D: -0.0013\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0364, Loss D: 0.0051\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:23<01:24, 16.93s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0525, Loss D: -0.0002\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0474, Loss D: 0.0027\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0501, Loss D: 0.0009\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0526, Loss D: 0.0044\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0563, Loss D: 0.0009\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0520, Loss D: -0.0061\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0527, Loss D: 0.0022\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:39<01:06, 16.59s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0059, Loss D: 0.0003\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0112, Loss D: 0.0078\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0074, Loss D: -0.0024\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0178, Loss D: 0.0069\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0159, Loss D: -0.0041\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0114, Loss D: -0.0007\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0147, Loss D: -0.0004\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:54<00:48, 16.10s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0676, Loss D: 0.0034\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0708, Loss D: 0.0039\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0695, Loss D: 0.0004\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0692, Loss D: 0.0055\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0702, Loss D: 0.0019\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0721, Loss D: 0.0020\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0703, Loss D: 0.0008\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:13<00:33, 16.96s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0234, Loss D: -0.0054\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0219, Loss D: -0.0023\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0219, Loss D: -0.0038\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0217, Loss D: 0.0000\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0235, Loss D: 0.0028\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0238, Loss D: -0.0007\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0225, Loss D: 0.0008\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:29<00:16, 16.68s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0365, Loss D: 0.0055\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0340, Loss D: 0.0030\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0433, Loss D: 0.0020\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0381, Loss D: 0.0008\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0394, Loss D: -0.0016\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0336, Loss D: -0.0049\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0415, Loss D: 0.0036\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:43<00:00, 16.34s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 163.4607789516449\n",
      "Metrics: \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        90  DP-CTGAN (eps=1)  Groundhog       0.5   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 1.0                  1.0            0.0           1.0  0.56   \n",
      "\n",
      "   effective_epsilon  \n",
      "0           0.287682  \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        90  DP-CTGAN (eps=1)  Groundhog       0.5   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 1.0                  1.0            0.0           1.0  0.56   \n",
      "\n",
      "   effective_epsilon  \n",
      "0           0.287682  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0484, Loss D: -0.0011\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0525, Loss D: 0.0014\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0457, Loss D: -0.0058\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0529, Loss D: 0.0034\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0492, Loss D: 0.0014\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0516, Loss D: -0.0033\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0506, Loss D: 0.0003\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:15<02:22, 15.78s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0491, Loss D: -0.0047\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0508, Loss D: -0.0024\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0498, Loss D: 0.0028\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0495, Loss D: -0.0046\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0506, Loss D: -0.0005\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0479, Loss D: 0.0022\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0511, Loss D: -0.0036\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:32<02:10, 16.26s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0382, Loss D: -0.0001\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0326, Loss D: 0.0009\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0411, Loss D: -0.0041\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0324, Loss D: 0.0022\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0367, Loss D: 0.0030\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0375, Loss D: 0.0014\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0346, Loss D: 0.0036\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:49<01:57, 16.72s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0437, Loss D: 0.0054\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0455, Loss D: 0.0005\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0414, Loss D: 0.0007\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0437, Loss D: -0.0007\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0446, Loss D: -0.0036\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0440, Loss D: -0.0017\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0414, Loss D: -0.0010\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:05<01:37, 16.29s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0548, Loss D: -0.0105\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0553, Loss D: -0.0003\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0559, Loss D: -0.0013\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0565, Loss D: 0.0071\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0538, Loss D: -0.0018\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0542, Loss D: 0.0035\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0510, Loss D: 0.0027\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:21<01:21, 16.26s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0197, Loss D: 0.0011\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0130, Loss D: -0.0032\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0214, Loss D: 0.0004\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0186, Loss D: 0.0015\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0218, Loss D: 0.0020\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0200, Loss D: 0.0012\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0179, Loss D: 0.0022\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:38<01:06, 16.67s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0314, Loss D: 0.0081\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0353, Loss D: 0.0023\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0402, Loss D: -0.0002\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0369, Loss D: 0.0022\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0383, Loss D: -0.0029\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0372, Loss D: 0.0070\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0358, Loss D: -0.0029\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:55<00:49, 16.49s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0128, Loss D: 0.0028\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0129, Loss D: 0.0045\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0139, Loss D: -0.0009\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0122, Loss D: 0.0006\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0146, Loss D: 0.0034\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0140, Loss D: 0.0020\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0144, Loss D: -0.0020\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:09<00:31, 15.84s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0066, Loss D: -0.0025\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0066, Loss D: 0.0049\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0037, Loss D: -0.0019\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0035, Loss D: 0.0010\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0030, Loss D: 0.0027\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0027, Loss D: 0.0003\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0053, Loss D: -0.0009\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:24<00:15, 15.52s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0510, Loss D: -0.0032\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0480, Loss D: 0.0013\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0443, Loss D: -0.0071\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0456, Loss D: -0.0005\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0475, Loss D: -0.0010\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0443, Loss D: -0.0010\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0526, Loss D: -0.0024\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:40<00:00, 16.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 160.81858897209167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0335, Loss D: 0.0011\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0309, Loss D: -0.0036\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0364, Loss D: 0.0032\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0336, Loss D: 0.0016\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0300, Loss D: -0.0064\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0366, Loss D: 0.0010\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0326, Loss D: 0.0050\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:16<02:27, 16.37s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0133, Loss D: -0.0067\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0112, Loss D: -0.0019\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0151, Loss D: 0.0019\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0054, Loss D: -0.0021\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0119, Loss D: 0.0050\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0093, Loss D: -0.0029\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0071, Loss D: 0.0010\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:33<02:15, 16.99s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0090, Loss D: 0.0042\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0139, Loss D: 0.0014\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0130, Loss D: 0.0020\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0106, Loss D: -0.0011\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0073, Loss D: -0.0042\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0117, Loss D: 0.0088\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0080, Loss D: 0.0040\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:50<01:56, 16.71s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0228, Loss D: 0.0001\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0217, Loss D: -0.0062\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0227, Loss D: 0.0037\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0201, Loss D: 0.0064\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0201, Loss D: 0.0107\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0180, Loss D: -0.0023\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0237, Loss D: 0.0052\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:06<01:40, 16.71s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0321, Loss D: -0.0021\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0335, Loss D: 0.0032\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0347, Loss D: -0.0002\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0381, Loss D: -0.0020\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0360, Loss D: 0.0025\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0309, Loss D: -0.0036\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0387, Loss D: 0.0023\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:21<01:20, 16.03s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0293, Loss D: 0.0002\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0406, Loss D: -0.0051\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0343, Loss D: 0.0028\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0308, Loss D: 0.0022\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0351, Loss D: 0.0051\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0354, Loss D: -0.0008\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0329, Loss D: 0.0015\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:38<01:04, 16.20s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0376, Loss D: -0.0033\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0347, Loss D: -0.0001\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0365, Loss D: 0.0020\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0354, Loss D: 0.0021\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0394, Loss D: 0.0071\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0342, Loss D: 0.0012\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0396, Loss D: -0.0072\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:53<00:47, 15.97s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0374, Loss D: -0.0017\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0418, Loss D: -0.0027\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0419, Loss D: 0.0077\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0404, Loss D: -0.0021\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0329, Loss D: -0.0028\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0376, Loss D: -0.0009\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0366, Loss D: -0.0062\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:09<00:31, 15.95s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0660, Loss D: -0.0027\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0652, Loss D: -0.0055\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0669, Loss D: 0.0083\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0633, Loss D: -0.0015\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0636, Loss D: 0.0042\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0660, Loss D: 0.0047\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0608, Loss D: 0.0098\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:25<00:15, 15.95s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0656, Loss D: -0.0038\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0641, Loss D: 0.0003\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0665, Loss D: 0.0040\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0647, Loss D: 0.0009\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0645, Loss D: -0.0049\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0671, Loss D: 0.0097\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0639, Loss D: 0.0005\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:41<00:00, 16.13s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 161.45936107635498\n",
      "Metrics: \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        66  DP-CTGAN (eps=1)  Groundhog       0.6   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain  auc  \\\n",
      "0                 1.0                  0.8            0.2           0.8  0.8   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        66  DP-CTGAN (eps=1)  Groundhog       0.6   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain  auc  \\\n",
      "0                 1.0                  0.8            0.2           0.8  0.8   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0208, Loss D: 0.0063\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0188, Loss D: 0.0001\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0261, Loss D: 0.0023\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0175, Loss D: 0.0009\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0215, Loss D: -0.0095\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0136, Loss D: 0.0010\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0140, Loss D: -0.0019\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:16<02:25, 16.11s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0412, Loss D: 0.0009\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0363, Loss D: 0.0074\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0367, Loss D: -0.0003\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0293, Loss D: 0.0068\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0407, Loss D: 0.0051\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0349, Loss D: 0.0024\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0335, Loss D: 0.0022\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:32<02:09, 16.19s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0736, Loss D: -0.0053\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0736, Loss D: 0.0019\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0700, Loss D: -0.0001\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0688, Loss D: 0.0047\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0696, Loss D: -0.0049\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0645, Loss D: 0.0050\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0734, Loss D: 0.0006\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:48<01:52, 16.01s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0152, Loss D: -0.0039\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0111, Loss D: 0.0006\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0194, Loss D: 0.0053\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0095, Loss D: 0.0027\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0098, Loss D: 0.0034\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0134, Loss D: -0.0014\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0101, Loss D: 0.0050\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:03<01:34, 15.69s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0488, Loss D: -0.0012\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0465, Loss D: -0.0018\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0460, Loss D: 0.0015\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0527, Loss D: 0.0001\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0431, Loss D: 0.0000\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0475, Loss D: 0.0051\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0458, Loss D: 0.0044\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:19<01:19, 15.82s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0684, Loss D: -0.0019\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0706, Loss D: -0.0057\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0698, Loss D: -0.0003\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0695, Loss D: -0.0024\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0748, Loss D: 0.0069\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0694, Loss D: 0.0008\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0664, Loss D: -0.0067\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:36<01:05, 16.41s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0716, Loss D: -0.0065\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0716, Loss D: -0.0048\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0743, Loss D: 0.0079\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0705, Loss D: -0.0049\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0694, Loss D: 0.0031\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0700, Loss D: 0.0035\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0723, Loss D: 0.0069\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:55<00:51, 17.14s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0510, Loss D: -0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0521, Loss D: 0.0015\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0520, Loss D: 0.0046\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0519, Loss D: -0.0019\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0509, Loss D: 0.0002\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0497, Loss D: -0.0044\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0523, Loss D: 0.0018\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:12<00:33, 16.90s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0461, Loss D: -0.0013\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0475, Loss D: -0.0013\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0484, Loss D: 0.0035\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0466, Loss D: 0.0066\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0478, Loss D: -0.0007\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0491, Loss D: 0.0008\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0423, Loss D: 0.0059\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:30<00:17, 17.30s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0502, Loss D: -0.0026\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0507, Loss D: 0.0015\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0498, Loss D: 0.0081\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0476, Loss D: -0.0010\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0486, Loss D: 0.0038\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0492, Loss D: 0.0007\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0485, Loss D: 0.0011\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:47<00:00, 16.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 167.3633131980896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0706, Loss D: -0.0008\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0734, Loss D: 0.0097\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0713, Loss D: -0.0032\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0688, Loss D: 0.0016\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0715, Loss D: -0.0018\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0717, Loss D: 0.0000\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0715, Loss D: 0.0025\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:14<02:13, 14.88s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0575, Loss D: -0.0001\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0522, Loss D: 0.0054\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0554, Loss D: 0.0002\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0540, Loss D: 0.0012\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0540, Loss D: 0.0046\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0564, Loss D: 0.0036\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0522, Loss D: 0.0008\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:29<01:59, 14.94s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0515, Loss D: 0.0043\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0449, Loss D: -0.0009\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0487, Loss D: -0.0001\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0471, Loss D: -0.0059\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0468, Loss D: 0.0007\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0477, Loss D: -0.0043\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0540, Loss D: 0.0017\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:44<01:44, 14.99s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0748, Loss D: -0.0036\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0742, Loss D: 0.0022\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0702, Loss D: 0.0017\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0771, Loss D: 0.0056\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0731, Loss D: -0.0005\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0794, Loss D: -0.0006\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0780, Loss D: 0.0030\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [00:59<01:28, 14.81s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0617, Loss D: 0.0010\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0549, Loss D: -0.0025\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0599, Loss D: 0.0036\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0585, Loss D: 0.0011\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0560, Loss D: -0.0001\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0561, Loss D: 0.0014\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0604, Loss D: -0.0014\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:14<01:14, 14.83s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0658, Loss D: 0.0052\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0670, Loss D: 0.0008\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0637, Loss D: -0.0019\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0664, Loss D: -0.0035\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0617, Loss D: 0.0015\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0678, Loss D: 0.0023\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0639, Loss D: 0.0067\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:28<00:58, 14.65s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0335, Loss D: 0.0029\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0318, Loss D: 0.0050\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0321, Loss D: -0.0031\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0355, Loss D: -0.0001\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0359, Loss D: 0.0022\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0337, Loss D: -0.0004\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0346, Loss D: -0.0014\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:43<00:43, 14.59s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0077, Loss D: 0.0005\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0148, Loss D: 0.0015\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0160, Loss D: 0.0004\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0127, Loss D: 0.0014\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0152, Loss D: 0.0030\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0096, Loss D: 0.0046\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0118, Loss D: 0.0009\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [01:57<00:29, 14.52s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0654, Loss D: 0.0046\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0641, Loss D: 0.0035\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0672, Loss D: -0.0039\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0651, Loss D: 0.0074\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0662, Loss D: 0.0026\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0632, Loss D: -0.0018\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0648, Loss D: 0.0097\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:12<00:14, 14.75s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0478, Loss D: 0.0020\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0443, Loss D: -0.0033\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0426, Loss D: 0.0029\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0498, Loss D: 0.0013\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0464, Loss D: 0.0004\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0492, Loss D: -0.0031\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0459, Loss D: 0.0016\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:27<00:00, 14.76s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 147.71361374855042\n",
      "Metrics: \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer       491  DP-CTGAN (eps=1)  Groundhog       0.9   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain  auc  \\\n",
      "0                 1.0                  0.2            0.8           0.2  0.8   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer       491  DP-CTGAN (eps=1)  Groundhog       0.9   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain  auc  \\\n",
      "0                 1.0                  0.2            0.8           0.2  0.8   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0597, Loss D: -0.0005\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0590, Loss D: 0.0004\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0598, Loss D: 0.0012\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0607, Loss D: 0.0048\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0596, Loss D: 0.0018\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0617, Loss D: -0.0013\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0618, Loss D: 0.0108\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:15<02:16, 15.20s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0379, Loss D: 0.0013\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0390, Loss D: 0.0021\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0370, Loss D: -0.0035\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0359, Loss D: 0.0058\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0398, Loss D: 0.0015\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0322, Loss D: -0.0041\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0356, Loss D: -0.0026\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:31<02:06, 15.77s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0689, Loss D: -0.0030\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0704, Loss D: 0.0010\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0682, Loss D: 0.0017\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0696, Loss D: 0.0050\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0695, Loss D: 0.0009\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0654, Loss D: -0.0003\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0692, Loss D: 0.0060\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:46<01:49, 15.67s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0614, Loss D: 0.0000\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0685, Loss D: 0.0047\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0753, Loss D: -0.0007\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0703, Loss D: 0.0014\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0680, Loss D: -0.0040\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0687, Loss D: -0.0037\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0651, Loss D: -0.0000\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:02<01:34, 15.81s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0702, Loss D: -0.0027\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0715, Loss D: 0.0038\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0691, Loss D: 0.0004\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0726, Loss D: 0.0013\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0724, Loss D: -0.0012\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0730, Loss D: -0.0038\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0687, Loss D: 0.0058\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:18<01:19, 15.86s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0694, Loss D: -0.0077\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0654, Loss D: -0.0045\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0686, Loss D: 0.0016\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0723, Loss D: 0.0096\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0713, Loss D: 0.0020\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0683, Loss D: 0.0048\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0739, Loss D: 0.0060\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:34<01:03, 15.77s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0640, Loss D: -0.0009\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0628, Loss D: 0.0034\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0633, Loss D: -0.0003\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0642, Loss D: -0.0014\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0635, Loss D: 0.0041\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0652, Loss D: 0.0001\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0631, Loss D: -0.0015\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:50<00:47, 15.87s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0633, Loss D: -0.0023\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0646, Loss D: 0.0062\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0652, Loss D: 0.0008\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0625, Loss D: 0.0069\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0648, Loss D: -0.0023\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0668, Loss D: -0.0001\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0656, Loss D: -0.0016\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:05<00:31, 15.73s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0252, Loss D: -0.0079\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0221, Loss D: 0.0020\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0233, Loss D: 0.0012\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0233, Loss D: 0.0073\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0186, Loss D: 0.0031\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0216, Loss D: -0.0022\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0198, Loss D: 0.0014\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:22<00:15, 15.97s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0333, Loss D: 0.0002\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0346, Loss D: 0.0037\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0323, Loss D: -0.0052\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0328, Loss D: 0.0004\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0381, Loss D: 0.0025\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0333, Loss D: 0.0027\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0359, Loss D: 0.0082\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:38<00:00, 15.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 158.7164340019226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0384, Loss D: 0.0022\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0330, Loss D: -0.0004\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0361, Loss D: 0.0061\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0356, Loss D: -0.0011\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0370, Loss D: 0.0079\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0363, Loss D: -0.0000\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0371, Loss D: 0.0061\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:15<02:21, 15.72s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0341, Loss D: 0.0051\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0324, Loss D: 0.0003\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0360, Loss D: 0.0051\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0353, Loss D: 0.0050\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0355, Loss D: 0.0030\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0360, Loss D: 0.0083\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0326, Loss D: 0.0006\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:31<02:04, 15.57s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0537, Loss D: 0.0065\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0528, Loss D: 0.0024\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0526, Loss D: -0.0018\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0509, Loss D: -0.0019\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0542, Loss D: 0.0057\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0502, Loss D: 0.0023\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0505, Loss D: -0.0067\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:47<01:50, 15.73s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0167, Loss D: 0.0012\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0172, Loss D: 0.0003\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0174, Loss D: -0.0008\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0109, Loss D: -0.0042\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0156, Loss D: -0.0045\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0210, Loss D: -0.0018\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0127, Loss D: 0.0051\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:04<01:37, 16.30s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0330, Loss D: 0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0356, Loss D: -0.0025\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0333, Loss D: 0.0071\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0347, Loss D: -0.0001\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0352, Loss D: 0.0008\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0357, Loss D: 0.0021\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0324, Loss D: 0.0016\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:21<01:23, 16.66s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0364, Loss D: 0.0017\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0315, Loss D: 0.0049\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0367, Loss D: 0.0003\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0341, Loss D: -0.0009\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0346, Loss D: 0.0059\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0359, Loss D: 0.0018\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0358, Loss D: -0.0066\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:38<01:07, 16.84s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0667, Loss D: 0.0034\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0619, Loss D: 0.0048\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0659, Loss D: -0.0023\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0634, Loss D: 0.0017\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0661, Loss D: -0.0002\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0687, Loss D: 0.0070\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0634, Loss D: 0.0056\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:54<00:49, 16.51s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0677, Loss D: 0.0024\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0692, Loss D: 0.0024\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0648, Loss D: -0.0008\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0664, Loss D: 0.0021\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0681, Loss D: 0.0083\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0641, Loss D: 0.0018\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0642, Loss D: 0.0105\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:11<00:33, 16.60s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0385, Loss D: -0.0055\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0362, Loss D: -0.0036\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0349, Loss D: -0.0015\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0382, Loss D: 0.0008\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0365, Loss D: -0.0011\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0386, Loss D: 0.0035\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0362, Loss D: -0.0008\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:26<00:16, 16.27s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0240, Loss D: -0.0003\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0272, Loss D: -0.0049\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0292, Loss D: -0.0037\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0306, Loss D: -0.0015\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0246, Loss D: -0.0051\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0282, Loss D: -0.0013\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0268, Loss D: -0.0072\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:42<00:00, 16.28s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 162.93203592300415\n",
      "Metrics: \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        73  DP-CTGAN (eps=1)  Groundhog       0.7   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.4                  0.0            0.4           0.6  0.92   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        73  DP-CTGAN (eps=1)  Groundhog       0.7   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.4                  0.0            0.4           0.6  0.92   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0415, Loss D: 0.0036\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0380, Loss D: -0.0052\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0343, Loss D: 0.0020\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0324, Loss D: 0.0053\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0335, Loss D: -0.0009\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0355, Loss D: 0.0027\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0349, Loss D: 0.0013\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:15<02:21, 15.77s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0520, Loss D: -0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0491, Loss D: 0.0030\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0491, Loss D: 0.0078\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0449, Loss D: 0.0053\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0463, Loss D: 0.0036\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0440, Loss D: 0.0017\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0499, Loss D: 0.0070\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:31<02:06, 15.80s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0197, Loss D: 0.0005\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0257, Loss D: -0.0027\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0273, Loss D: 0.0007\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0189, Loss D: -0.0014\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0257, Loss D: 0.0059\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0202, Loss D: -0.0013\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0249, Loss D: 0.0014\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:47<01:50, 15.73s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0359, Loss D: -0.0046\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0347, Loss D: -0.0005\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0330, Loss D: -0.0070\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0382, Loss D: 0.0037\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0332, Loss D: -0.0008\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0376, Loss D: 0.0102\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0351, Loss D: -0.0079\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:03<01:36, 16.06s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0759, Loss D: 0.0015\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0794, Loss D: -0.0056\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0787, Loss D: -0.0059\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0751, Loss D: 0.0023\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0818, Loss D: -0.0003\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0786, Loss D: -0.0005\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0847, Loss D: 0.0045\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:19<01:19, 16.00s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0776, Loss D: -0.0049\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0751, Loss D: -0.0043\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0801, Loss D: -0.0004\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0783, Loss D: -0.0023\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0760, Loss D: 0.0038\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0728, Loss D: -0.0017\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0779, Loss D: 0.0013\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:36<01:04, 16.11s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0361, Loss D: -0.0011\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0404, Loss D: 0.0011\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0386, Loss D: 0.0067\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0384, Loss D: -0.0056\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0366, Loss D: 0.0057\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0337, Loss D: 0.0051\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0380, Loss D: -0.0016\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:51<00:47, 15.85s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0963, Loss D: 0.0005\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0978, Loss D: 0.0008\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0908, Loss D: -0.0055\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0961, Loss D: 0.0037\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0999, Loss D: 0.0064\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.1004, Loss D: 0.0042\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0950, Loss D: 0.0076\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:07<00:31, 15.98s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0130, Loss D: 0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0103, Loss D: -0.0009\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0085, Loss D: -0.0020\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0135, Loss D: 0.0006\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0156, Loss D: 0.0070\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0141, Loss D: 0.0015\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0146, Loss D: 0.0030\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:23<00:16, 16.10s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0368, Loss D: -0.0016\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0331, Loss D: -0.0018\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0359, Loss D: 0.0003\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0365, Loss D: -0.0026\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0339, Loss D: 0.0031\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0356, Loss D: 0.0075\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0360, Loss D: 0.0049\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:39<00:00, 15.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 159.61027812957764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0374, Loss D: 0.0008\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0388, Loss D: -0.0049\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0375, Loss D: -0.0070\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0368, Loss D: -0.0019\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0360, Loss D: 0.0033\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0367, Loss D: -0.0025\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0362, Loss D: 0.0012\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:15<02:23, 15.93s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0654, Loss D: 0.0017\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0732, Loss D: 0.0005\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0697, Loss D: 0.0089\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0693, Loss D: 0.0006\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0714, Loss D: 0.0042\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0719, Loss D: 0.0027\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0670, Loss D: 0.0005\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:31<02:03, 15.45s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0349, Loss D: 0.0008\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0375, Loss D: 0.0039\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0323, Loss D: -0.0025\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0342, Loss D: 0.0088\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0388, Loss D: 0.0134\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0387, Loss D: 0.0006\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0399, Loss D: -0.0031\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:47<01:51, 15.91s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0552, Loss D: -0.0033\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0568, Loss D: 0.0040\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0541, Loss D: 0.0016\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0537, Loss D: -0.0000\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0498, Loss D: -0.0015\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0520, Loss D: -0.0019\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0536, Loss D: 0.0050\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:05<01:40, 16.67s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0413, Loss D: 0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0365, Loss D: 0.0027\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0391, Loss D: 0.0018\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0363, Loss D: 0.0031\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0371, Loss D: 0.0021\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0392, Loss D: 0.0093\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0405, Loss D: -0.0012\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:21<01:22, 16.51s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0546, Loss D: -0.0005\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0532, Loss D: 0.0010\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0577, Loss D: 0.0006\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0534, Loss D: 0.0049\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0551, Loss D: 0.0016\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0552, Loss D: -0.0030\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0531, Loss D: 0.0070\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:37<01:05, 16.27s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0334, Loss D: -0.0025\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0339, Loss D: 0.0026\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0317, Loss D: -0.0057\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0379, Loss D: -0.0014\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0376, Loss D: -0.0011\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0335, Loss D: -0.0052\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0363, Loss D: 0.0025\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:54<00:49, 16.51s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0306, Loss D: -0.0039\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0308, Loss D: -0.0032\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0316, Loss D: 0.0010\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0323, Loss D: 0.0018\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0369, Loss D: -0.0002\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0377, Loss D: -0.0065\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0360, Loss D: 0.0062\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:11<00:33, 16.57s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0369, Loss D: 0.0065\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0337, Loss D: 0.0016\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0343, Loss D: -0.0006\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0396, Loss D: 0.0040\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0344, Loss D: 0.0026\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0356, Loss D: -0.0026\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0353, Loss D: 0.0028\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:29<00:17, 17.11s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0392, Loss D: -0.0029\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0378, Loss D: 0.0013\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0381, Loss D: -0.0001\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0384, Loss D: 0.0060\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0388, Loss D: 0.0021\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0418, Loss D: 0.0066\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0378, Loss D: 0.0015\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:48<00:00, 16.86s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 168.6602189540863\n",
      "Metrics: \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        57  DP-CTGAN (eps=1)  Groundhog       0.4   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.2                  0.4           -0.2           1.2  0.34   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                0.0  \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        57  DP-CTGAN (eps=1)  Groundhog       0.4   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.2                  0.4           -0.2           1.2  0.34   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0301, Loss D: -0.0044\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0355, Loss D: -0.0015\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0337, Loss D: -0.0034\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0309, Loss D: -0.0018\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0299, Loss D: -0.0068\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0337, Loss D: -0.0020\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0335, Loss D: 0.0076\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:16<02:28, 16.55s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0446, Loss D: -0.0057\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0471, Loss D: 0.0053\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0504, Loss D: 0.0078\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0437, Loss D: 0.0004\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0487, Loss D: 0.0031\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0505, Loss D: 0.0027\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0475, Loss D: 0.0001\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:32<02:11, 16.44s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0715, Loss D: 0.0025\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0683, Loss D: 0.0015\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0701, Loss D: -0.0005\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0727, Loss D: -0.0024\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0694, Loss D: -0.0010\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0683, Loss D: 0.0062\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0706, Loss D: 0.0075\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:48<01:53, 16.27s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0484, Loss D: -0.0023\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0520, Loss D: -0.0074\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0541, Loss D: 0.0055\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0549, Loss D: -0.0026\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0527, Loss D: -0.0025\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0565, Loss D: -0.0062\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0559, Loss D: 0.0062\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:05<01:38, 16.34s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0363, Loss D: 0.0008\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0392, Loss D: 0.0015\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0429, Loss D: 0.0029\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0412, Loss D: 0.0048\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0393, Loss D: -0.0016\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0446, Loss D: -0.0033\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0407, Loss D: 0.0008\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:21<01:22, 16.41s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0274, Loss D: 0.0014\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0287, Loss D: 0.0010\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0298, Loss D: 0.0026\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0260, Loss D: -0.0063\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0344, Loss D: 0.0026\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0260, Loss D: 0.0029\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0308, Loss D: -0.0042\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:38<01:05, 16.42s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0076, Loss D: -0.0015\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0075, Loss D: -0.0046\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0077, Loss D: -0.0035\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0090, Loss D: 0.0029\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0098, Loss D: 0.0017\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0026, Loss D: 0.0000\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0130, Loss D: 0.0009\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:54<00:49, 16.42s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0365, Loss D: 0.0008\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0331, Loss D: -0.0067\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0344, Loss D: -0.0045\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0383, Loss D: 0.0058\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0408, Loss D: 0.0015\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0355, Loss D: 0.0035\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0346, Loss D: -0.0006\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:11<00:32, 16.43s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0245, Loss D: -0.0020\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0231, Loss D: -0.0008\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0258, Loss D: 0.0004\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0190, Loss D: -0.0025\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0241, Loss D: -0.0024\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0211, Loss D: 0.0016\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0219, Loss D: -0.0017\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:27<00:16, 16.35s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0524, Loss D: -0.0077\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0476, Loss D: -0.0001\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0489, Loss D: 0.0048\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0484, Loss D: -0.0016\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0466, Loss D: -0.0028\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0469, Loss D: -0.0003\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0445, Loss D: 0.0016\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:44<00:00, 16.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 164.41669487953186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0351, Loss D: -0.0015\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0389, Loss D: 0.0002\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0359, Loss D: -0.0016\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0365, Loss D: 0.0088\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0359, Loss D: 0.0006\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0401, Loss D: -0.0016\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0354, Loss D: -0.0010\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:16<02:29, 16.61s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0513, Loss D: -0.0013\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0534, Loss D: 0.0060\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0528, Loss D: 0.0016\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0558, Loss D: -0.0071\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0499, Loss D: 0.0060\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0536, Loss D: 0.0013\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0537, Loss D: 0.0011\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:33<02:12, 16.62s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0164, Loss D: 0.0048\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0086, Loss D: 0.0054\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0150, Loss D: -0.0021\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0172, Loss D: 0.0120\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0165, Loss D: 0.0010\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0104, Loss D: 0.0032\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0142, Loss D: 0.0092\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:49<01:56, 16.62s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0320, Loss D: 0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0295, Loss D: 0.0034\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0340, Loss D: 0.0008\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0311, Loss D: 0.0010\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0381, Loss D: 0.0042\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0290, Loss D: 0.0026\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0322, Loss D: 0.0028\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:06<01:39, 16.59s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0345, Loss D: 0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0340, Loss D: 0.0014\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0330, Loss D: 0.0032\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0348, Loss D: -0.0040\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0313, Loss D: 0.0014\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0366, Loss D: 0.0004\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0368, Loss D: 0.0031\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:22<01:22, 16.58s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0434, Loss D: -0.0030\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0504, Loss D: 0.0002\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0524, Loss D: 0.0034\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0512, Loss D: 0.0010\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0494, Loss D: 0.0029\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0472, Loss D: 0.0021\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0465, Loss D: 0.0062\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:39<01:06, 16.58s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0657, Loss D: 0.0005\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0688, Loss D: 0.0041\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0599, Loss D: -0.0014\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0691, Loss D: -0.0063\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0623, Loss D: 0.0052\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0642, Loss D: 0.0030\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0617, Loss D: -0.0042\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:56<00:50, 16.72s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0510, Loss D: -0.0059\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0511, Loss D: 0.0021\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0516, Loss D: -0.0017\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0469, Loss D: -0.0008\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0473, Loss D: 0.0045\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0453, Loss D: -0.0018\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0453, Loss D: 0.0005\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:13<00:33, 16.67s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0374, Loss D: -0.0023\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0376, Loss D: -0.0012\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0372, Loss D: -0.0038\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0405, Loss D: 0.0017\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0401, Loss D: 0.0025\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0390, Loss D: 0.0024\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0404, Loss D: 0.0020\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:29<00:16, 16.60s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0407, Loss D: 0.0023\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0351, Loss D: 0.0014\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0393, Loss D: -0.0021\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0372, Loss D: 0.0028\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0357, Loss D: -0.0000\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0421, Loss D: 0.0023\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0378, Loss D: 0.0025\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:45<00:00, 16.60s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 166.0838611125946\n",
      "Metrics: \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer       491  DP-CTGAN (eps=1)  Groundhog       0.6   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 1.0                  0.8            0.2           0.8  0.78   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer       491  DP-CTGAN (eps=1)  Groundhog       0.6   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 1.0                  0.8            0.2           0.8  0.78   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0401, Loss D: -0.0031\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0453, Loss D: -0.0002\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0477, Loss D: -0.0071\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0369, Loss D: 0.0045\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0443, Loss D: -0.0050\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0413, Loss D: 0.0097\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0454, Loss D: 0.0024\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:16<02:30, 16.76s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0183, Loss D: -0.0041\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0186, Loss D: 0.0081\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0224, Loss D: 0.0012\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0197, Loss D: 0.0006\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0178, Loss D: -0.0016\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0150, Loss D: 0.0047\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0150, Loss D: 0.0034\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:33<02:14, 16.78s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0198, Loss D: 0.0043\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0174, Loss D: -0.0031\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0130, Loss D: 0.0048\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0123, Loss D: -0.0014\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0150, Loss D: 0.0049\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0122, Loss D: -0.0038\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0134, Loss D: -0.0018\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:50<01:56, 16.71s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0137, Loss D: 0.0018\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0166, Loss D: 0.0003\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0105, Loss D: 0.0026\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0181, Loss D: -0.0034\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0139, Loss D: -0.0013\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0111, Loss D: -0.0049\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0152, Loss D: 0.0022\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:06<01:39, 16.66s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0537, Loss D: -0.0023\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0559, Loss D: -0.0021\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0549, Loss D: 0.0050\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0597, Loss D: 0.0064\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0501, Loss D: -0.0034\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0511, Loss D: -0.0039\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0550, Loss D: 0.0086\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:24<01:24, 16.99s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0404, Loss D: 0.0029\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0447, Loss D: 0.0002\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0420, Loss D: 0.0024\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0408, Loss D: 0.0001\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0425, Loss D: -0.0011\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0436, Loss D: 0.0063\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0413, Loss D: -0.0009\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:41<01:08, 17.06s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0371, Loss D: 0.0024\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0331, Loss D: 0.0039\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0383, Loss D: 0.0071\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0328, Loss D: 0.0036\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0271, Loss D: -0.0010\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0377, Loss D: -0.0035\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0348, Loss D: 0.0027\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:59<00:51, 17.30s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0424, Loss D: 0.0022\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0452, Loss D: -0.0013\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0426, Loss D: 0.0058\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0419, Loss D: 0.0013\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0464, Loss D: 0.0040\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0452, Loss D: -0.0010\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0457, Loss D: 0.0086\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:18<00:35, 17.76s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0342, Loss D: 0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0349, Loss D: 0.0019\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0349, Loss D: -0.0006\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0375, Loss D: -0.0004\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0355, Loss D: 0.0019\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0338, Loss D: 0.0041\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0323, Loss D: 0.0052\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:35<00:17, 17.63s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0470, Loss D: 0.0008\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0466, Loss D: 0.0005\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0475, Loss D: -0.0023\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0479, Loss D: 0.0062\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0509, Loss D: 0.0069\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0460, Loss D: 0.0079\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0467, Loss D: -0.0069\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:52<00:00, 17.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 172.77852964401245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0415, Loss D: 0.0027\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0452, Loss D: -0.0028\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0425, Loss D: -0.0018\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0424, Loss D: -0.0004\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0419, Loss D: -0.0070\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0391, Loss D: 0.0092\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0461, Loss D: 0.0027\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:16<02:26, 16.24s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0554, Loss D: 0.0086\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0550, Loss D: -0.0005\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0534, Loss D: 0.0004\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0539, Loss D: -0.0002\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0519, Loss D: 0.0024\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0533, Loss D: 0.0043\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0532, Loss D: -0.0037\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:32<02:11, 16.48s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0666, Loss D: 0.0021\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0648, Loss D: -0.0001\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0640, Loss D: 0.0048\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0618, Loss D: 0.0046\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0634, Loss D: 0.0016\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0621, Loss D: 0.0073\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0609, Loss D: 0.0056\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:49<01:55, 16.47s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0485, Loss D: -0.0005\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0494, Loss D: 0.0010\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0492, Loss D: -0.0042\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0463, Loss D: -0.0030\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0468, Loss D: 0.0077\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0479, Loss D: -0.0033\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0455, Loss D: 0.0007\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:07<01:43, 17.27s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0483, Loss D: 0.0017\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0437, Loss D: 0.0015\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0487, Loss D: -0.0013\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0454, Loss D: -0.0022\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0440, Loss D: 0.0016\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0470, Loss D: -0.0040\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0477, Loss D: -0.0008\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:24<01:25, 17.06s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0678, Loss D: 0.0044\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0668, Loss D: 0.0040\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0654, Loss D: 0.0005\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0641, Loss D: -0.0008\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0607, Loss D: 0.0055\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0625, Loss D: 0.0044\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0674, Loss D: -0.0013\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:40<01:07, 16.77s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0336, Loss D: -0.0044\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0365, Loss D: -0.0077\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0356, Loss D: 0.0035\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0353, Loss D: 0.0038\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0328, Loss D: -0.0001\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0361, Loss D: -0.0044\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0322, Loss D: 0.0078\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:57<00:50, 16.75s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0504, Loss D: -0.0015\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0511, Loss D: -0.0010\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0472, Loss D: -0.0009\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0500, Loss D: 0.0121\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0492, Loss D: 0.0049\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0494, Loss D: -0.0023\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0469, Loss D: -0.0047\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:13<00:33, 16.61s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0074, Loss D: -0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0104, Loss D: 0.0020\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0155, Loss D: -0.0005\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0117, Loss D: -0.0026\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0087, Loss D: 0.0108\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0141, Loss D: -0.0016\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0156, Loss D: 0.0051\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:29<00:16, 16.38s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0517, Loss D: 0.0004\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0525, Loss D: 0.0008\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0492, Loss D: 0.0015\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0512, Loss D: 0.0039\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0493, Loss D: 0.0040\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0525, Loss D: -0.0005\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0535, Loss D: -0.0019\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:45<00:00, 16.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 165.61830806732178\n",
      "Metrics: \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer       550  DP-CTGAN (eps=1)  Groundhog       0.4   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.2                  0.4           -0.2           1.2  0.42   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer       550  DP-CTGAN (eps=1)  Groundhog       0.4   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.2                  0.4           -0.2           1.2  0.42   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    }
   ],
   "source": [
    "# Calling the main function and printing all metrics and summaries from training and testing the models\n",
    "all_metrics, all_summaries = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "974201a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          dataset target_id         generator     attack  accuracy  \\\n",
      "0   Breast Cancer        84               Raw  Groundhog       0.6   \n",
      "1   Breast Cancer        70               Raw  Groundhog       0.6   \n",
      "2   Breast Cancer        70               Raw  Groundhog       0.5   \n",
      "3   Breast Cancer        90               Raw  Groundhog       0.7   \n",
      "4   Breast Cancer        66               Raw  Groundhog       0.5   \n",
      "5   Breast Cancer       491               Raw  Groundhog       0.5   \n",
      "6   Breast Cancer        73               Raw  Groundhog       0.7   \n",
      "7   Breast Cancer        57               Raw  Groundhog       0.7   \n",
      "8   Breast Cancer       491               Raw  Groundhog       0.9   \n",
      "9   Breast Cancer       550               Raw  Groundhog       0.6   \n",
      "10  Breast Cancer        84             CTGAN  Groundhog       1.0   \n",
      "11  Breast Cancer        70             CTGAN  Groundhog       1.0   \n",
      "12  Breast Cancer        70             CTGAN  Groundhog       0.9   \n",
      "13  Breast Cancer        90             CTGAN  Groundhog       1.0   \n",
      "14  Breast Cancer        66             CTGAN  Groundhog       1.0   \n",
      "15  Breast Cancer       491             CTGAN  Groundhog       1.0   \n",
      "16  Breast Cancer        73             CTGAN  Groundhog       0.8   \n",
      "17  Breast Cancer        57             CTGAN  Groundhog       1.0   \n",
      "18  Breast Cancer       491             CTGAN  Groundhog       1.0   \n",
      "19  Breast Cancer       550             CTGAN  Groundhog       1.0   \n",
      "20  Breast Cancer        84  DP-CTGAN (eps=1)  Groundhog       0.7   \n",
      "21  Breast Cancer        70  DP-CTGAN (eps=1)  Groundhog       0.6   \n",
      "22  Breast Cancer        70  DP-CTGAN (eps=1)  Groundhog       0.5   \n",
      "23  Breast Cancer        90  DP-CTGAN (eps=1)  Groundhog       0.5   \n",
      "24  Breast Cancer        66  DP-CTGAN (eps=1)  Groundhog       0.6   \n",
      "25  Breast Cancer       491  DP-CTGAN (eps=1)  Groundhog       0.9   \n",
      "26  Breast Cancer        73  DP-CTGAN (eps=1)  Groundhog       0.7   \n",
      "27  Breast Cancer        57  DP-CTGAN (eps=1)  Groundhog       0.4   \n",
      "28  Breast Cancer       491  DP-CTGAN (eps=1)  Groundhog       0.6   \n",
      "29  Breast Cancer       550  DP-CTGAN (eps=1)  Groundhog       0.4   \n",
      "\n",
      "    true_positive_rate  false_positive_rate  mia_advantage  privacy_gain  \\\n",
      "0                  0.6                  0.4            0.2           0.8   \n",
      "1                  0.6                  0.4            0.2           0.8   \n",
      "2                  0.0                  0.0            0.0           1.0   \n",
      "3                  1.0                  0.6            0.4           0.6   \n",
      "4                  0.2                  0.2            0.0           1.0   \n",
      "5                  1.0                  1.0            0.0           1.0   \n",
      "6                  1.0                  0.6            0.4           0.6   \n",
      "7                  1.0                  0.6            0.4           0.6   \n",
      "8                  0.8                  0.0            0.8           0.2   \n",
      "9                  1.0                  0.8            0.2           0.8   \n",
      "10                 1.0                  0.0            1.0           0.0   \n",
      "11                 1.0                  0.0            1.0           0.0   \n",
      "12                 1.0                  0.2            0.8           0.2   \n",
      "13                 1.0                  0.0            1.0           0.0   \n",
      "14                 1.0                  0.0            1.0           0.0   \n",
      "15                 1.0                  0.0            1.0           0.0   \n",
      "16                 1.0                  0.4            0.6           0.4   \n",
      "17                 1.0                  0.0            1.0           0.0   \n",
      "18                 1.0                  0.0            1.0           0.0   \n",
      "19                 1.0                  0.0            1.0           0.0   \n",
      "20                 0.6                  0.2            0.4           0.6   \n",
      "21                 0.6                  0.4            0.2           0.8   \n",
      "22                 0.4                  0.4            0.0           1.0   \n",
      "23                 1.0                  1.0            0.0           1.0   \n",
      "24                 1.0                  0.8            0.2           0.8   \n",
      "25                 1.0                  0.2            0.8           0.2   \n",
      "26                 0.4                  0.0            0.4           0.6   \n",
      "27                 0.2                  0.4           -0.2           1.2   \n",
      "28                 1.0                  0.8            0.2           0.8   \n",
      "29                 0.2                  0.4           -0.2           1.2   \n",
      "\n",
      "     auc  effective_epsilon  \n",
      "0   0.72                inf  \n",
      "1   0.68                inf  \n",
      "2   0.60                inf  \n",
      "3   0.76                inf  \n",
      "4   0.66           1.386294  \n",
      "5   0.92                inf  \n",
      "6   0.88                inf  \n",
      "7   0.86           1.609438  \n",
      "8   0.94                inf  \n",
      "9   0.84                inf  \n",
      "10  1.00                inf  \n",
      "11  1.00                inf  \n",
      "12  1.00                inf  \n",
      "13  1.00                inf  \n",
      "14  1.00                inf  \n",
      "15  1.00                inf  \n",
      "16  1.00                inf  \n",
      "17  1.00                inf  \n",
      "18  1.00                inf  \n",
      "19  1.00                inf  \n",
      "20  0.74                inf  \n",
      "21  0.78                inf  \n",
      "22  0.58                inf  \n",
      "23  0.56           0.287682  \n",
      "24  0.80                inf  \n",
      "25  0.80                inf  \n",
      "26  0.92                inf  \n",
      "27  0.34           0.000000  \n",
      "28  0.78                inf  \n",
      "29  0.42                inf  \n",
      "[<tapas.report.attack_summary.MIAttackSummary object at 0x157c7eeb0>, <tapas.report.attack_summary.MIAttackSummary object at 0x158db7dc0>, <tapas.report.attack_summary.MIAttackSummary object at 0x157fec7f0>, <tapas.report.attack_summary.MIAttackSummary object at 0x158dbd430>, <tapas.report.attack_summary.MIAttackSummary object at 0x158c26f40>, <tapas.report.attack_summary.MIAttackSummary object at 0x158d9dfd0>, <tapas.report.attack_summary.MIAttackSummary object at 0x158c09b20>, <tapas.report.attack_summary.MIAttackSummary object at 0x158da41c0>, <tapas.report.attack_summary.MIAttackSummary object at 0x158c09760>, <tapas.report.attack_summary.MIAttackSummary object at 0x158d37fa0>, <tapas.report.attack_summary.MIAttackSummary object at 0x15e7c1520>, <tapas.report.attack_summary.MIAttackSummary object at 0x15e7c8790>, <tapas.report.attack_summary.MIAttackSummary object at 0x15e5bfcd0>, <tapas.report.attack_summary.MIAttackSummary object at 0x158d8e250>, <tapas.report.attack_summary.MIAttackSummary object at 0x15e19e4c0>, <tapas.report.attack_summary.MIAttackSummary object at 0x15e5b4460>, <tapas.report.attack_summary.MIAttackSummary object at 0x15e4213d0>, <tapas.report.attack_summary.MIAttackSummary object at 0x158d1a2b0>, <tapas.report.attack_summary.MIAttackSummary object at 0x158d1a070>, <tapas.report.attack_summary.MIAttackSummary object at 0x15e7ed580>, <tapas.report.attack_summary.MIAttackSummary object at 0x158ba03a0>, <tapas.report.attack_summary.MIAttackSummary object at 0x176892760>, <tapas.report.attack_summary.MIAttackSummary object at 0x15e925220>, <tapas.report.attack_summary.MIAttackSummary object at 0x17686b850>, <tapas.report.attack_summary.MIAttackSummary object at 0x176c4c640>, <tapas.report.attack_summary.MIAttackSummary object at 0x176c4c0a0>, <tapas.report.attack_summary.MIAttackSummary object at 0x176dc1fa0>, <tapas.report.attack_summary.MIAttackSummary object at 0x176d2eaf0>, <tapas.report.attack_summary.MIAttackSummary object at 0x15e6e44f0>, <tapas.report.attack_summary.MIAttackSummary object at 0x176c29a00>]\n"
     ]
    }
   ],
   "source": [
    "# Printing all the collected metrics and summaries \n",
    "print(all_metrics)\n",
    "print(all_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0431890",
   "metadata": {},
   "source": [
    "## Results and Plots\n",
    "\n",
    "### Description\n",
    "Contains utilities to generate results and produce plots/reports to compare generators of random targets and compare attacks for different target record types of random and outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27694802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates and saves the report of comparison of generators on random targets\n",
    "def generateReport_ComparisonOfGeneratorsOnRandomTargets():\n",
    "    num_attacks = all_metrics.shape[0]\n",
    "    random_indices = [num for i in range(0, num_attacks, num_targets*2) for num in range(i, i+num_targets)]\n",
    "    report = tapas.report.BinaryLabelAttackReport(all_metrics.iloc[random_indices])\n",
    "    report.metrics = [\"privacy_gain\", \"auc\", \"effective_epsilon\"]\n",
    "    report.compare(comparison_column='generator', fixed_pair_columns=['attack', 'dataset'], marker_column='target_id', filepath=\"../figures_cancer_synthetic_Dec24/cancer_compare_generators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c581376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/numpy/lib/function_base.py:4527: RuntimeWarning: invalid value encountered in subtract\n",
      "  diff_b_a = subtract(b, a)\n"
     ]
    }
   ],
   "source": [
    "generateReport_ComparisonOfGeneratorsOnRandomTargets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d008c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates report that compares attack for different target record types of random and outlier\n",
    "def generateReport_CompareAttacksForDifferentTargetRecordTypes():\n",
    "    len_generators = 3\n",
    "    all_metrics['target_type'] = (['Random']*num_targets + ['Outlier']*num_targets) * len_generators\n",
    "    report = tapas.report.BinaryLabelAttackReport(all_metrics)\n",
    "    report.metrics = [\"privacy_gain\", \"auc\"]\n",
    "    report.compare(comparison_column='generator', fixed_pair_columns=['attack', 'dataset'], marker_column='target_type', filepath=\"../figures_cancer_synthetic_Dec24/Cancer_random_versus_outlier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f945e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "generateReport_CompareAttacksForDifferentTargetRecordTypes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
