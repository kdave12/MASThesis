{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e947b7",
   "metadata": {},
   "source": [
    "# MAS Thesis Notebook\n",
    "## Krishna Dave (UID: 905636874)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a16f151",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook contains example code for privacy auditing using the TAPAS toolbox of synthetic data generated from the Breast Cancer Wisconsin Diagnostic Data (https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic). This toolbox is used for evaluating the privacy of synthetic data using adversarial techniques. The code for the TAPAS toolbox is from: https://github.com/alan-turing-institute/tapas "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd24a565",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "1. Install poetry (system-wide) from https://python-poetry.org/docs/ \n",
    "2. It can also be installed using pip: `pip install git+https://github.com/alan-turing-institute/privacy-sdg-toolbox`\n",
    "3. Activate virtual environment inside the Jupyter notebook using `poetry shell`\n",
    "4. Add the virtual environment to the available kernels for the notebook.\n",
    "5. Make sure that this notebook is located in the correct directory inside the wider directory to ensure the correctness of the relative imports and file paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b157cb5",
   "metadata": {},
   "source": [
    "### Setup: Installation of other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5309b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.python.org/simple, https://pypi.apple.com/simple\n",
      "Requirement already satisfied: be-great in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (0.0.7)\n",
      "Requirement already satisfied: datasets>=2.5.2 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from be-great) (2.16.0)\n",
      "Requirement already satisfied: numpy>=1.23.1 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from be-great) (1.23.4)\n",
      "Requirement already satisfied: pandas>=1.4.4 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from be-great) (1.5.0)\n",
      "Requirement already satisfied: scikit-learn>=1.1.1 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from be-great) (1.1.2)\n",
      "Requirement already satisfied: torch>=1.10.2 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from be-great) (1.12.1)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from be-great) (4.64.1)\n",
      "Requirement already satisfied: transformers>=4.22.1 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from be-great) (4.36.2)\n",
      "Requirement already satisfied: accelerate>=0.20.1 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from be-great) (0.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from accelerate>=0.20.1->be-great) (21.3)\n",
      "Requirement already satisfied: psutil in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from accelerate>=0.20.1->be-great) (5.9.2)\n",
      "Requirement already satisfied: pyyaml in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from accelerate>=0.20.1->be-great) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from accelerate>=0.20.1->be-great) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from accelerate>=0.20.1->be-great) (0.4.1)\n",
      "Requirement already satisfied: filelock in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from datasets>=2.5.2->be-great) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from datasets>=2.5.2->be-great) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from datasets>=2.5.2->be-great) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from datasets>=2.5.2->be-great) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from datasets>=2.5.2->be-great) (2.28.1)\n",
      "Requirement already satisfied: xxhash in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from datasets>=2.5.2->be-great) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from datasets>=2.5.2->be-great) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets>=2.5.2->be-great) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from datasets>=2.5.2->be-great) (3.9.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from pandas>=1.4.4->be-great) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from pandas>=1.4.4->be-great) (2022.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from scikit-learn>=1.1.1->be-great) (1.9.2)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from scikit-learn>=1.1.1->be-great) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from scikit-learn>=1.1.1->be-great) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from torch>=1.10.2->be-great) (4.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from transformers>=4.22.1->be-great) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from transformers>=4.22.1->be-great) (0.15.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from aiohttp->datasets>=2.5.2->be-great) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from aiohttp->datasets>=2.5.2->be-great) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from aiohttp->datasets>=2.5.2->be-great) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from aiohttp->datasets>=2.5.2->be-great) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from aiohttp->datasets>=2.5.2->be-great) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from aiohttp->datasets>=2.5.2->be-great) (4.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from packaging>=20.0->accelerate>=0.20.1->be-great) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=1.4.4->be-great) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=2.5.2->be-great) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=2.5.2->be-great) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=2.5.2->be-great) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=2.5.2->be-great) (2022.9.24)\n"
     ]
    }
   ],
   "source": [
    "!pip install be-great\n",
    "!pip install --upgrade pip --quiet\n",
    "!pip install --upgrade ctgan --quiet\n",
    "!pip install sdv opacus autodp smartnoise-synth --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f823005a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                       Version     Editable project location\n",
      "----------------------------- ----------- ----------------------------------------------------------------------------------------------------------------\n",
      "absl-py                       1.3.0\n",
      "accelerate                    0.25.0\n",
      "aiohttp                       3.9.1\n",
      "aiosignal                     1.3.1\n",
      "alabaster                     0.7.12\n",
      "altgraph                      0.17.3\n",
      "antlr4-python3-runtime        4.9.3\n",
      "anyio                         3.6.2\n",
      "appnope                       0.1.3\n",
      "argon2-cffi                   21.3.0\n",
      "argon2-cffi-bindings          21.2.0\n",
      "asttokens                     2.2.0\n",
      "astunparse                    1.6.3\n",
      "async-timeout                 4.0.3\n",
      "attrs                         22.1.0\n",
      "autodp                        0.2.3.1\n",
      "Babel                         2.10.3\n",
      "backcall                      0.2.0\n",
      "be-great                      0.0.7\n",
      "beautifulsoup4                4.11.1\n",
      "black                         22.10.0\n",
      "bleach                        5.0.1\n",
      "boto3                         1.34.2\n",
      "botocore                      1.34.2\n",
      "cachetools                    5.2.0\n",
      "certifi                       2022.9.24\n",
      "cffi                          1.15.1\n",
      "charset-normalizer            2.1.1\n",
      "click                         8.1.3\n",
      "cloudpickle                   2.2.0\n",
      "contourpy                     1.0.5\n",
      "copulas                       0.9.2\n",
      "ctgan                         0.8.0\n",
      "cycler                        0.11.0\n",
      "Cython                        0.29.32\n",
      "datasets                      2.16.0\n",
      "DataSynthesizer               0.1.11\n",
      "debugpy                       1.6.3\n",
      "decorator                     5.1.1\n",
      "deepecho                      0.5.0\n",
      "defusedxml                    0.7.1\n",
      "dill                          0.3.7\n",
      "disjoint-set                  0.7.3\n",
      "docutils                      0.17.1\n",
      "ektelo                        0.1.0\n",
      "entrypoints                   0.4\n",
      "executing                     1.2.0\n",
      "Faker                         19.13.0\n",
      "fastjsonschema                2.16.2\n",
      "filelock                      3.13.1\n",
      "flake8                        4.0.1\n",
      "flatbuffers                   22.9.24\n",
      "fonttools                     4.37.4\n",
      "frozenlist                    1.4.1\n",
      "fsspec                        2023.10.0\n",
      "future                        0.18.2\n",
      "gast                          0.4.0\n",
      "google-auth                   2.12.0\n",
      "google-auth-oauthlib          0.4.6\n",
      "google-pasta                  0.2.0\n",
      "graphviz                      0.17\n",
      "greenlet                      3.0.2\n",
      "grpcio                        1.49.1\n",
      "h5py                          3.7.0\n",
      "huggingface-hub               0.20.1\n",
      "idna                          3.4\n",
      "imagesize                     1.4.1\n",
      "importlib-metadata            5.0.0\n",
      "iniconfig                     1.1.1\n",
      "ipykernel                     6.16.0\n",
      "ipython                       8.7.0\n",
      "ipython-genutils              0.2.0\n",
      "isort                         5.10.1\n",
      "jedi                          0.18.1\n",
      "Jinja2                        3.1.2\n",
      "jmespath                      1.0.1\n",
      "joblib                        1.2.0\n",
      "json5                         0.9.10\n",
      "jsonschema                    4.17.3\n",
      "jupyter_client                7.4.2\n",
      "jupyter-core                  4.11.1\n",
      "jupyter-server                1.23.3\n",
      "jupyterlab                    3.5.0\n",
      "jupyterlab-pygments           0.2.2\n",
      "jupyterlab_server             2.16.3\n",
      "keras                         2.10.0\n",
      "Keras-Preprocessing           1.1.2\n",
      "kiwisolver                    1.4.4\n",
      "libclang                      14.0.6\n",
      "macholib                      1.16.2\n",
      "Markdown                      3.4.1\n",
      "MarkupSafe                    2.1.1\n",
      "matplotlib                    3.6.1\n",
      "matplotlib-inline             0.1.6\n",
      "mccabe                        0.6.1\n",
      "mistune                       2.0.4\n",
      "multidict                     6.0.4\n",
      "multiprocess                  0.70.15\n",
      "mypy-extensions               0.4.3\n",
      "nbclassic                     0.4.8\n",
      "nbclient                      0.5.13\n",
      "nbconvert                     7.2.5\n",
      "nbformat                      5.7.0\n",
      "nest-asyncio                  1.5.6\n",
      "networkx                      2.8.7\n",
      "nose                          1.3.7\n",
      "notebook                      6.5.2\n",
      "notebook_shim                 0.2.2\n",
      "numpy                         1.23.4\n",
      "oauthlib                      3.2.1\n",
      "opacus                        0.14.0\n",
      "opendp                        0.6.2\n",
      "opt-einsum                    3.3.0\n",
      "pac-synth                     0.0.6\n",
      "packaging                     21.3\n",
      "palettable                    3.3.0\n",
      "pandas                        1.5.0\n",
      "pandasql                      0.7.3\n",
      "pandocfilters                 1.5.0\n",
      "parso                         0.8.3\n",
      "pathspec                      0.10.1\n",
      "pexpect                       4.8.0\n",
      "pickleshare                   0.7.5\n",
      "Pillow                        9.2.0\n",
      "pip                           23.3.2\n",
      "platformdirs                  2.5.2\n",
      "plotly                        5.18.0\n",
      "pluggy                        1.0.0\n",
      "private-pgm                   0.0.1\n",
      "prometheus-client             0.15.0\n",
      "prompt-toolkit                3.0.31\n",
      "protobuf                      3.19.6\n",
      "psutil                        5.9.2\n",
      "psycopg2-binary               2.9.4\n",
      "ptyprocess                    0.7.0\n",
      "pure-eval                     0.2.2\n",
      "py                            1.11.0\n",
      "py-synthpop                   0.1.2\n",
      "pyarrow                       14.0.2\n",
      "pyarrow-hotfix                0.6\n",
      "pyasn1                        0.4.8\n",
      "pyasn1-modules                0.2.8\n",
      "pycodestyle                   2.8.0\n",
      "pycparser                     2.21\n",
      "pyflakes                      2.4.0\n",
      "Pygments                      2.13.0\n",
      "pyinstaller                   5.5\n",
      "pyinstaller-hooks-contrib     2022.10\n",
      "pyparsing                     3.0.9\n",
      "pyrsistent                    0.19.2\n",
      "pytest                        7.1.3\n",
      "python-dateutil               2.8.2\n",
      "pytz                          2022.4\n",
      "PyYAML                        5.4.1\n",
      "pyzmq                         24.0.1\n",
      "rdt                           1.9.0\n",
      "regex                         2023.10.3\n",
      "reprosyn                      0.1.0\n",
      "requests                      2.28.1\n",
      "requests-oauthlib             1.3.1\n",
      "rsa                           4.9\n",
      "s3transfer                    0.9.0\n",
      "safetensors                   0.4.1\n",
      "scikit-learn                  1.1.2\n",
      "scipy                         1.9.2\n",
      "sdmetrics                     0.13.0\n",
      "sdv                           1.8.0\n",
      "seaborn                       0.11.2\n",
      "Send2Trash                    1.8.0\n",
      "setuptools                    65.5.0\n",
      "setuptools-scm                7.0.5\n",
      "six                           1.16.0\n",
      "sklearn                       0.0\n",
      "smartnoise-sql                0.2.12\n",
      "smartnoise-synth              0.3.3\n",
      "sniffio                       1.3.0\n",
      "snowballstemmer               2.2.0\n",
      "soupsieve                     2.3.2.post1\n",
      "Sphinx                        4.5.0\n",
      "sphinx-rtd-theme              1.0.0\n",
      "sphinxcontrib-applehelp       1.0.2\n",
      "sphinxcontrib-devhelp         1.0.2\n",
      "sphinxcontrib-htmlhelp        2.0.0\n",
      "sphinxcontrib-jsmath          1.0.1\n",
      "sphinxcontrib-qthelp          1.0.3\n",
      "sphinxcontrib-serializinghtml 1.1.5\n",
      "spyder-kernels                2.2.0\n",
      "SQLAlchemy                    1.4.50\n",
      "stack-data                    0.6.2\n",
      "tapas                         1.0.0       /Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main\n",
      "tenacity                      8.2.3\n",
      "tensorboard                   2.10.1\n",
      "tensorboard-data-server       0.6.1\n",
      "tensorboard-plugin-wit        1.8.1\n",
      "tensorflow                    2.10.0\n",
      "tensorflow-estimator          2.10.0\n",
      "tensorflow-io-gcs-filesystem  0.27.0\n",
      "termcolor                     2.0.1\n",
      "terminado                     0.17.0\n",
      "threadpoolctl                 3.1.0\n",
      "tinycss2                      1.2.1\n",
      "tokenizers                    0.15.0\n",
      "tomli                         2.0.1\n",
      "torch                         1.12.1\n",
      "torchvision                   0.13.1\n",
      "tornado                       6.2\n",
      "tqdm                          4.64.1\n",
      "traitlets                     5.4.0\n",
      "transformers                  4.36.2\n",
      "typing_extensions             4.4.0\n",
      "urllib3                       1.26.12\n",
      "validators                    0.20.0\n",
      "wcwidth                       0.2.5\n",
      "webencodings                  0.5.1\n",
      "websocket-client              1.4.2\n",
      "Werkzeug                      2.2.2\n",
      "wheel                         0.37.1\n",
      "wrapt                         1.14.1\n",
      "wurlitzer                     3.0.2\n",
      "xxhash                        3.4.1\n",
      "yarl                          1.9.4\n",
      "zipp                          3.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee6109d",
   "metadata": {},
   "source": [
    "## Importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "818c4d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/mbi/__init__.py:15: UserWarning: MixtureInference disabled, please install jax and jaxlib\n",
      "  warnings.warn('MixtureInference disabled, please install jax and jaxlib')\n",
      "2023-12-24 00:09:49.596538: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Importing standard packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import itertools\n",
    "from enum import Enum\n",
    "\n",
    "# Importing from tapas\n",
    "import tapas.datasets\n",
    "import tapas.generators\n",
    "import tapas.attacks\n",
    "import tapas.threat_models\n",
    "import tapas.report\n",
    "from tapas.generators import Generator, ReprosynGenerator, Raw\n",
    "import tqdm\n",
    "from modules.myctgan import CTGAN\n",
    "from modules.myctgan import DPCTGAN\n",
    "from modules.myctgan import PATEGAN\n",
    "\n",
    "# Importing from sklearn\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Importing from mbi and be_great\n",
    "from mbi import Dataset, Domain\n",
    "from be_great import GReaT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c10704",
   "metadata": {},
   "source": [
    "## Using the GReaT framework API\n",
    "### Description\n",
    "Generates Realistic Synthetic Tabular data with pretrained Transformer-based language models. Here is an example that uses their API to generate synthetic data for the breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "612c577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates synethetic data using the GReaT framework's API\n",
    "def generateSyntheticDataFromGReaT(batchSize: int, epochs: int, nSamples: int):\n",
    "    # Loading the breast cancer dataset as a frame\n",
    "    breastCancerData = load_breast_cancer(as_frame=True).frame\n",
    "    \n",
    "    # Defining an LLM model using the GReaT API\n",
    "    model = GReaT(llm='distilgpt2', batch_size=batchSize, epochs=epochs)\n",
    "    \n",
    "    # Fitting the generated model to the breastCancerDataSet\n",
    "    model.fit(breastCancerData)\n",
    "    \n",
    "    # Sampling the model to generate the synethic data\n",
    "    synthetic_data = model.sample(n_samples=nSamples)\n",
    "    \n",
    "    # Returning the produced and sampled synthetic data\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f0d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function call to generate the synthetic data using GReaT; need to run this only once, takes a while to generate \n",
    "# synthetic_data = generateSyntheticDataFromGReaT(batch_size=32, epochs=25, nSamples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27dc96f",
   "metadata": {},
   "source": [
    "## Data Pre-processing: Labeling the Data Files\n",
    "\n",
    "### Description\n",
    "For data pre-processing, I had to label the raw and the generated data files with column names and indices since those were not already present in the raw data downloaded from the source.\n",
    "\n",
    "The column names for the data files include:\n",
    "1. ID (Patient ID)\n",
    "2. DIAGNOSIS (M: Malignant, B: Benign)\n",
    "3. RADIUS1\n",
    "4. TEXTURE1\n",
    "5. PERIMETER1\n",
    "6. AREA1\n",
    "7. SMOOTHNESS1\n",
    "8. COMPACTNESS1\n",
    "9. CONCAVITY1\n",
    "10. CONCAVE_POINTS1\n",
    "11. SYMMETRY1\n",
    "12. FRACTAL_DIMENSION1\n",
    "13. RADIUS2\n",
    "14. TEXTURE2\n",
    "15. PERIMETER2\n",
    "16. AREA2\n",
    "17. SMOOTHNESS2\n",
    "18. COMPACTNESS2\n",
    "19. CONCAVITY2\n",
    "20. CONCAVE_POINTS2\n",
    "21. SYMMETRY2\n",
    "22. FRACTAL_DIMENSION2\n",
    "23. RADIUS3\n",
    "24. TEXTURE3\n",
    "25. PERIMETER3\n",
    "26. AREA3\n",
    "27. SMOOTHNESS3\n",
    "28. COMPACTNESS3\n",
    "29. CONCAVITY3\n",
    "30. CONCAVE_POINTS3\n",
    "31. SYMMETRY3\n",
    "32. FRACTAL_DIMENSION3\n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus in the dataset:\n",
    "\ta) radius (mean of distances from center to points on the perimeter)\n",
    "\tb) texture (standard deviation of gray-scale values)\n",
    "\tc) perimeter\n",
    "\td) area\n",
    "\te) smoothness (local variation in radius lengths)\n",
    "\tf) compactness (perimeter^2 / area - 1.0)\n",
    "\tg) concavity (severity of concave portions of the contour)\n",
    "\th) concave points (number of concave portions of the contour)\n",
    "\ti) symmetry \n",
    "\tj) fractal dimension (\"coastline approximation\" - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2915ef2",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "### Description\n",
    "Loading data from the input csvPath, assigning it to the appropriate data types and returning a dataframe as a data type dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd89ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads data from the input csvPath, assigns it to appropriate data types and returns a dataframe as data type dictionary\n",
    "def loadBreastCancerData(csvPath: str):\n",
    "    # Reads the csv file using the input csvPath\n",
    "    df = pd.read_csv(csvPath, index_col=None)\n",
    "    \n",
    "    # Stores the column names of the DataFrame df in the variable cols\n",
    "    cols = df.columns\n",
    "    \n",
    "    # Uses dictionary comprehension to set the data type of the columns to data types of str or float64\n",
    "    dtype_dict = {\"ID\": \"str\", **{col: 'str' for col in cols[1:3]}, **{col: 'float64' for col in cols[3:]}}\n",
    "    \n",
    "    # Applies the data type conversions specified in dtype_dict to the DataFrame df\n",
    "    # The astype() function is used to cast the columns of the DataFrame to specified data types\n",
    "    df = df.astype(dtype_dict)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04799544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the synthetic data files\n",
    "breastCancerData_real = \"../data/breastcancer_real_labeled.csv\"\n",
    "breastCancerData_synthetic = \"../data/breastcancer_synthetic_labeled.csv\"\n",
    "\n",
    "# Callings loadBreastCancerData to load breast cancer data into a data frame\n",
    "df = loadBreastCancerData(breastCancerData_synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cce9069f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>DIAGNOSIS</th>\n",
       "      <th>RADIUS1</th>\n",
       "      <th>TEXTURE1</th>\n",
       "      <th>PERIMETER1</th>\n",
       "      <th>AREA1</th>\n",
       "      <th>SMOOTHNESS1</th>\n",
       "      <th>COMPACTNESS1</th>\n",
       "      <th>CONCAVITY1</th>\n",
       "      <th>...</th>\n",
       "      <th>RADIUS3</th>\n",
       "      <th>TEXTURE3</th>\n",
       "      <th>PERIMETER3</th>\n",
       "      <th>AREA3</th>\n",
       "      <th>SMOOTHNESS3</th>\n",
       "      <th>COMPACTNESS3</th>\n",
       "      <th>CONCAVITY3</th>\n",
       "      <th>CONCAVE_POINTS3</th>\n",
       "      <th>SYMMETRY3</th>\n",
       "      <th>FRACTAL_DIMENSION3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.990</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.300100</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.711900</td>\n",
       "      <td>0.265400</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>90483</td>\n",
       "      <td>M</td>\n",
       "      <td>11.380</td>\n",
       "      <td>19.53</td>\n",
       "      <td>108.52</td>\n",
       "      <td>419.1</td>\n",
       "      <td>0.08833</td>\n",
       "      <td>0.13113</td>\n",
       "      <td>0.170541</td>\n",
       "      <td>...</td>\n",
       "      <td>11.774</td>\n",
       "      <td>17.97</td>\n",
       "      <td>82.56</td>\n",
       "      <td>732.2</td>\n",
       "      <td>0.15339</td>\n",
       "      <td>0.30083</td>\n",
       "      <td>0.183181</td>\n",
       "      <td>0.047841</td>\n",
       "      <td>0.2656</td>\n",
       "      <td>0.06180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>499225</td>\n",
       "      <td>M</td>\n",
       "      <td>12.763</td>\n",
       "      <td>22.84</td>\n",
       "      <td>84.15</td>\n",
       "      <td>258.8</td>\n",
       "      <td>0.10846</td>\n",
       "      <td>0.12834</td>\n",
       "      <td>0.017313</td>\n",
       "      <td>...</td>\n",
       "      <td>14.117</td>\n",
       "      <td>23.75</td>\n",
       "      <td>99.28</td>\n",
       "      <td>802.7</td>\n",
       "      <td>0.13176</td>\n",
       "      <td>0.12695</td>\n",
       "      <td>0.303214</td>\n",
       "      <td>0.078201</td>\n",
       "      <td>0.2453</td>\n",
       "      <td>0.10657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>912337</td>\n",
       "      <td>M</td>\n",
       "      <td>14.905</td>\n",
       "      <td>16.39</td>\n",
       "      <td>74.85</td>\n",
       "      <td>478.9</td>\n",
       "      <td>0.08700</td>\n",
       "      <td>0.13332</td>\n",
       "      <td>0.048864</td>\n",
       "      <td>...</td>\n",
       "      <td>11.043</td>\n",
       "      <td>22.53</td>\n",
       "      <td>107.42</td>\n",
       "      <td>628.2</td>\n",
       "      <td>0.12498</td>\n",
       "      <td>0.17828</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>0.057869</td>\n",
       "      <td>0.2806</td>\n",
       "      <td>0.08021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>865589</td>\n",
       "      <td>M</td>\n",
       "      <td>13.467</td>\n",
       "      <td>26.63</td>\n",
       "      <td>74.55</td>\n",
       "      <td>255.1</td>\n",
       "      <td>0.11280</td>\n",
       "      <td>0.06524</td>\n",
       "      <td>0.029281</td>\n",
       "      <td>...</td>\n",
       "      <td>16.614</td>\n",
       "      <td>23.62</td>\n",
       "      <td>87.28</td>\n",
       "      <td>1968.9</td>\n",
       "      <td>0.10603</td>\n",
       "      <td>0.26378</td>\n",
       "      <td>0.688444</td>\n",
       "      <td>0.177147</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>0.09457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      ID DIAGNOSIS  RADIUS1  TEXTURE1  PERIMETER1   AREA1  \\\n",
       "0           1  842302         M   17.990     10.38      122.80  1001.0   \n",
       "1           2   90483         M   11.380     19.53      108.52   419.1   \n",
       "2           3  499225         M   12.763     22.84       84.15   258.8   \n",
       "3           4  912337         M   14.905     16.39       74.85   478.9   \n",
       "4           5  865589         M   13.467     26.63       74.55   255.1   \n",
       "\n",
       "   SMOOTHNESS1  COMPACTNESS1  CONCAVITY1  ...  RADIUS3  TEXTURE3  PERIMETER3  \\\n",
       "0      0.11840       0.27760    0.300100  ...   25.380     17.33      184.60   \n",
       "1      0.08833       0.13113    0.170541  ...   11.774     17.97       82.56   \n",
       "2      0.10846       0.12834    0.017313  ...   14.117     23.75       99.28   \n",
       "3      0.08700       0.13332    0.048864  ...   11.043     22.53      107.42   \n",
       "4      0.11280       0.06524    0.029281  ...   16.614     23.62       87.28   \n",
       "\n",
       "    AREA3  SMOOTHNESS3  COMPACTNESS3  CONCAVITY3  CONCAVE_POINTS3  SYMMETRY3  \\\n",
       "0  2019.0      0.16220       0.66560    0.711900         0.265400     0.4601   \n",
       "1   732.2      0.15339       0.30083    0.183181         0.047841     0.2656   \n",
       "2   802.7      0.13176       0.12695    0.303214         0.078201     0.2453   \n",
       "3   628.2      0.12498       0.17828    0.009774         0.057869     0.2806   \n",
       "4  1968.9      0.10603       0.26378    0.688444         0.177147     0.2684   \n",
       "\n",
       "   FRACTAL_DIMENSION3  \n",
       "0             0.11890  \n",
       "1             0.06180  \n",
       "2             0.10657  \n",
       "3             0.08021  \n",
       "4             0.09457  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79fee7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>RADIUS1</th>\n",
       "      <th>TEXTURE1</th>\n",
       "      <th>PERIMETER1</th>\n",
       "      <th>AREA1</th>\n",
       "      <th>SMOOTHNESS1</th>\n",
       "      <th>COMPACTNESS1</th>\n",
       "      <th>CONCAVITY1</th>\n",
       "      <th>CONCAVE_POINTS1</th>\n",
       "      <th>SYMMETRY1</th>\n",
       "      <th>...</th>\n",
       "      <th>RADIUS3</th>\n",
       "      <th>TEXTURE3</th>\n",
       "      <th>PERIMETER3</th>\n",
       "      <th>AREA3</th>\n",
       "      <th>SMOOTHNESS3</th>\n",
       "      <th>COMPACTNESS3</th>\n",
       "      <th>CONCAVITY3</th>\n",
       "      <th>CONCAVE_POINTS3</th>\n",
       "      <th>SYMMETRY3</th>\n",
       "      <th>FRACTAL_DIMENSION3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.00000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>285.000000</td>\n",
       "      <td>14.028819</td>\n",
       "      <td>19.292707</td>\n",
       "      <td>93.046942</td>\n",
       "      <td>651.976450</td>\n",
       "      <td>0.096386</td>\n",
       "      <td>0.106391</td>\n",
       "      <td>0.097223</td>\n",
       "      <td>0.048596</td>\n",
       "      <td>0.180786</td>\n",
       "      <td>...</td>\n",
       "      <td>16.31687</td>\n",
       "      <td>25.886784</td>\n",
       "      <td>107.430246</td>\n",
       "      <td>926.653603</td>\n",
       "      <td>0.131671</td>\n",
       "      <td>0.262885</td>\n",
       "      <td>0.271272</td>\n",
       "      <td>0.117380</td>\n",
       "      <td>0.287135</td>\n",
       "      <td>0.084605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>164.400426</td>\n",
       "      <td>3.487421</td>\n",
       "      <td>3.944997</td>\n",
       "      <td>22.544642</td>\n",
       "      <td>341.800801</td>\n",
       "      <td>0.013397</td>\n",
       "      <td>0.052466</td>\n",
       "      <td>0.084546</td>\n",
       "      <td>0.038079</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>...</td>\n",
       "      <td>4.70510</td>\n",
       "      <td>6.015113</td>\n",
       "      <td>32.782185</td>\n",
       "      <td>589.146829</td>\n",
       "      <td>0.021753</td>\n",
       "      <td>0.155781</td>\n",
       "      <td>0.206014</td>\n",
       "      <td>0.065451</td>\n",
       "      <td>0.054948</td>\n",
       "      <td>0.016916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.220000</td>\n",
       "      <td>10.380000</td>\n",
       "      <td>53.790000</td>\n",
       "      <td>205.600000</td>\n",
       "      <td>0.068290</td>\n",
       "      <td>0.032270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.10200</td>\n",
       "      <td>14.960000</td>\n",
       "      <td>58.450000</td>\n",
       "      <td>257.400000</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.058560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>143.000000</td>\n",
       "      <td>11.672000</td>\n",
       "      <td>16.300000</td>\n",
       "      <td>76.920000</td>\n",
       "      <td>417.900000</td>\n",
       "      <td>0.086050</td>\n",
       "      <td>0.064400</td>\n",
       "      <td>0.034094</td>\n",
       "      <td>0.020312</td>\n",
       "      <td>0.163200</td>\n",
       "      <td>...</td>\n",
       "      <td>13.08600</td>\n",
       "      <td>21.380000</td>\n",
       "      <td>84.540000</td>\n",
       "      <td>525.300000</td>\n",
       "      <td>0.116680</td>\n",
       "      <td>0.155020</td>\n",
       "      <td>0.118685</td>\n",
       "      <td>0.067587</td>\n",
       "      <td>0.247900</td>\n",
       "      <td>0.072300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>285.000000</td>\n",
       "      <td>13.171000</td>\n",
       "      <td>18.960000</td>\n",
       "      <td>87.790000</td>\n",
       "      <td>549.400000</td>\n",
       "      <td>0.095930</td>\n",
       "      <td>0.097110</td>\n",
       "      <td>0.068367</td>\n",
       "      <td>0.034707</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91400</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>98.470000</td>\n",
       "      <td>711.900000</td>\n",
       "      <td>0.130780</td>\n",
       "      <td>0.222110</td>\n",
       "      <td>0.225394</td>\n",
       "      <td>0.102542</td>\n",
       "      <td>0.282300</td>\n",
       "      <td>0.081390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>427.000000</td>\n",
       "      <td>15.623000</td>\n",
       "      <td>21.960000</td>\n",
       "      <td>106.340000</td>\n",
       "      <td>766.200000</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>0.132940</td>\n",
       "      <td>0.143995</td>\n",
       "      <td>0.074055</td>\n",
       "      <td>0.195300</td>\n",
       "      <td>...</td>\n",
       "      <td>18.66700</td>\n",
       "      <td>30.330000</td>\n",
       "      <td>125.030000</td>\n",
       "      <td>1227.700000</td>\n",
       "      <td>0.143730</td>\n",
       "      <td>0.350530</td>\n",
       "      <td>0.380726</td>\n",
       "      <td>0.166465</td>\n",
       "      <td>0.315600</td>\n",
       "      <td>0.093040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>24.621000</td>\n",
       "      <td>30.630000</td>\n",
       "      <td>166.140000</td>\n",
       "      <td>1839.000000</td>\n",
       "      <td>0.133340</td>\n",
       "      <td>0.277600</td>\n",
       "      <td>0.352245</td>\n",
       "      <td>0.167989</td>\n",
       "      <td>0.259600</td>\n",
       "      <td>...</td>\n",
       "      <td>30.78900</td>\n",
       "      <td>41.840000</td>\n",
       "      <td>211.310000</td>\n",
       "      <td>2939.100000</td>\n",
       "      <td>0.190030</td>\n",
       "      <td>0.790320</td>\n",
       "      <td>0.903177</td>\n",
       "      <td>0.269674</td>\n",
       "      <td>0.487900</td>\n",
       "      <td>0.140800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     RADIUS1    TEXTURE1  PERIMETER1        AREA1  \\\n",
       "count  569.000000  569.000000  569.000000  569.000000   569.000000   \n",
       "mean   285.000000   14.028819   19.292707   93.046942   651.976450   \n",
       "std    164.400426    3.487421    3.944997   22.544642   341.800801   \n",
       "min      1.000000    8.220000   10.380000   53.790000   205.600000   \n",
       "25%    143.000000   11.672000   16.300000   76.920000   417.900000   \n",
       "50%    285.000000   13.171000   18.960000   87.790000   549.400000   \n",
       "75%    427.000000   15.623000   21.960000  106.340000   766.200000   \n",
       "max    569.000000   24.621000   30.630000  166.140000  1839.000000   \n",
       "\n",
       "       SMOOTHNESS1  COMPACTNESS1  CONCAVITY1  CONCAVE_POINTS1   SYMMETRY1  \\\n",
       "count   569.000000    569.000000  569.000000       569.000000  569.000000   \n",
       "mean      0.096386      0.106391    0.097223         0.048596    0.180786   \n",
       "std       0.013397      0.052466    0.084546         0.038079    0.024300   \n",
       "min       0.068290      0.032270    0.000000         0.000000    0.128000   \n",
       "25%       0.086050      0.064400    0.034094         0.020312    0.163200   \n",
       "50%       0.095930      0.097110    0.068367         0.034707    0.179200   \n",
       "75%       0.105500      0.132940    0.143995         0.074055    0.195300   \n",
       "max       0.133340      0.277600    0.352245         0.167989    0.259600   \n",
       "\n",
       "       ...    RADIUS3    TEXTURE3  PERIMETER3        AREA3  SMOOTHNESS3  \\\n",
       "count  ...  569.00000  569.000000  569.000000   569.000000   569.000000   \n",
       "mean   ...   16.31687   25.886784  107.430246   926.653603     0.131671   \n",
       "std    ...    4.70510    6.015113   32.782185   589.146829     0.021753   \n",
       "min    ...    9.10200   14.960000   58.450000   257.400000     0.088400   \n",
       "25%    ...   13.08600   21.380000   84.540000   525.300000     0.116680   \n",
       "50%    ...   14.91400   25.400000   98.470000   711.900000     0.130780   \n",
       "75%    ...   18.66700   30.330000  125.030000  1227.700000     0.143730   \n",
       "max    ...   30.78900   41.840000  211.310000  2939.100000     0.190030   \n",
       "\n",
       "       COMPACTNESS3  CONCAVITY3  CONCAVE_POINTS3   SYMMETRY3  \\\n",
       "count    569.000000  569.000000       569.000000  569.000000   \n",
       "mean       0.262885    0.271272         0.117380    0.287135   \n",
       "std        0.155781    0.206014         0.065451    0.054948   \n",
       "min        0.049800    0.000000         0.000000    0.171400   \n",
       "25%        0.155020    0.118685         0.067587    0.247900   \n",
       "50%        0.222110    0.225394         0.102542    0.282300   \n",
       "75%        0.350530    0.380726         0.166465    0.315600   \n",
       "max        0.790320    0.903177         0.269674    0.487900   \n",
       "\n",
       "       FRACTAL_DIMENSION3  \n",
       "count          569.000000  \n",
       "mean             0.084605  \n",
       "std              0.016916  \n",
       "min              0.058560  \n",
       "25%              0.072300  \n",
       "50%              0.081390  \n",
       "75%              0.093040  \n",
       "max              0.140800  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "922b3df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating TabularDataset object\n",
    "data = tapas.datasets.TabularDataset(data=df, \n",
    "                                     description=tapas.datasets.DataDescription(json.load(open(\"../data/cancer.json\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fee0e94",
   "metadata": {},
   "source": [
    "## Attacks, Threat Models and Privacy Auditing\n",
    "\n",
    "### Description\n",
    "Contains helper functions and methods for defining attacker knowledge on data, attacker knowledge on generator, threat models, different attack types provided in the TAPAS toolbox, generators for synthetic data, creation of target indices, functions that define the attack, train/test the model and generate metrics and reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38c5208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enum for attacker knowledge on data\n",
    "class AttackerKnowledgeOnData(Enum):\n",
    "    # This attacker knowledge assumes access to some auxiliary dataset from which training datasets are sampled, as random subset of this auxiliary data.\n",
    "    # A distinct testing dataset, sampled from the same distribution, is also used to generate testing samples.\n",
    "    AuxiliaryDataKnowledge = 1\n",
    "    \n",
    "    # Also called worst-case attack, this assumes that the attacker knows the exact dataset used to generate \n",
    "    ExactDataKnowledge = 2\n",
    "    \n",
    "# Helper function that returns attacker knowledge on data\n",
    "def getAttackerKnowledgeOnData(dataset, attackerKnowledgeOnData):\n",
    "    if attackerKnowledgeOnData == AttackerKnowledgeOnData.AuxiliaryDataKnowledge:\n",
    "        return tapas.threat_models.AuxiliaryDataKnowledge(dataset,\n",
    "                        auxiliary_split=0.5, num_training_records=100, )\n",
    "    elif attackerKnowledgeOnData == AttackerKnowledgeOnData.ExactDataKnowledge:\n",
    "        return tapas.threat_models.ExactDataKnowledge(dataset, num_training_records=100, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1035c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enum for attacker knowledge on generator\n",
    "class AttackerKnowledgeOnGenerator(Enum):\n",
    "    # BlackBoxKnowledge: The attacker has access to the generator method with access to the generator has an exact black-box.\n",
    "    # The attacker can call the generator with the same parameters as were used to produce the real dataset. This is the recommended assumption on attacker knowledge.\n",
    "    BlackBoxKnowledge = 1\n",
    "    \n",
    "    # The attacker does not have access to the generator. The attacker cannot call the generator, and the .generate method thus fails in training mode.\n",
    "    # A generator is still needed to generate evaluation samples.\n",
    "    NoBoxKnowledge = 2\n",
    "    \n",
    "    # The attacker has uncertain knowledge of the generator: they have access to the code, but not to some \"parameters\" of the code. \n",
    "    # Instead, the attacker has a prior (distribution) of acceptable parameters.\n",
    "    UncertainBoxKnowledge = 3\n",
    "    \n",
    "# Helper function that returns attacker knowledge on generator\n",
    "def getAttackerKnowledgeOnGenerator(generator, attackerKnowledgeOnGenerator):\n",
    "    if attackerKnowledgeOnGenerator == AttackerKnowledgeOnGenerator.BlackBoxKnowledge:\n",
    "        return tapas.threat_models.BlackBoxKnowledge(generator, num_synthetic_records=100, )\n",
    "    elif attackerKnowledgeOnGenerator == AttackerKnowledgeOnGenerator.NoBoxKnowledge:\n",
    "        return tapas.threat_models.NoBoxKnowledge(generator, num_synthetic_records=100, )\n",
    "    elif attackerKnowledgeOnGenerator == AttackerKnowledgeOnGenerator.UncertainBoxKnowledge:\n",
    "        return tapas.threat_models.UncertainBoxKnowledge(generator, num_synthetic_records=100, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69285fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enum for threat models\n",
    "class LabelInferenceTrainableThreatModel(Enum):\n",
    "    # This threat model implements a MIA (membership inference attacks) with arbitrary attacker knowledge on data and generator.\n",
    "    TargetedMIA = 1\n",
    "    \n",
    "    # This threat model implements a AIA (attribute inference attacks) with arbitrary attacker knowledge on data and generator.\n",
    "    TargetedAIA = 2\n",
    "\n",
    "# Helper function that returns threat model from the input parameters\n",
    "def getThreatModel(data_knowledge, target_index, sdg_knowledge, threatModel):\n",
    "    if threatModel == LabelInferenceTrainableThreatModel.TargetedMIA:\n",
    "        return tapas.threat_models.TargetedMIA(attacker_knowledge_data=data_knowledge,\n",
    "                        target_record=dataset.get_records([target_index]),\n",
    "                        attacker_knowledge_generator=sdg_knowledge,\n",
    "                        generate_pairs=True,\n",
    "                        replace_target=True,\n",
    "                        iterator_tracker=tqdm.tqdm)\n",
    "    elif threatModel == LabelInferenceTrainableThreatModel.TargetedAIA:\n",
    "        return tapas.threat_models.TargetedAIA(attacker_knowledge_data=data_knowledge,\n",
    "                        target_record=dataset.get_records([target_index]),\n",
    "                        attacker_knowledge_generator=sdg_knowledge,\n",
    "                        generate_pairs=True,\n",
    "                        replace_target=True,\n",
    "                        iterator_tracker=tqdm.tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "585f7e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enum for attacks\n",
    "class Attack(Enum):\n",
    "    # Shadow-modelling Attack: Simulates the dataset generation process, using auxiliary info available to the attacker, and train a classifier to predict a property of the training dataset from the synethic dataset\n",
    "    ShadowModellingAttack = 1\n",
    "    \n",
    "    # Groundhog Attack: The attack introduced by Stadler et al.\n",
    "    GroundhogAttack = 2\n",
    "    \n",
    "    # ProbabilityEstimationAttack: Membership Inference Attack that first estimates a statistical model p_x of the distribution of records in the synthetic data, and then uses p_x(target_record) as score. The intuition is that the distribution of the synthetic data, which is defined by the generator trained on the real data, is more likely to be high for records in the real data.\n",
    "    ProbabilityEstimationAttack = 3\n",
    "    \n",
    "    # SyntheticPredictorAttack: Attribute inference attack that first trains a classifier C on the synthetic data to predict the sensitive value v of a record x, then uses C(target_record) as prediction for the target record.\n",
    "    SyntheticPredictorAttack = 4\n",
    "    \n",
    "    # ClosestDistanceAIA: Attack that finds the closest-record to the target record, and uses the value of the sensitive attribute of that closest-record as answer to the attribute-inference attack\n",
    "    ClosestDistanceAIA = 5\n",
    "    \n",
    "    # ClosestDistanceMIA: Attack that looks for the closest record to a given target in the synthetic data to determine whether the target was in the training set\n",
    "    ClosestDistanceMIA = 6\n",
    "    \n",
    "    # LocalNeighbourhoodAttack: Attack that makes a decision based on records similar to the target record, specifically all records within a sphere of a given radius, for a specific choice of distance\n",
    "    LocalNeighbourhoodAttack = 7\n",
    "    \n",
    "    # DirectLinkage: Attack that checks only whether or not the target is in the generated synthetic dataset or not\n",
    "    DirectLinkage = 8\n",
    "\n",
    "# Returns attack for the input args\n",
    "def getAttack(attack, feature_classifier):\n",
    "    if attack == Attack.ShadowModellingAttack:\n",
    "        return tapas.attacks.ShadowModellingAttack(feature_classifier)\n",
    "    elif attack == Attack.GroundhogAttack:\n",
    "        return tapas.attacks.GroundhogAttack(feature_classifier)\n",
    "    elif attack == Attack.ProbabilityEstimationAttack:\n",
    "        return tapas.attacks.ProbabilityEstimationAttack()\n",
    "    elif attack == Attack.SyntheticPredictorAttack:\n",
    "        return tapas.attacks.SyntheticPredictorAttack()\n",
    "    elif attack == Attack.ClosestDistanceAIA:\n",
    "        return tapas.attacks.ClosestDistanceAIA()\n",
    "    elif attack == Attack.ClosestDistanceMIA:\n",
    "        return tapas.attacks.ClosestDistanceMIA()\n",
    "    elif attack == Attack.LocalNeighbourhoodAttack:\n",
    "        return tapas.attacks.LocalNeighbourhoodAttack()\n",
    "    elif attack == Attack.DirectLinkage:\n",
    "        return tapas.attacks.DirectLinkage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9e79fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enum for synthetic data generators\n",
    "class Generator(Enum):\n",
    "    # Raw data\n",
    "    Raw = 1\n",
    "    # Conditional Tabular Generative Adversarial Network models single table - tabular data distribution for synthetic data generators\n",
    "    CTGAN = 2\n",
    "    # Differentially Private Conditional Tabular GAN: Incorporates differential privacy guarantees into conditional tabular generative model\n",
    "    DPCTGAN = 3\n",
    "    # Private Aggregation of Teacher Ensembles (PATE) framework applied to GANs; modified framework (which is called PATE-GAN) allows us to tightly bound the influence of any individual sample on the model, resulting in tight differential privacy guarantees and thus an improved performance over models with the same guarantees. \n",
    "    PATEGAN = 4\n",
    "    \n",
    "# Helper function for getting the generator with the input specs\n",
    "def getGenerator(generator, epsilon, batch_size, epochs):\n",
    "    if generator == Generator.Raw:\n",
    "        return Raw()\n",
    "    elif generator == Generator.CTGAN:\n",
    "        return CTGAN(epochs=1)\n",
    "    elif generator == Generator.DPCTGAN:\n",
    "        return DPCTGAN(epsilon=epsilon, batch_size=batch_size, epochs=epochs)\n",
    "    elif generator == Generator.PATEGAN:\n",
    "        return PATEGAN(epsilon=epsilon, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02e736c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of records to target for random and outlier\n",
    "num_targets = 5\n",
    "    \n",
    "# Selecting random target record indices\n",
    "random_index = list(np.random.randint(1, 100, num_targets))\n",
    "\n",
    "# Creates target indices to target for random and outlier\n",
    "def createTargetIndices():\n",
    "    # Selecting outlier target record indices\n",
    "    model_isoforest = IsolationForest()\n",
    "    preds = model_isoforest.fit_predict(data.data.iloc[:, 3:])\n",
    "    outlier_index = list(np.random.choice(np.where(preds == -1)[0], num_targets))\n",
    "\n",
    "    # Listing out target indices by combining random and outlier indicies\n",
    "    targets = random_index + outlier_index\n",
    "\n",
    "    return targets\n",
    "\n",
    "targets = createTargetIndices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66341900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[84, 70, 70, 90, 66, 491, 73, 57, 491, 550]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb23f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that defines the attack by consolidating data knowledge, knowledge on the generator as well as training and testing the attacks by calling the helper functions\n",
    "def attack(dataset, target_index, generator):\n",
    "    # Getting attacker knowledge on data\n",
    "    data_knowledge = getAttackerKnowledgeOnData(dataset, AttackerKnowledgeOnData.AuxiliaryDataKnowledge)\n",
    "    \n",
    "    # Getting knowledge on generator\n",
    "    sdg_knowledge = getAttackerKnowledgeOnGenerator(generator, AttackerKnowledgeOnGenerator.BlackBoxKnowledge)\n",
    "    \n",
    "    # Defining the threat model membership inference attack on a random record with attacker goal\n",
    "    threat_model = tapas.threat_models.TargetedMIA(attacker_knowledge_data=data_knowledge,\n",
    "                        target_record=dataset.get_records([target_index]),\n",
    "                        attacker_knowledge_generator=sdg_knowledge,\n",
    "                        generate_pairs=True,\n",
    "                        replace_target=True,\n",
    "                        iterator_tracker=tqdm.tqdm)\n",
    "\n",
    "    # Initializing an attacker of Groundhog attack with standard parameters\n",
    "    random_forest = RandomForestClassifier(n_estimators=100)\n",
    "    feature_set = tapas.attacks.NaiveSetFeature() + tapas.attacks.HistSetFeature() + tapas.attacks.CorrSetFeature()\n",
    "    feature_classifier = tapas.attacks.FeatureBasedSetClassifier(feature_set, random_forest)\n",
    "    attacker = getAttack(Attack.GroundhogAttack, feature_classifier)\n",
    "    \n",
    "    # Training the attack\n",
    "    start = time.time()\n",
    "    try:\n",
    "        attacker.train(threat_model, num_samples=10)\n",
    "    except:\n",
    "        print(\"An exception occurred while training the threat model.\")\n",
    "    end = time.time()\n",
    "    print(\"Training time delta for the attacker: {}\".format(end-start))\n",
    "\n",
    "    # Testing the attack\n",
    "    start = time.time()\n",
    "    try:\n",
    "        summary = threat_model.test(attacker, num_samples=10)\n",
    "    except:\n",
    "        print(\"An exception occurred while testing the threat model.\")\n",
    "    end = time.time()\n",
    "    print(\"Testing time delta for the attacker: {}\".format(end-start))\n",
    "    \n",
    "    try:\n",
    "        metrics = summary.get_metrics()\n",
    "    except:\n",
    "        print(\"An exception occurred while getting metrics for the summary.\")\n",
    "        \n",
    "    # Defining metrics for data set of Breast Cancer\n",
    "    metrics[\"dataset\"] = \"Breast Cancer\"\n",
    "    \n",
    "    print(\"Metrics: \")\n",
    "    print(metrics)\n",
    "    \n",
    "    return summary, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9935ed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function that collects metrics and summaries from the attacks conducted on generators and targets\n",
    "def main():\n",
    "    # Initializing an empty data frame and all_summaries\n",
    "    all_metrics = pd.DataFrame()\n",
    "    all_summaries = []\n",
    "    \n",
    "    # Defining an array of generators\n",
    "    generators = [getGenerator(Generator.Raw, 0, 0, 0),\n",
    "                  getGenerator(Generator.CTGAN, 0, 0, 1), \n",
    "                  getGenerator(Generator.DPCTGAN, 1, 64, 1)]\n",
    "\n",
    "    # Looping through generators\n",
    "    for generator in generators: \n",
    "        # Looping through targets\n",
    "        for target in targets: \n",
    "            try:\n",
    "                summ, metr = attack(dataset=data, target_index=target, generator=generator)\n",
    "                all_summaries.append(summ)\n",
    "                all_metrics = pd.concat([all_metrics, metr], axis=0, ignore_index=True)\n",
    "                print(metr.head())\n",
    "            except Exception:\n",
    "                continue\n",
    "    return all_metrics, all_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb098720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 825.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 0.23276209831237793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1673.37it/s]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 0.14879107475280762\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        84       Raw  Groundhog       0.6                 0.6   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.4            0.2           0.8  0.72                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        84       Raw  Groundhog       0.6                 0.6   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.4            0.2           0.8  0.72                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 2007.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 0.17675089836120605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1856.38it/s]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 0.395251989364624\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        70       Raw  Groundhog       0.6                 0.6   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.4            0.2           0.8  0.68                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        70       Raw  Groundhog       0.6                 0.6   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.4            0.2           0.8  0.68                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1873.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 0.2001509666442871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1787.02it/s]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 0.13936686515808105\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        70       Raw  Groundhog       0.5                 0.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            0.0           1.0  0.6                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        70       Raw  Groundhog       0.5                 0.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            0.0           1.0  0.6                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1998.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 0.1834869384765625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1829.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 0.16707897186279297\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        90       Raw  Groundhog       0.7                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.6            0.4           0.6  0.76                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        90       Raw  Groundhog       0.7                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.6            0.4           0.6  0.76                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1211.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 0.21870207786560059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 2366.32it/s]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 0.1372051239013672\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        66       Raw  Groundhog       0.5                 0.2   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.2            0.0           1.0  0.66           1.386294  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        66       Raw  Groundhog       0.5                 0.2   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.2            0.0           1.0  0.66           1.386294  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 2246.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 0.18952393531799316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 2244.62it/s]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 0.1678481101989746\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       491       Raw  Groundhog       0.5                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  1.0            0.0           1.0  0.92                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       491       Raw  Groundhog       0.5                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  1.0            0.0           1.0  0.92                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1735.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 0.21111416816711426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 2295.61it/s]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 0.1347339153289795\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        73       Raw  Groundhog       0.7                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.6            0.4           0.6  0.88                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        73       Raw  Groundhog       0.7                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.6            0.4           0.6  0.88                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1817.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 0.23004603385925293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1112.99it/s]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 0.16250991821289062\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        57       Raw  Groundhog       0.7                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.6            0.4           0.6  0.86           1.609438  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        57       Raw  Groundhog       0.7                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.6            0.4           0.6  0.86           1.609438  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1956.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 0.18204641342163086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 2033.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 0.1876697540283203\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       491       Raw  Groundhog       0.9                 0.8   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.0            0.8           0.2  0.94                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       491       Raw  Groundhog       0.9                 0.8   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.0            0.8           0.2  0.94                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1997.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 0.21445298194885254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 2186.24it/s]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 0.12266707420349121\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       550       Raw  Groundhog       0.6                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.8            0.2           0.8  0.84                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       550       Raw  Groundhog       0.6                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain   auc  effective_epsilon  \n",
      "0                  0.8            0.2           0.8  0.84                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:40<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 40.3291437625885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:40<00:00,  4.04s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 40.49183368682861\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        84     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        84     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:37<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 37.57262897491455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:34<00:00,  3.41s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 34.23529386520386\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        70     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        70     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:34<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 35.14689612388611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:38<00:00,  3.83s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 38.41364097595215\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        70     CTGAN  Groundhog       0.9                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.2            0.8           0.2  1.0                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        70     CTGAN  Groundhog       0.9                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.2            0.8           0.2  1.0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:35<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 35.206493854522705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:34<00:00,  3.45s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 34.59374403953552\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        90     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        90     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:36<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 36.311142921447754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:34<00:00,  3.44s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 34.57537579536438\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        66     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        66     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:34<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 35.00323700904846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:34<00:00,  3.41s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 34.19427418708801\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       491     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       491     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:34<00:00,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 34.87544322013855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:32<00:00,  3.28s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 32.93224096298218\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        73     CTGAN  Groundhog       0.8                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.4            0.6           0.4  1.0                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        73     CTGAN  Groundhog       0.8                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.4            0.6           0.4  1.0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:36<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 36.338988065719604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:37<00:00,  3.78s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 37.94095587730408\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        57     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer        57     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:34<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 35.06590819358826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:35<00:00,  3.57s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 35.86284279823303\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       491     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       491     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:34<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 34.2682991027832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:33<00:00,  3.40s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 34.08963680267334\n",
      "Metrics: \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       550     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n",
      "         dataset target_id generator     attack  accuracy  true_positive_rate  \\\n",
      "0  Breast Cancer       550     CTGAN  Groundhog       1.0                 1.0   \n",
      "\n",
      "   false_positive_rate  mia_advantage  privacy_gain  auc  effective_epsilon  \n",
      "0                  0.0            1.0           0.0  1.0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0209, Loss D: 0.0001\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0192, Loss D: 0.0003\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0205, Loss D: 0.0037\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0213, Loss D: -0.0030\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0159, Loss D: -0.0010\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0139, Loss D: -0.0018\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0218, Loss D: -0.0036\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:19<02:51, 19.02s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0196, Loss D: 0.0002\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0150, Loss D: 0.0023\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0186, Loss D: 0.0002\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0112, Loss D: -0.0018\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0155, Loss D: 0.0092\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0151, Loss D: -0.0003\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0184, Loss D: -0.0104\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:36<02:25, 18.16s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0670, Loss D: -0.0035\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0665, Loss D: -0.0030\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0627, Loss D: 0.0045\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0671, Loss D: 0.0055\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0672, Loss D: 0.0021\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0622, Loss D: 0.0095\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0627, Loss D: -0.0020\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:54<02:06, 18.14s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0619, Loss D: -0.0045\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0592, Loss D: 0.0012\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0632, Loss D: 0.0057\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0659, Loss D: -0.0006\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0630, Loss D: 0.0074\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0621, Loss D: 0.0039\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0685, Loss D: 0.0029\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:12<01:47, 17.89s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0698, Loss D: 0.0019\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0741, Loss D: -0.0006\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0681, Loss D: -0.0032\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0690, Loss D: 0.0047\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0673, Loss D: -0.0042\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0713, Loss D: 0.0036\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0662, Loss D: 0.0032\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:29<01:29, 17.84s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0439, Loss D: 0.0050\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0427, Loss D: -0.0016\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0482, Loss D: -0.0002\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0427, Loss D: -0.0024\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0491, Loss D: 0.0025\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0456, Loss D: 0.0028\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0470, Loss D: 0.0027\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:48<01:12, 18.12s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0307, Loss D: 0.0025\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0326, Loss D: -0.0019\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0291, Loss D: 0.0044\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0267, Loss D: -0.0030\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0294, Loss D: 0.0052\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0307, Loss D: 0.0057\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0365, Loss D: 0.0029\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [02:13<01:00, 20.26s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0504, Loss D: 0.0054\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0498, Loss D: 0.0034\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0535, Loss D: 0.0066\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0502, Loss D: -0.0006\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0523, Loss D: 0.0017\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0516, Loss D: -0.0013\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0499, Loss D: -0.0062\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:33<00:40, 20.15s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0337, Loss D: 0.0068\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0368, Loss D: 0.0006\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0380, Loss D: 0.0015\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0396, Loss D: -0.0027\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0360, Loss D: -0.0017\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0338, Loss D: 0.0019\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0402, Loss D: -0.0048\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:51<00:19, 19.53s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0308, Loss D: -0.0049\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0252, Loss D: 0.0033\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0239, Loss D: 0.0027\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0266, Loss D: -0.0017\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0239, Loss D: 0.0013\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0251, Loss D: 0.0020\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0285, Loss D: 0.0027\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [03:12<00:00, 19.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 193.16021609306335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0398, Loss D: -0.0067\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0472, Loss D: 0.0034\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0473, Loss D: -0.0012\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0444, Loss D: 0.0007\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0400, Loss D: 0.0006\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0388, Loss D: 0.0017\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0451, Loss D: -0.0020\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:19<02:55, 19.48s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0561, Loss D: 0.0002\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0535, Loss D: -0.0047\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0530, Loss D: 0.0051\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0526, Loss D: 0.0082\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0511, Loss D: -0.0007\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0597, Loss D: 0.0010\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0556, Loss D: 0.0062\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:38<02:31, 18.93s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0140, Loss D: -0.0014\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0133, Loss D: -0.0052\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0190, Loss D: 0.0000\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0133, Loss D: 0.0061\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0141, Loss D: 0.0059\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0172, Loss D: -0.0002\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0160, Loss D: 0.0009\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:55<02:08, 18.37s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0131, Loss D: 0.0032\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0076, Loss D: 0.0012\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0104, Loss D: -0.0014\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0118, Loss D: 0.0065\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0119, Loss D: -0.0016\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0106, Loss D: -0.0006\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0148, Loss D: 0.0045\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:13<01:48, 18.07s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0698, Loss D: -0.0077\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0728, Loss D: 0.0005\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0673, Loss D: 0.0053\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0670, Loss D: 0.0019\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0718, Loss D: 0.0000\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0758, Loss D: 0.0008\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0730, Loss D: 0.0001\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:30<01:29, 17.86s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0314, Loss D: 0.0026\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0298, Loss D: -0.0013\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0283, Loss D: -0.0003\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0294, Loss D: 0.0028\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0299, Loss D: 0.0020\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0282, Loss D: 0.0026\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0306, Loss D: 0.0006\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:47<01:10, 17.53s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0476, Loss D: -0.0045\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0419, Loss D: -0.0022\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0491, Loss D: -0.0054\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0425, Loss D: -0.0010\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0466, Loss D: 0.0007\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0411, Loss D: 0.0027\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0410, Loss D: 0.0023\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [02:04<00:51, 17.20s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0149, Loss D: 0.0000\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0163, Loss D: -0.0010\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0185, Loss D: 0.0005\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0158, Loss D: -0.0022\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0207, Loss D: -0.0021\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0167, Loss D: 0.0037\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0107, Loss D: 0.0004\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:20<00:33, 16.96s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0299, Loss D: 0.0027\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0253, Loss D: 0.0024\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0257, Loss D: -0.0017\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0246, Loss D: 0.0049\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0238, Loss D: 0.0096\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0250, Loss D: 0.0103\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0240, Loss D: 0.0042\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:36<00:16, 16.75s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0423, Loss D: 0.0053\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0404, Loss D: 0.0002\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0428, Loss D: 0.0038\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0417, Loss D: -0.0006\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0444, Loss D: -0.0006\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0424, Loss D: 0.0026\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0420, Loss D: -0.0003\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:53<00:00, 17.35s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 173.61134910583496\n",
      "Metrics: \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        84  DP-CTGAN (eps=1)  Groundhog       0.7   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.6                  0.2            0.4           0.6  0.74   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        84  DP-CTGAN (eps=1)  Groundhog       0.7   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.6                  0.2            0.4           0.6  0.74   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0331, Loss D: 0.0060\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0346, Loss D: 0.0068\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0353, Loss D: 0.0013\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0312, Loss D: 0.0004\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0308, Loss D: -0.0024\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0337, Loss D: 0.0015\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0330, Loss D: 0.0013\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:16<02:29, 16.59s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0627, Loss D: -0.0015\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0608, Loss D: -0.0065\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0654, Loss D: -0.0037\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0673, Loss D: -0.0028\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0610, Loss D: 0.0026\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0628, Loss D: -0.0019\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0650, Loss D: 0.0029\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:33<02:12, 16.60s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0359, Loss D: -0.0006\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0379, Loss D: 0.0057\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0353, Loss D: 0.0007\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0348, Loss D: 0.0036\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0404, Loss D: -0.0033\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0382, Loss D: 0.0029\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0367, Loss D: 0.0023\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:49<01:56, 16.60s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0147, Loss D: -0.0030\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0147, Loss D: 0.0042\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0143, Loss D: -0.0030\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0137, Loss D: 0.0010\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0120, Loss D: 0.0001\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0194, Loss D: -0.0033\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0140, Loss D: 0.0063\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:08<01:43, 17.31s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0388, Loss D: -0.0068\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0329, Loss D: 0.0004\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0383, Loss D: 0.0006\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0384, Loss D: -0.0008\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0396, Loss D: 0.0076\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0370, Loss D: -0.0039\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0411, Loss D: 0.0052\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:24<01:25, 17.07s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0338, Loss D: -0.0014\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0385, Loss D: 0.0024\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0350, Loss D: -0.0001\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0336, Loss D: -0.0000\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0316, Loss D: -0.0024\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0335, Loss D: 0.0055\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0368, Loss D: -0.0004\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:41<01:07, 16.94s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0454, Loss D: 0.0012\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0440, Loss D: 0.0016\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0431, Loss D: 0.0037\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0451, Loss D: 0.0029\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0410, Loss D: -0.0036\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0435, Loss D: 0.0082\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0451, Loss D: 0.0016\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:58<00:50, 16.88s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0389, Loss D: 0.0046\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0354, Loss D: -0.0014\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0358, Loss D: 0.0062\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0374, Loss D: 0.0071\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0377, Loss D: -0.0040\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0368, Loss D: 0.0065\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0332, Loss D: 0.0092\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:14<00:33, 16.80s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0337, Loss D: -0.0049\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0349, Loss D: 0.0003\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0357, Loss D: 0.0048\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0348, Loss D: 0.0047\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0347, Loss D: 0.0015\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0301, Loss D: 0.0020\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0315, Loss D: 0.0011\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:31<00:16, 16.77s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0633, Loss D: 0.0002\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0662, Loss D: 0.0043\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0653, Loss D: -0.0020\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0652, Loss D: -0.0025\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0658, Loss D: -0.0019\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0626, Loss D: 0.0038\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0679, Loss D: 0.0001\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:48<00:00, 16.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 168.50265979766846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0557, Loss D: -0.0020\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0564, Loss D: 0.0028\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0566, Loss D: 0.0029\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0536, Loss D: -0.0050\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0510, Loss D: 0.0006\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0581, Loss D: -0.0076\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0583, Loss D: 0.0049\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:16<02:30, 16.75s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0477, Loss D: -0.0006\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0467, Loss D: 0.0008\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0521, Loss D: 0.0035\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0437, Loss D: -0.0008\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0444, Loss D: -0.0079\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0447, Loss D: 0.0051\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0456, Loss D: 0.0045\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:33<02:14, 16.87s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0171, Loss D: 0.0036\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0152, Loss D: 0.0080\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0173, Loss D: 0.0020\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0148, Loss D: 0.0050\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0139, Loss D: -0.0049\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0139, Loss D: 0.0061\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0163, Loss D: 0.0022\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:52<02:02, 17.55s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0127, Loss D: -0.0008\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0178, Loss D: -0.0086\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0143, Loss D: -0.0034\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0185, Loss D: -0.0075\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0145, Loss D: 0.0024\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0194, Loss D: -0.0012\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0120, Loss D: 0.0056\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:08<01:43, 17.18s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0427, Loss D: -0.0054\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0387, Loss D: 0.0004\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0362, Loss D: 0.0065\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0384, Loss D: 0.0101\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0360, Loss D: 0.0018\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0389, Loss D: -0.0040\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0386, Loss D: 0.0050\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:25<01:24, 16.92s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0351, Loss D: -0.0038\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0367, Loss D: 0.0007\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0376, Loss D: 0.0014\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0395, Loss D: -0.0013\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0373, Loss D: -0.0004\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0376, Loss D: 0.0091\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0397, Loss D: 0.0009\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:42<01:08, 17.12s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0483, Loss D: -0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0468, Loss D: -0.0001\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0470, Loss D: -0.0010\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0524, Loss D: -0.0001\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0476, Loss D: 0.0049\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0467, Loss D: 0.0025\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0465, Loss D: 0.0091\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:59<00:51, 17.12s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0204, Loss D: 0.0016\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0195, Loss D: -0.0003\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0218, Loss D: 0.0071\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0227, Loss D: 0.0060\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0229, Loss D: -0.0012\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0291, Loss D: 0.0018\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0241, Loss D: 0.0011\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:16<00:33, 16.93s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0043, Loss D: 0.0006\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0029, Loss D: 0.0030\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0060, Loss D: 0.0043\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0037, Loss D: 0.0003\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0008, Loss D: 0.0023\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0023, Loss D: 0.0014\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0007, Loss D: 0.0003\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:32<00:16, 16.84s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0462, Loss D: 0.0023\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0440, Loss D: 0.0066\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0517, Loss D: 0.0039\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0485, Loss D: -0.0017\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0426, Loss D: 0.0041\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0466, Loss D: -0.0001\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0507, Loss D: 0.0025\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:49<00:00, 16.99s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 169.99311089515686\n",
      "Metrics: \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        70  DP-CTGAN (eps=1)  Groundhog       0.6   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.6                  0.4            0.2           0.8  0.78   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        70  DP-CTGAN (eps=1)  Groundhog       0.6   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.6                  0.4            0.2           0.8  0.78   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0388, Loss D: 0.0032\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0396, Loss D: -0.0037\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0347, Loss D: 0.0004\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0388, Loss D: -0.0026\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0403, Loss D: 0.0052\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0373, Loss D: -0.0016\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0315, Loss D: 0.0003\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:16<02:30, 16.67s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0662, Loss D: 0.0019\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0676, Loss D: -0.0002\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0649, Loss D: 0.0027\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0615, Loss D: 0.0042\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0607, Loss D: 0.0037\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0653, Loss D: 0.0011\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0626, Loss D: 0.0021\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:33<02:13, 16.73s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0369, Loss D: -0.0038\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0354, Loss D: 0.0037\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0340, Loss D: 0.0052\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0344, Loss D: -0.0027\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0334, Loss D: -0.0045\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0316, Loss D: 0.0030\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0366, Loss D: -0.0038\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:50<01:57, 16.78s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0368, Loss D: -0.0077\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0337, Loss D: -0.0019\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0348, Loss D: 0.0036\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0358, Loss D: 0.0007\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0363, Loss D: -0.0005\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0349, Loss D: -0.0066\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0320, Loss D: -0.0013\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:14<01:57, 19.62s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0124, Loss D: 0.0019\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0099, Loss D: 0.0052\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0087, Loss D: -0.0004\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0155, Loss D: 0.0051\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0100, Loss D: 0.0008\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0123, Loss D: 0.0034\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0096, Loss D: -0.0030\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:32<01:35, 19.08s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0101, Loss D: 0.0013\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0136, Loss D: 0.0036\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0110, Loss D: -0.0002\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0098, Loss D: 0.0064\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0143, Loss D: 0.0079\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0144, Loss D: 0.0011\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0128, Loss D: 0.0022\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:49<01:14, 18.55s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0465, Loss D: -0.0026\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0505, Loss D: -0.0012\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0506, Loss D: 0.0006\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0460, Loss D: 0.0004\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0507, Loss D: -0.0001\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0524, Loss D: 0.0019\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0460, Loss D: 0.0043\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [02:06<00:53, 17.99s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0331, Loss D: 0.0012\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0357, Loss D: -0.0001\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0335, Loss D: 0.0070\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0406, Loss D: 0.0072\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0376, Loss D: 0.0064\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0338, Loss D: 0.0041\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0331, Loss D: -0.0008\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:24<00:35, 17.78s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0320, Loss D: 0.0044\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0396, Loss D: 0.0059\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0306, Loss D: -0.0011\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0408, Loss D: 0.0062\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0399, Loss D: 0.0030\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0352, Loss D: 0.0049\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0329, Loss D: -0.0005\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:41<00:17, 17.57s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0340, Loss D: -0.0046\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0386, Loss D: 0.0029\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0401, Loss D: 0.0008\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0401, Loss D: -0.0003\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0376, Loss D: 0.0051\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0376, Loss D: -0.0028\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0388, Loss D: 0.0060\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:59<00:00, 17.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 179.28112196922302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0430, Loss D: 0.0054\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0441, Loss D: -0.0020\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0424, Loss D: -0.0034\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0477, Loss D: -0.0011\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0415, Loss D: 0.0058\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0442, Loss D: 0.0033\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0380, Loss D: 0.0017\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:18<02:46, 18.54s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0382, Loss D: 0.0010\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0376, Loss D: 0.0026\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0345, Loss D: 0.0035\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0360, Loss D: -0.0018\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0423, Loss D: 0.0007\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0435, Loss D: 0.0019\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0338, Loss D: -0.0013\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:36<02:23, 17.95s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0633, Loss D: 0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0664, Loss D: 0.0029\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0642, Loss D: 0.0065\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0669, Loss D: -0.0050\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0659, Loss D: 0.0063\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0667, Loss D: -0.0001\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0643, Loss D: 0.0012\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:52<02:00, 17.23s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0427, Loss D: -0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0363, Loss D: -0.0011\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0366, Loss D: -0.0003\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0379, Loss D: 0.0072\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0386, Loss D: 0.0037\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0334, Loss D: 0.0015\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0385, Loss D: -0.0022\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:08<01:39, 16.63s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0460, Loss D: 0.0038\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0489, Loss D: -0.0012\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0493, Loss D: -0.0017\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0484, Loss D: 0.0025\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0482, Loss D: 0.0029\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0488, Loss D: -0.0030\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0427, Loss D: 0.0002\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:24<01:22, 16.50s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0133, Loss D: -0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0164, Loss D: 0.0018\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0130, Loss D: -0.0017\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0144, Loss D: 0.0049\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0152, Loss D: -0.0017\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0104, Loss D: 0.0035\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0159, Loss D: 0.0035\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:40<01:05, 16.33s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0121, Loss D: -0.0019\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0099, Loss D: -0.0005\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0104, Loss D: 0.0013\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0133, Loss D: -0.0017\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0061, Loss D: -0.0004\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0112, Loss D: 0.0036\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0087, Loss D: 0.0043\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [02:00<00:52, 17.64s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0315, Loss D: -0.0066\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0302, Loss D: -0.0017\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0283, Loss D: 0.0045\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0297, Loss D: 0.0002\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0317, Loss D: 0.0002\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0336, Loss D: 0.0000\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0303, Loss D: 0.0017\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:19<00:35, 17.83s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0366, Loss D: 0.0071\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0310, Loss D: -0.0008\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0338, Loss D: 0.0074\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0346, Loss D: 0.0010\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0362, Loss D: 0.0067\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0348, Loss D: 0.0073\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0343, Loss D: -0.0001\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:34<00:16, 16.94s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0475, Loss D: -0.0048\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0452, Loss D: -0.0035\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0470, Loss D: -0.0026\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0505, Loss D: 0.0013\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0494, Loss D: 0.0053\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0477, Loss D: -0.0002\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0478, Loss D: -0.0014\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:49<00:00, 16.93s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 169.41314888000488\n",
      "Metrics: \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        70  DP-CTGAN (eps=1)  Groundhog       0.5   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.4                  0.4            0.0           1.0  0.58   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        70  DP-CTGAN (eps=1)  Groundhog       0.5   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.4                  0.4            0.0           1.0  0.58   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0720, Loss D: 0.0026\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0727, Loss D: 0.0004\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0671, Loss D: 0.0014\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0699, Loss D: 0.0008\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0760, Loss D: 0.0047\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0716, Loss D: 0.0021\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0729, Loss D: 0.0011\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:15<02:15, 15.10s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0511, Loss D: -0.0050\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0569, Loss D: 0.0019\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0567, Loss D: -0.0073\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0514, Loss D: -0.0013\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0536, Loss D: 0.0006\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0517, Loss D: -0.0024\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0561, Loss D: 0.0095\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:30<02:00, 15.10s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0725, Loss D: -0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0700, Loss D: 0.0038\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0675, Loss D: -0.0016\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0696, Loss D: 0.0004\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0721, Loss D: 0.0036\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0727, Loss D: 0.0067\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0719, Loss D: -0.0012\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:45<01:47, 15.33s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0422, Loss D: 0.0006\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0402, Loss D: -0.0045\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0356, Loss D: 0.0014\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0378, Loss D: -0.0012\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0380, Loss D: 0.0035\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0351, Loss D: 0.0034\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0354, Loss D: 0.0041\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:01<01:33, 15.50s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0341, Loss D: 0.0011\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0367, Loss D: 0.0045\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0319, Loss D: 0.0020\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0326, Loss D: -0.0007\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0383, Loss D: 0.0002\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0330, Loss D: 0.0011\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0346, Loss D: 0.0006\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:16<01:17, 15.42s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0468, Loss D: 0.0041\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0490, Loss D: 0.0001\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0494, Loss D: 0.0020\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0536, Loss D: 0.0010\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0464, Loss D: 0.0058\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0446, Loss D: -0.0043\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0480, Loss D: -0.0004\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:32<01:02, 15.56s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0221, Loss D: -0.0062\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0219, Loss D: 0.0039\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0176, Loss D: -0.0030\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0151, Loss D: -0.0027\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0114, Loss D: 0.0050\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0128, Loss D: -0.0018\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0103, Loss D: 0.0039\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:47<00:46, 15.39s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0374, Loss D: -0.0044\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0395, Loss D: 0.0011\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0426, Loss D: 0.0011\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0390, Loss D: 0.0030\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0363, Loss D: 0.0087\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0392, Loss D: -0.0036\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0385, Loss D: -0.0002\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:03<00:31, 15.50s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0367, Loss D: 0.0026\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0371, Loss D: 0.0040\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0387, Loss D: -0.0053\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0418, Loss D: -0.0014\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0399, Loss D: 0.0031\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0362, Loss D: 0.0048\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0390, Loss D: 0.0024\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:19<00:15, 15.78s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0408, Loss D: 0.0004\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0405, Loss D: -0.0002\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0397, Loss D: -0.0002\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0380, Loss D: -0.0028\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0355, Loss D: 0.0040\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0396, Loss D: -0.0038\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0416, Loss D: 0.0056\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:36<00:00, 15.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 156.2742838859558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0034, Loss D: -0.0020\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0019, Loss D: -0.0018\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0041, Loss D: 0.0006\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0057, Loss D: 0.0042\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0081, Loss D: 0.0065\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0092, Loss D: -0.0014\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0082, Loss D: -0.0009\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:15<02:16, 15.17s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0292, Loss D: -0.0011\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0270, Loss D: 0.0000\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0242, Loss D: -0.0032\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0293, Loss D: 0.0035\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0264, Loss D: 0.0103\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0234, Loss D: 0.0016\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0297, Loss D: 0.0032\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:31<02:06, 15.76s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0336, Loss D: 0.0016\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0388, Loss D: -0.0022\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0363, Loss D: -0.0065\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0381, Loss D: -0.0015\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0406, Loss D: 0.0003\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0376, Loss D: 0.0063\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0343, Loss D: 0.0045\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:48<01:55, 16.48s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0342, Loss D: -0.0066\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0374, Loss D: 0.0034\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0368, Loss D: 0.0046\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0357, Loss D: -0.0058\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0364, Loss D: 0.0036\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0397, Loss D: 0.0036\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0432, Loss D: 0.0028\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:06<01:41, 16.90s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0428, Loss D: -0.0038\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0390, Loss D: -0.0030\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0408, Loss D: -0.0031\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0362, Loss D: 0.0045\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0371, Loss D: 0.0067\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0381, Loss D: -0.0013\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0364, Loss D: 0.0051\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:23<01:24, 16.93s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0525, Loss D: -0.0002\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0474, Loss D: 0.0027\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0501, Loss D: 0.0009\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0526, Loss D: 0.0044\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0563, Loss D: 0.0009\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0520, Loss D: -0.0061\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0527, Loss D: 0.0022\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:39<01:06, 16.59s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0059, Loss D: 0.0003\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0112, Loss D: 0.0078\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0074, Loss D: -0.0024\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0178, Loss D: 0.0069\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0159, Loss D: -0.0041\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0114, Loss D: -0.0007\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0147, Loss D: -0.0004\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:54<00:48, 16.10s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0676, Loss D: 0.0034\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0708, Loss D: 0.0039\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0695, Loss D: 0.0004\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0692, Loss D: 0.0055\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0702, Loss D: 0.0019\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0721, Loss D: 0.0020\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0703, Loss D: 0.0008\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:13<00:33, 16.96s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0234, Loss D: -0.0054\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0219, Loss D: -0.0023\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0219, Loss D: -0.0038\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0217, Loss D: 0.0000\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0235, Loss D: 0.0028\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0238, Loss D: -0.0007\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0225, Loss D: 0.0008\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:29<00:16, 16.68s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0365, Loss D: 0.0055\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0340, Loss D: 0.0030\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0433, Loss D: 0.0020\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0381, Loss D: 0.0008\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0394, Loss D: -0.0016\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0336, Loss D: -0.0049\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0415, Loss D: 0.0036\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:43<00:00, 16.34s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 163.4607789516449\n",
      "Metrics: \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        90  DP-CTGAN (eps=1)  Groundhog       0.5   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 1.0                  1.0            0.0           1.0  0.56   \n",
      "\n",
      "   effective_epsilon  \n",
      "0           0.287682  \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        90  DP-CTGAN (eps=1)  Groundhog       0.5   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 1.0                  1.0            0.0           1.0  0.56   \n",
      "\n",
      "   effective_epsilon  \n",
      "0           0.287682  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0484, Loss D: -0.0011\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0525, Loss D: 0.0014\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0457, Loss D: -0.0058\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0529, Loss D: 0.0034\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0492, Loss D: 0.0014\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0516, Loss D: -0.0033\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0506, Loss D: 0.0003\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:15<02:22, 15.78s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0491, Loss D: -0.0047\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0508, Loss D: -0.0024\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0498, Loss D: 0.0028\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0495, Loss D: -0.0046\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0506, Loss D: -0.0005\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0479, Loss D: 0.0022\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0511, Loss D: -0.0036\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:32<02:10, 16.26s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0382, Loss D: -0.0001\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0326, Loss D: 0.0009\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0411, Loss D: -0.0041\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0324, Loss D: 0.0022\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0367, Loss D: 0.0030\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0375, Loss D: 0.0014\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0346, Loss D: 0.0036\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:49<01:57, 16.72s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0437, Loss D: 0.0054\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0455, Loss D: 0.0005\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0414, Loss D: 0.0007\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0437, Loss D: -0.0007\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0446, Loss D: -0.0036\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0440, Loss D: -0.0017\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0414, Loss D: -0.0010\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:05<01:37, 16.29s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0548, Loss D: -0.0105\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0553, Loss D: -0.0003\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0559, Loss D: -0.0013\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0565, Loss D: 0.0071\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0538, Loss D: -0.0018\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0542, Loss D: 0.0035\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0510, Loss D: 0.0027\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:21<01:21, 16.26s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0197, Loss D: 0.0011\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0130, Loss D: -0.0032\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0214, Loss D: 0.0004\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0186, Loss D: 0.0015\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0218, Loss D: 0.0020\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0200, Loss D: 0.0012\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0179, Loss D: 0.0022\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:38<01:06, 16.67s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0314, Loss D: 0.0081\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0353, Loss D: 0.0023\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0402, Loss D: -0.0002\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0369, Loss D: 0.0022\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0383, Loss D: -0.0029\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0372, Loss D: 0.0070\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0358, Loss D: -0.0029\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:55<00:49, 16.49s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0128, Loss D: 0.0028\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0129, Loss D: 0.0045\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0139, Loss D: -0.0009\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0122, Loss D: 0.0006\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0146, Loss D: 0.0034\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0140, Loss D: 0.0020\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0144, Loss D: -0.0020\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:09<00:31, 15.84s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0066, Loss D: -0.0025\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0066, Loss D: 0.0049\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0037, Loss D: -0.0019\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0035, Loss D: 0.0010\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0030, Loss D: 0.0027\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0027, Loss D: 0.0003\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0053, Loss D: -0.0009\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:24<00:15, 15.52s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0510, Loss D: -0.0032\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0480, Loss D: 0.0013\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0443, Loss D: -0.0071\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0456, Loss D: -0.0005\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0475, Loss D: -0.0010\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0443, Loss D: -0.0010\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0526, Loss D: -0.0024\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:40<00:00, 16.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 160.81858897209167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0335, Loss D: 0.0011\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0309, Loss D: -0.0036\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0364, Loss D: 0.0032\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0336, Loss D: 0.0016\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0300, Loss D: -0.0064\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0366, Loss D: 0.0010\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0326, Loss D: 0.0050\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:16<02:27, 16.37s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0133, Loss D: -0.0067\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0112, Loss D: -0.0019\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0151, Loss D: 0.0019\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0054, Loss D: -0.0021\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0119, Loss D: 0.0050\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0093, Loss D: -0.0029\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0071, Loss D: 0.0010\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:33<02:15, 16.99s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0090, Loss D: 0.0042\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0139, Loss D: 0.0014\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0130, Loss D: 0.0020\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0106, Loss D: -0.0011\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0073, Loss D: -0.0042\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0117, Loss D: 0.0088\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0080, Loss D: 0.0040\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:50<01:56, 16.71s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0228, Loss D: 0.0001\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0217, Loss D: -0.0062\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0227, Loss D: 0.0037\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0201, Loss D: 0.0064\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0201, Loss D: 0.0107\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0180, Loss D: -0.0023\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0237, Loss D: 0.0052\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:06<01:40, 16.71s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0321, Loss D: -0.0021\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0335, Loss D: 0.0032\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0347, Loss D: -0.0002\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0381, Loss D: -0.0020\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0360, Loss D: 0.0025\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0309, Loss D: -0.0036\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0387, Loss D: 0.0023\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:21<01:20, 16.03s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0293, Loss D: 0.0002\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0406, Loss D: -0.0051\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0343, Loss D: 0.0028\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0308, Loss D: 0.0022\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0351, Loss D: 0.0051\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0354, Loss D: -0.0008\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0329, Loss D: 0.0015\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:38<01:04, 16.20s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0376, Loss D: -0.0033\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0347, Loss D: -0.0001\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0365, Loss D: 0.0020\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0354, Loss D: 0.0021\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0394, Loss D: 0.0071\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0342, Loss D: 0.0012\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0396, Loss D: -0.0072\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:53<00:47, 15.97s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0374, Loss D: -0.0017\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0418, Loss D: -0.0027\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0419, Loss D: 0.0077\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0404, Loss D: -0.0021\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0329, Loss D: -0.0028\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0376, Loss D: -0.0009\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0366, Loss D: -0.0062\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:09<00:31, 15.95s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0660, Loss D: -0.0027\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0652, Loss D: -0.0055\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0669, Loss D: 0.0083\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0633, Loss D: -0.0015\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0636, Loss D: 0.0042\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0660, Loss D: 0.0047\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0608, Loss D: 0.0098\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:25<00:15, 15.95s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0656, Loss D: -0.0038\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0641, Loss D: 0.0003\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0665, Loss D: 0.0040\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0647, Loss D: 0.0009\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0645, Loss D: -0.0049\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0671, Loss D: 0.0097\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0639, Loss D: 0.0005\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:41<00:00, 16.13s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 161.45936107635498\n",
      "Metrics: \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        66  DP-CTGAN (eps=1)  Groundhog       0.6   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain  auc  \\\n",
      "0                 1.0                  0.8            0.2           0.8  0.8   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        66  DP-CTGAN (eps=1)  Groundhog       0.6   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain  auc  \\\n",
      "0                 1.0                  0.8            0.2           0.8  0.8   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0208, Loss D: 0.0063\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0188, Loss D: 0.0001\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0261, Loss D: 0.0023\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0175, Loss D: 0.0009\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0215, Loss D: -0.0095\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0136, Loss D: 0.0010\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0140, Loss D: -0.0019\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:16<02:25, 16.11s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0412, Loss D: 0.0009\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0363, Loss D: 0.0074\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0367, Loss D: -0.0003\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0293, Loss D: 0.0068\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0407, Loss D: 0.0051\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0349, Loss D: 0.0024\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0335, Loss D: 0.0022\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:32<02:09, 16.19s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0736, Loss D: -0.0053\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0736, Loss D: 0.0019\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0700, Loss D: -0.0001\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0688, Loss D: 0.0047\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0696, Loss D: -0.0049\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0645, Loss D: 0.0050\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0734, Loss D: 0.0006\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:48<01:52, 16.01s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0152, Loss D: -0.0039\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0111, Loss D: 0.0006\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0194, Loss D: 0.0053\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0095, Loss D: 0.0027\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0098, Loss D: 0.0034\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0134, Loss D: -0.0014\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0101, Loss D: 0.0050\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:03<01:34, 15.69s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0488, Loss D: -0.0012\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0465, Loss D: -0.0018\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0460, Loss D: 0.0015\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0527, Loss D: 0.0001\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0431, Loss D: 0.0000\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0475, Loss D: 0.0051\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0458, Loss D: 0.0044\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:19<01:19, 15.82s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0684, Loss D: -0.0019\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0706, Loss D: -0.0057\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0698, Loss D: -0.0003\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0695, Loss D: -0.0024\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0748, Loss D: 0.0069\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0694, Loss D: 0.0008\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0664, Loss D: -0.0067\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:36<01:05, 16.41s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0716, Loss D: -0.0065\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0716, Loss D: -0.0048\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0743, Loss D: 0.0079\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0705, Loss D: -0.0049\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0694, Loss D: 0.0031\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0700, Loss D: 0.0035\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0723, Loss D: 0.0069\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:55<00:51, 17.14s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0510, Loss D: -0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0521, Loss D: 0.0015\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0520, Loss D: 0.0046\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0519, Loss D: -0.0019\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0509, Loss D: 0.0002\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0497, Loss D: -0.0044\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0523, Loss D: 0.0018\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:12<00:33, 16.90s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0461, Loss D: -0.0013\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0475, Loss D: -0.0013\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0484, Loss D: 0.0035\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0466, Loss D: 0.0066\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0478, Loss D: -0.0007\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0491, Loss D: 0.0008\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0423, Loss D: 0.0059\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:30<00:17, 17.30s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0502, Loss D: -0.0026\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0507, Loss D: 0.0015\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0498, Loss D: 0.0081\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0476, Loss D: -0.0010\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0486, Loss D: 0.0038\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0492, Loss D: 0.0007\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0485, Loss D: 0.0011\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:47<00:00, 16.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 167.3633131980896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0706, Loss D: -0.0008\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0734, Loss D: 0.0097\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0713, Loss D: -0.0032\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0688, Loss D: 0.0016\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0715, Loss D: -0.0018\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0717, Loss D: 0.0000\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0715, Loss D: 0.0025\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:14<02:13, 14.88s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0575, Loss D: -0.0001\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0522, Loss D: 0.0054\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0554, Loss D: 0.0002\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0540, Loss D: 0.0012\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0540, Loss D: 0.0046\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0564, Loss D: 0.0036\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0522, Loss D: 0.0008\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:29<01:59, 14.94s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0515, Loss D: 0.0043\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0449, Loss D: -0.0009\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0487, Loss D: -0.0001\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0471, Loss D: -0.0059\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0468, Loss D: 0.0007\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0477, Loss D: -0.0043\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0540, Loss D: 0.0017\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:44<01:44, 14.99s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0748, Loss D: -0.0036\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0742, Loss D: 0.0022\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0702, Loss D: 0.0017\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0771, Loss D: 0.0056\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0731, Loss D: -0.0005\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0794, Loss D: -0.0006\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0780, Loss D: 0.0030\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [00:59<01:28, 14.81s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0617, Loss D: 0.0010\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0549, Loss D: -0.0025\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0599, Loss D: 0.0036\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0585, Loss D: 0.0011\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0560, Loss D: -0.0001\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0561, Loss D: 0.0014\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0604, Loss D: -0.0014\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:14<01:14, 14.83s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0658, Loss D: 0.0052\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0670, Loss D: 0.0008\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0637, Loss D: -0.0019\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0664, Loss D: -0.0035\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0617, Loss D: 0.0015\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0678, Loss D: 0.0023\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0639, Loss D: 0.0067\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:28<00:58, 14.65s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0335, Loss D: 0.0029\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0318, Loss D: 0.0050\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0321, Loss D: -0.0031\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0355, Loss D: -0.0001\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0359, Loss D: 0.0022\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0337, Loss D: -0.0004\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0346, Loss D: -0.0014\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:43<00:43, 14.59s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0077, Loss D: 0.0005\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0148, Loss D: 0.0015\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0160, Loss D: 0.0004\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0127, Loss D: 0.0014\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0152, Loss D: 0.0030\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0096, Loss D: 0.0046\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0118, Loss D: 0.0009\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [01:57<00:29, 14.52s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0654, Loss D: 0.0046\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0641, Loss D: 0.0035\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0672, Loss D: -0.0039\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0651, Loss D: 0.0074\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0662, Loss D: 0.0026\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0632, Loss D: -0.0018\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0648, Loss D: 0.0097\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:12<00:14, 14.75s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0478, Loss D: 0.0020\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0443, Loss D: -0.0033\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0426, Loss D: 0.0029\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0498, Loss D: 0.0013\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0464, Loss D: 0.0004\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0492, Loss D: -0.0031\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0459, Loss D: 0.0016\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:27<00:00, 14.76s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 147.71361374855042\n",
      "Metrics: \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer       491  DP-CTGAN (eps=1)  Groundhog       0.9   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain  auc  \\\n",
      "0                 1.0                  0.2            0.8           0.2  0.8   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer       491  DP-CTGAN (eps=1)  Groundhog       0.9   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain  auc  \\\n",
      "0                 1.0                  0.2            0.8           0.2  0.8   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0597, Loss D: -0.0005\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0590, Loss D: 0.0004\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0598, Loss D: 0.0012\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0607, Loss D: 0.0048\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0596, Loss D: 0.0018\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0617, Loss D: -0.0013\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0618, Loss D: 0.0108\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:15<02:16, 15.20s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0379, Loss D: 0.0013\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0390, Loss D: 0.0021\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0370, Loss D: -0.0035\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0359, Loss D: 0.0058\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0398, Loss D: 0.0015\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0322, Loss D: -0.0041\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0356, Loss D: -0.0026\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:31<02:06, 15.77s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0689, Loss D: -0.0030\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0704, Loss D: 0.0010\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0682, Loss D: 0.0017\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0696, Loss D: 0.0050\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0695, Loss D: 0.0009\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0654, Loss D: -0.0003\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0692, Loss D: 0.0060\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:46<01:49, 15.67s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0614, Loss D: 0.0000\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0685, Loss D: 0.0047\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0753, Loss D: -0.0007\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0703, Loss D: 0.0014\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0680, Loss D: -0.0040\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0687, Loss D: -0.0037\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0651, Loss D: -0.0000\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:02<01:34, 15.81s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0702, Loss D: -0.0027\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0715, Loss D: 0.0038\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0691, Loss D: 0.0004\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0726, Loss D: 0.0013\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0724, Loss D: -0.0012\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0730, Loss D: -0.0038\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0687, Loss D: 0.0058\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:18<01:19, 15.86s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0694, Loss D: -0.0077\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0654, Loss D: -0.0045\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0686, Loss D: 0.0016\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0723, Loss D: 0.0096\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0713, Loss D: 0.0020\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0683, Loss D: 0.0048\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0739, Loss D: 0.0060\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:34<01:03, 15.77s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0640, Loss D: -0.0009\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0628, Loss D: 0.0034\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0633, Loss D: -0.0003\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0642, Loss D: -0.0014\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0635, Loss D: 0.0041\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0652, Loss D: 0.0001\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0631, Loss D: -0.0015\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:50<00:47, 15.87s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0633, Loss D: -0.0023\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0646, Loss D: 0.0062\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0652, Loss D: 0.0008\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0625, Loss D: 0.0069\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0648, Loss D: -0.0023\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0668, Loss D: -0.0001\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0656, Loss D: -0.0016\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:05<00:31, 15.73s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0252, Loss D: -0.0079\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0221, Loss D: 0.0020\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0233, Loss D: 0.0012\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0233, Loss D: 0.0073\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0186, Loss D: 0.0031\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0216, Loss D: -0.0022\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0198, Loss D: 0.0014\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:22<00:15, 15.97s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0333, Loss D: 0.0002\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0346, Loss D: 0.0037\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0323, Loss D: -0.0052\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0328, Loss D: 0.0004\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0381, Loss D: 0.0025\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0333, Loss D: 0.0027\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0359, Loss D: 0.0082\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:38<00:00, 15.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 158.7164340019226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0384, Loss D: 0.0022\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0330, Loss D: -0.0004\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0361, Loss D: 0.0061\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0356, Loss D: -0.0011\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0370, Loss D: 0.0079\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0363, Loss D: -0.0000\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0371, Loss D: 0.0061\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:15<02:21, 15.72s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0341, Loss D: 0.0051\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0324, Loss D: 0.0003\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0360, Loss D: 0.0051\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0353, Loss D: 0.0050\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0355, Loss D: 0.0030\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0360, Loss D: 0.0083\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0326, Loss D: 0.0006\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:31<02:04, 15.57s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0537, Loss D: 0.0065\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0528, Loss D: 0.0024\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0526, Loss D: -0.0018\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0509, Loss D: -0.0019\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0542, Loss D: 0.0057\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0502, Loss D: 0.0023\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0505, Loss D: -0.0067\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:47<01:50, 15.73s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0167, Loss D: 0.0012\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0172, Loss D: 0.0003\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0174, Loss D: -0.0008\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0109, Loss D: -0.0042\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0156, Loss D: -0.0045\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0210, Loss D: -0.0018\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0127, Loss D: 0.0051\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:04<01:37, 16.30s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0330, Loss D: 0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0356, Loss D: -0.0025\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0333, Loss D: 0.0071\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0347, Loss D: -0.0001\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0352, Loss D: 0.0008\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0357, Loss D: 0.0021\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0324, Loss D: 0.0016\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:21<01:23, 16.66s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0364, Loss D: 0.0017\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0315, Loss D: 0.0049\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0367, Loss D: 0.0003\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0341, Loss D: -0.0009\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0346, Loss D: 0.0059\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0359, Loss D: 0.0018\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0358, Loss D: -0.0066\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:38<01:07, 16.84s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0667, Loss D: 0.0034\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0619, Loss D: 0.0048\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0659, Loss D: -0.0023\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0634, Loss D: 0.0017\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0661, Loss D: -0.0002\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0687, Loss D: 0.0070\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0634, Loss D: 0.0056\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:54<00:49, 16.51s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0677, Loss D: 0.0024\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0692, Loss D: 0.0024\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0648, Loss D: -0.0008\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0664, Loss D: 0.0021\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0681, Loss D: 0.0083\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0641, Loss D: 0.0018\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0642, Loss D: 0.0105\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:11<00:33, 16.60s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0385, Loss D: -0.0055\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0362, Loss D: -0.0036\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0349, Loss D: -0.0015\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0382, Loss D: 0.0008\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0365, Loss D: -0.0011\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0386, Loss D: 0.0035\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0362, Loss D: -0.0008\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:26<00:16, 16.27s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0240, Loss D: -0.0003\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0272, Loss D: -0.0049\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0292, Loss D: -0.0037\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0306, Loss D: -0.0015\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0246, Loss D: -0.0051\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0282, Loss D: -0.0013\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0268, Loss D: -0.0072\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:42<00:00, 16.28s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 162.93203592300415\n",
      "Metrics: \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        73  DP-CTGAN (eps=1)  Groundhog       0.7   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.4                  0.0            0.4           0.6  0.92   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        73  DP-CTGAN (eps=1)  Groundhog       0.7   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.4                  0.0            0.4           0.6  0.92   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0415, Loss D: 0.0036\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0380, Loss D: -0.0052\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0343, Loss D: 0.0020\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0324, Loss D: 0.0053\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0335, Loss D: -0.0009\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0355, Loss D: 0.0027\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0349, Loss D: 0.0013\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:15<02:21, 15.77s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0520, Loss D: -0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0491, Loss D: 0.0030\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0491, Loss D: 0.0078\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0449, Loss D: 0.0053\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0463, Loss D: 0.0036\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0440, Loss D: 0.0017\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0499, Loss D: 0.0070\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:31<02:06, 15.80s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0197, Loss D: 0.0005\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0257, Loss D: -0.0027\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0273, Loss D: 0.0007\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0189, Loss D: -0.0014\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0257, Loss D: 0.0059\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0202, Loss D: -0.0013\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0249, Loss D: 0.0014\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:47<01:50, 15.73s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0359, Loss D: -0.0046\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0347, Loss D: -0.0005\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0330, Loss D: -0.0070\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0382, Loss D: 0.0037\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0332, Loss D: -0.0008\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0376, Loss D: 0.0102\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0351, Loss D: -0.0079\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:03<01:36, 16.06s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0759, Loss D: 0.0015\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0794, Loss D: -0.0056\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0787, Loss D: -0.0059\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0751, Loss D: 0.0023\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0818, Loss D: -0.0003\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0786, Loss D: -0.0005\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0847, Loss D: 0.0045\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:19<01:19, 16.00s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0776, Loss D: -0.0049\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0751, Loss D: -0.0043\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0801, Loss D: -0.0004\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0783, Loss D: -0.0023\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0760, Loss D: 0.0038\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0728, Loss D: -0.0017\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0779, Loss D: 0.0013\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:36<01:04, 16.11s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0361, Loss D: -0.0011\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0404, Loss D: 0.0011\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0386, Loss D: 0.0067\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0384, Loss D: -0.0056\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0366, Loss D: 0.0057\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0337, Loss D: 0.0051\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0380, Loss D: -0.0016\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:51<00:47, 15.85s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0963, Loss D: 0.0005\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0978, Loss D: 0.0008\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0908, Loss D: -0.0055\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0961, Loss D: 0.0037\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0999, Loss D: 0.0064\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.1004, Loss D: 0.0042\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0950, Loss D: 0.0076\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:07<00:31, 15.98s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0130, Loss D: 0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0103, Loss D: -0.0009\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0085, Loss D: -0.0020\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0135, Loss D: 0.0006\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0156, Loss D: 0.0070\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0141, Loss D: 0.0015\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0146, Loss D: 0.0030\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:23<00:16, 16.10s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0368, Loss D: -0.0016\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0331, Loss D: -0.0018\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0359, Loss D: 0.0003\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0365, Loss D: -0.0026\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0339, Loss D: 0.0031\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0356, Loss D: 0.0075\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0360, Loss D: 0.0049\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:39<00:00, 15.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 159.61027812957764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0374, Loss D: 0.0008\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0388, Loss D: -0.0049\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0375, Loss D: -0.0070\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0368, Loss D: -0.0019\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0360, Loss D: 0.0033\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0367, Loss D: -0.0025\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0362, Loss D: 0.0012\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:15<02:23, 15.93s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0654, Loss D: 0.0017\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0732, Loss D: 0.0005\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0697, Loss D: 0.0089\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0693, Loss D: 0.0006\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0714, Loss D: 0.0042\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0719, Loss D: 0.0027\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0670, Loss D: 0.0005\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:31<02:03, 15.45s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0349, Loss D: 0.0008\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0375, Loss D: 0.0039\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0323, Loss D: -0.0025\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0342, Loss D: 0.0088\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0388, Loss D: 0.0134\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0387, Loss D: 0.0006\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0399, Loss D: -0.0031\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:47<01:51, 15.91s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0552, Loss D: -0.0033\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0568, Loss D: 0.0040\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0541, Loss D: 0.0016\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0537, Loss D: -0.0000\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0498, Loss D: -0.0015\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0520, Loss D: -0.0019\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0536, Loss D: 0.0050\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:05<01:40, 16.67s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0413, Loss D: 0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0365, Loss D: 0.0027\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0391, Loss D: 0.0018\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0363, Loss D: 0.0031\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0371, Loss D: 0.0021\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0392, Loss D: 0.0093\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0405, Loss D: -0.0012\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:21<01:22, 16.51s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0546, Loss D: -0.0005\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0532, Loss D: 0.0010\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0577, Loss D: 0.0006\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0534, Loss D: 0.0049\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0551, Loss D: 0.0016\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0552, Loss D: -0.0030\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0531, Loss D: 0.0070\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:37<01:05, 16.27s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0334, Loss D: -0.0025\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0339, Loss D: 0.0026\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0317, Loss D: -0.0057\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0379, Loss D: -0.0014\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0376, Loss D: -0.0011\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0335, Loss D: -0.0052\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0363, Loss D: 0.0025\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:54<00:49, 16.51s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0306, Loss D: -0.0039\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0308, Loss D: -0.0032\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0316, Loss D: 0.0010\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0323, Loss D: 0.0018\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0369, Loss D: -0.0002\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0377, Loss D: -0.0065\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0360, Loss D: 0.0062\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:11<00:33, 16.57s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0369, Loss D: 0.0065\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0337, Loss D: 0.0016\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0343, Loss D: -0.0006\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0396, Loss D: 0.0040\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0344, Loss D: 0.0026\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0356, Loss D: -0.0026\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0353, Loss D: 0.0028\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:29<00:17, 17.11s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0392, Loss D: -0.0029\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0378, Loss D: 0.0013\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0381, Loss D: -0.0001\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0384, Loss D: 0.0060\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0388, Loss D: 0.0021\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0418, Loss D: 0.0066\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0378, Loss D: 0.0015\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:48<00:00, 16.86s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 168.6602189540863\n",
      "Metrics: \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        57  DP-CTGAN (eps=1)  Groundhog       0.4   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.2                  0.4           -0.2           1.2  0.34   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                0.0  \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer        57  DP-CTGAN (eps=1)  Groundhog       0.4   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.2                  0.4           -0.2           1.2  0.34   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                0.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0301, Loss D: -0.0044\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0355, Loss D: -0.0015\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0337, Loss D: -0.0034\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0309, Loss D: -0.0018\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0299, Loss D: -0.0068\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0337, Loss D: -0.0020\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0335, Loss D: 0.0076\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:16<02:28, 16.55s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0446, Loss D: -0.0057\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0471, Loss D: 0.0053\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0504, Loss D: 0.0078\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0437, Loss D: 0.0004\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0487, Loss D: 0.0031\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0505, Loss D: 0.0027\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0475, Loss D: 0.0001\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:32<02:11, 16.44s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0715, Loss D: 0.0025\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0683, Loss D: 0.0015\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0701, Loss D: -0.0005\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0727, Loss D: -0.0024\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0694, Loss D: -0.0010\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0683, Loss D: 0.0062\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0706, Loss D: 0.0075\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:48<01:53, 16.27s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0484, Loss D: -0.0023\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0520, Loss D: -0.0074\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0541, Loss D: 0.0055\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0549, Loss D: -0.0026\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0527, Loss D: -0.0025\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0565, Loss D: -0.0062\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0559, Loss D: 0.0062\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:05<01:38, 16.34s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0363, Loss D: 0.0008\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0392, Loss D: 0.0015\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0429, Loss D: 0.0029\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0412, Loss D: 0.0048\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0393, Loss D: -0.0016\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0446, Loss D: -0.0033\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0407, Loss D: 0.0008\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:21<01:22, 16.41s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0274, Loss D: 0.0014\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0287, Loss D: 0.0010\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0298, Loss D: 0.0026\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0260, Loss D: -0.0063\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0344, Loss D: 0.0026\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0260, Loss D: 0.0029\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0308, Loss D: -0.0042\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:38<01:05, 16.42s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0076, Loss D: -0.0015\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0075, Loss D: -0.0046\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0077, Loss D: -0.0035\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0090, Loss D: 0.0029\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0098, Loss D: 0.0017\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0026, Loss D: 0.0000\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0130, Loss D: 0.0009\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:54<00:49, 16.42s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0365, Loss D: 0.0008\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0331, Loss D: -0.0067\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0344, Loss D: -0.0045\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0383, Loss D: 0.0058\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0408, Loss D: 0.0015\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0355, Loss D: 0.0035\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0346, Loss D: -0.0006\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:11<00:32, 16.43s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0245, Loss D: -0.0020\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0231, Loss D: -0.0008\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0258, Loss D: 0.0004\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0190, Loss D: -0.0025\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0241, Loss D: -0.0024\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0211, Loss D: 0.0016\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0219, Loss D: -0.0017\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:27<00:16, 16.35s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0524, Loss D: -0.0077\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0476, Loss D: -0.0001\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0489, Loss D: 0.0048\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0484, Loss D: -0.0016\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0466, Loss D: -0.0028\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0469, Loss D: -0.0003\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0445, Loss D: 0.0016\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:44<00:00, 16.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 164.41669487953186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0351, Loss D: -0.0015\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0389, Loss D: 0.0002\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0359, Loss D: -0.0016\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0365, Loss D: 0.0088\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0359, Loss D: 0.0006\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0401, Loss D: -0.0016\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0354, Loss D: -0.0010\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:16<02:29, 16.61s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0513, Loss D: -0.0013\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0534, Loss D: 0.0060\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0528, Loss D: 0.0016\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0558, Loss D: -0.0071\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0499, Loss D: 0.0060\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0536, Loss D: 0.0013\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0537, Loss D: 0.0011\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:33<02:12, 16.62s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0164, Loss D: 0.0048\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0086, Loss D: 0.0054\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0150, Loss D: -0.0021\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0172, Loss D: 0.0120\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0165, Loss D: 0.0010\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0104, Loss D: 0.0032\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0142, Loss D: 0.0092\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:49<01:56, 16.62s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0320, Loss D: 0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0295, Loss D: 0.0034\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0340, Loss D: 0.0008\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0311, Loss D: 0.0010\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0381, Loss D: 0.0042\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0290, Loss D: 0.0026\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0322, Loss D: 0.0028\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:06<01:39, 16.59s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0345, Loss D: 0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0340, Loss D: 0.0014\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0330, Loss D: 0.0032\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0348, Loss D: -0.0040\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0313, Loss D: 0.0014\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0366, Loss D: 0.0004\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0368, Loss D: 0.0031\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:22<01:22, 16.58s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0434, Loss D: -0.0030\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0504, Loss D: 0.0002\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0524, Loss D: 0.0034\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0512, Loss D: 0.0010\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0494, Loss D: 0.0029\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0472, Loss D: 0.0021\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0465, Loss D: 0.0062\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:39<01:06, 16.58s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0657, Loss D: 0.0005\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0688, Loss D: 0.0041\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0599, Loss D: -0.0014\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0691, Loss D: -0.0063\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0623, Loss D: 0.0052\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0642, Loss D: 0.0030\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0617, Loss D: -0.0042\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:56<00:50, 16.72s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0510, Loss D: -0.0059\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0511, Loss D: 0.0021\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0516, Loss D: -0.0017\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0469, Loss D: -0.0008\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0473, Loss D: 0.0045\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0453, Loss D: -0.0018\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0453, Loss D: 0.0005\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:13<00:33, 16.67s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0374, Loss D: -0.0023\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0376, Loss D: -0.0012\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0372, Loss D: -0.0038\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0405, Loss D: 0.0017\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0401, Loss D: 0.0025\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0390, Loss D: 0.0024\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0404, Loss D: 0.0020\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:29<00:16, 16.60s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0407, Loss D: 0.0023\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0351, Loss D: 0.0014\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0393, Loss D: -0.0021\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0372, Loss D: 0.0028\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0357, Loss D: -0.0000\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0421, Loss D: 0.0023\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0378, Loss D: 0.0025\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:45<00:00, 16.60s/it]\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 166.0838611125946\n",
      "Metrics: \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer       491  DP-CTGAN (eps=1)  Groundhog       0.6   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 1.0                  0.8            0.2           0.8  0.78   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer       491  DP-CTGAN (eps=1)  Groundhog       0.6   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 1.0                  0.8            0.2           0.8  0.78   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0401, Loss D: -0.0031\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0453, Loss D: -0.0002\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0477, Loss D: -0.0071\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0369, Loss D: 0.0045\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0443, Loss D: -0.0050\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0413, Loss D: 0.0097\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0454, Loss D: 0.0024\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:16<02:30, 16.76s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0183, Loss D: -0.0041\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0186, Loss D: 0.0081\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0224, Loss D: 0.0012\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0197, Loss D: 0.0006\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0178, Loss D: -0.0016\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0150, Loss D: 0.0047\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0150, Loss D: 0.0034\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:33<02:14, 16.78s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0198, Loss D: 0.0043\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0174, Loss D: -0.0031\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0130, Loss D: 0.0048\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0123, Loss D: -0.0014\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0150, Loss D: 0.0049\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0122, Loss D: -0.0038\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0134, Loss D: -0.0018\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:50<01:56, 16.71s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0137, Loss D: 0.0018\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0166, Loss D: 0.0003\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0105, Loss D: 0.0026\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0181, Loss D: -0.0034\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0139, Loss D: -0.0013\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0111, Loss D: -0.0049\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0152, Loss D: 0.0022\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:06<01:39, 16.66s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0537, Loss D: -0.0023\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0559, Loss D: -0.0021\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0549, Loss D: 0.0050\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0597, Loss D: 0.0064\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0501, Loss D: -0.0034\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0511, Loss D: -0.0039\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0550, Loss D: 0.0086\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:24<01:24, 16.99s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0404, Loss D: 0.0029\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0447, Loss D: 0.0002\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0420, Loss D: 0.0024\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0408, Loss D: 0.0001\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0425, Loss D: -0.0011\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0436, Loss D: 0.0063\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0413, Loss D: -0.0009\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:41<01:08, 17.06s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0371, Loss D: 0.0024\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0331, Loss D: 0.0039\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0383, Loss D: 0.0071\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0328, Loss D: 0.0036\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0271, Loss D: -0.0010\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0377, Loss D: -0.0035\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0348, Loss D: 0.0027\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:59<00:51, 17.30s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0424, Loss D: 0.0022\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0452, Loss D: -0.0013\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0426, Loss D: 0.0058\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0419, Loss D: 0.0013\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0464, Loss D: 0.0040\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0452, Loss D: -0.0010\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0457, Loss D: 0.0086\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:18<00:35, 17.76s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0342, Loss D: 0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0349, Loss D: 0.0019\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0349, Loss D: -0.0006\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0375, Loss D: -0.0004\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0355, Loss D: 0.0019\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0338, Loss D: 0.0041\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0323, Loss D: 0.0052\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:35<00:17, 17.63s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0470, Loss D: 0.0008\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0466, Loss D: 0.0005\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0475, Loss D: -0.0023\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0479, Loss D: 0.0062\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0509, Loss D: 0.0069\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0460, Loss D: 0.0079\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0467, Loss D: -0.0069\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:52<00:00, 17.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time delta for the attacker: 172.77852964401245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                           | 0/10 [00:00<?, ?it/s]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0415, Loss D: 0.0027\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0452, Loss D: -0.0028\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0425, Loss D: -0.0018\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0424, Loss D: -0.0004\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0419, Loss D: -0.0070\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0391, Loss D: 0.0092\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0461, Loss D: 0.0027\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████▉                                                                                         | 1/10 [00:16<02:26, 16.24s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0554, Loss D: 0.0086\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0550, Loss D: -0.0005\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0534, Loss D: 0.0004\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0539, Loss D: -0.0002\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0519, Loss D: 0.0024\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0533, Loss D: 0.0043\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0532, Loss D: -0.0037\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████▊                                                                               | 2/10 [00:32<02:11, 16.48s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0666, Loss D: 0.0021\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0648, Loss D: -0.0001\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0640, Loss D: 0.0048\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0618, Loss D: 0.0046\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0634, Loss D: 0.0016\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0621, Loss D: 0.0073\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0609, Loss D: 0.0056\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████▋                                                                     | 3/10 [00:49<01:55, 16.47s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0485, Loss D: -0.0005\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0494, Loss D: 0.0010\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0492, Loss D: -0.0042\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0463, Loss D: -0.0030\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0468, Loss D: 0.0077\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0479, Loss D: -0.0033\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0455, Loss D: 0.0007\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████████████▌                                                           | 4/10 [01:07<01:43, 17.27s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0483, Loss D: 0.0017\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0437, Loss D: 0.0015\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0487, Loss D: -0.0013\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0454, Loss D: -0.0022\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0440, Loss D: 0.0016\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0470, Loss D: -0.0040\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0477, Loss D: -0.0008\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████████████▌                                                 | 5/10 [01:24<01:25, 17.06s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0678, Loss D: 0.0044\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0668, Loss D: 0.0040\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0654, Loss D: 0.0005\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0641, Loss D: -0.0008\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0607, Loss D: 0.0055\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0625, Loss D: 0.0044\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0674, Loss D: -0.0013\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████▍                                       | 6/10 [01:40<01:07, 16.77s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0336, Loss D: -0.0044\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0365, Loss D: -0.0077\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0356, Loss D: 0.0035\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0353, Loss D: 0.0038\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0328, Loss D: -0.0001\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0361, Loss D: -0.0044\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0322, Loss D: 0.0078\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████▎                             | 7/10 [01:57<00:50, 16.75s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: 0.0504, Loss D: -0.0015\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: 0.0511, Loss D: -0.0010\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: 0.0472, Loss D: -0.0009\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: 0.0500, Loss D: 0.0121\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: 0.0492, Loss D: 0.0049\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: 0.0494, Loss D: -0.0023\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: 0.0469, Loss D: -0.0047\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████▏                   | 8/10 [02:13<00:33, 16.61s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0074, Loss D: -0.0007\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0104, Loss D: 0.0020\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0155, Loss D: -0.0005\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0117, Loss D: -0.0026\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0087, Loss D: 0.0108\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0141, Loss D: -0.0016\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0156, Loss D: 0.0051\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████          | 9/10 [02:29<00:16, 16.38s/it]/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss G: -0.0517, Loss D: 0.0004\n",
      "epsilon is 0.028590312727600813, alpha is 63.0\n",
      "Epoch 2, Loss G: -0.0525, Loss D: 0.0008\n",
      "epsilon is 0.3508497978739808, alpha is 19.0\n",
      "Epoch 3, Loss G: -0.0492, Loss D: 0.0015\n",
      "epsilon is 0.5118818491933068, alpha is 15.0\n",
      "Epoch 4, Loss G: -0.0512, Loss D: 0.0039\n",
      "epsilon is 0.6401869495928957, alpha is 13.0\n",
      "Epoch 5, Loss G: -0.0493, Loss D: 0.0040\n",
      "epsilon is 0.7519399509030614, alpha is 12.0\n",
      "Epoch 6, Loss G: -0.0525, Loss D: -0.0005\n",
      "epsilon is 0.8505949589561193, alpha is 10.6\n",
      "Epoch 7, Loss G: -0.0535, Loss D: -0.0019\n",
      "epsilon is 0.9421075032742222, alpha is 9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [02:45<00:00, 16.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing time delta for the attacker: 165.61830806732178\n",
      "Metrics: \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer       550  DP-CTGAN (eps=1)  Groundhog       0.4   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.2                  0.4           -0.2           1.2  0.42   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n",
      "         dataset target_id         generator     attack  accuracy  \\\n",
      "0  Breast Cancer       550  DP-CTGAN (eps=1)  Groundhog       0.4   \n",
      "\n",
      "   true_positive_rate  false_positive_rate  mia_advantage  privacy_gain   auc  \\\n",
      "0                 0.2                  0.4           -0.2           1.2  0.42   \n",
      "\n",
      "   effective_epsilon  \n",
      "0                inf  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Volumes/Data_Repos/Desktop Folders/UCLA/Thesis/privacy-sdg-toolbox/tabular-synthetic-data-privacy-auditing-main/tapas/report/attack_summary.py:250: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.log(max(np.max(tp/fp), np.max((1-fp)/(1-tp))))\n"
     ]
    }
   ],
   "source": [
    "# Calling the main function and printing all metrics and summaries from training and testing the models\n",
    "all_metrics, all_summaries = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "974201a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          dataset target_id         generator     attack  accuracy  \\\n",
      "0   Breast Cancer        84               Raw  Groundhog       0.6   \n",
      "1   Breast Cancer        70               Raw  Groundhog       0.6   \n",
      "2   Breast Cancer        70               Raw  Groundhog       0.5   \n",
      "3   Breast Cancer        90               Raw  Groundhog       0.7   \n",
      "4   Breast Cancer        66               Raw  Groundhog       0.5   \n",
      "5   Breast Cancer       491               Raw  Groundhog       0.5   \n",
      "6   Breast Cancer        73               Raw  Groundhog       0.7   \n",
      "7   Breast Cancer        57               Raw  Groundhog       0.7   \n",
      "8   Breast Cancer       491               Raw  Groundhog       0.9   \n",
      "9   Breast Cancer       550               Raw  Groundhog       0.6   \n",
      "10  Breast Cancer        84             CTGAN  Groundhog       1.0   \n",
      "11  Breast Cancer        70             CTGAN  Groundhog       1.0   \n",
      "12  Breast Cancer        70             CTGAN  Groundhog       0.9   \n",
      "13  Breast Cancer        90             CTGAN  Groundhog       1.0   \n",
      "14  Breast Cancer        66             CTGAN  Groundhog       1.0   \n",
      "15  Breast Cancer       491             CTGAN  Groundhog       1.0   \n",
      "16  Breast Cancer        73             CTGAN  Groundhog       0.8   \n",
      "17  Breast Cancer        57             CTGAN  Groundhog       1.0   \n",
      "18  Breast Cancer       491             CTGAN  Groundhog       1.0   \n",
      "19  Breast Cancer       550             CTGAN  Groundhog       1.0   \n",
      "20  Breast Cancer        84  DP-CTGAN (eps=1)  Groundhog       0.7   \n",
      "21  Breast Cancer        70  DP-CTGAN (eps=1)  Groundhog       0.6   \n",
      "22  Breast Cancer        70  DP-CTGAN (eps=1)  Groundhog       0.5   \n",
      "23  Breast Cancer        90  DP-CTGAN (eps=1)  Groundhog       0.5   \n",
      "24  Breast Cancer        66  DP-CTGAN (eps=1)  Groundhog       0.6   \n",
      "25  Breast Cancer       491  DP-CTGAN (eps=1)  Groundhog       0.9   \n",
      "26  Breast Cancer        73  DP-CTGAN (eps=1)  Groundhog       0.7   \n",
      "27  Breast Cancer        57  DP-CTGAN (eps=1)  Groundhog       0.4   \n",
      "28  Breast Cancer       491  DP-CTGAN (eps=1)  Groundhog       0.6   \n",
      "29  Breast Cancer       550  DP-CTGAN (eps=1)  Groundhog       0.4   \n",
      "\n",
      "    true_positive_rate  false_positive_rate  mia_advantage  privacy_gain  \\\n",
      "0                  0.6                  0.4            0.2           0.8   \n",
      "1                  0.6                  0.4            0.2           0.8   \n",
      "2                  0.0                  0.0            0.0           1.0   \n",
      "3                  1.0                  0.6            0.4           0.6   \n",
      "4                  0.2                  0.2            0.0           1.0   \n",
      "5                  1.0                  1.0            0.0           1.0   \n",
      "6                  1.0                  0.6            0.4           0.6   \n",
      "7                  1.0                  0.6            0.4           0.6   \n",
      "8                  0.8                  0.0            0.8           0.2   \n",
      "9                  1.0                  0.8            0.2           0.8   \n",
      "10                 1.0                  0.0            1.0           0.0   \n",
      "11                 1.0                  0.0            1.0           0.0   \n",
      "12                 1.0                  0.2            0.8           0.2   \n",
      "13                 1.0                  0.0            1.0           0.0   \n",
      "14                 1.0                  0.0            1.0           0.0   \n",
      "15                 1.0                  0.0            1.0           0.0   \n",
      "16                 1.0                  0.4            0.6           0.4   \n",
      "17                 1.0                  0.0            1.0           0.0   \n",
      "18                 1.0                  0.0            1.0           0.0   \n",
      "19                 1.0                  0.0            1.0           0.0   \n",
      "20                 0.6                  0.2            0.4           0.6   \n",
      "21                 0.6                  0.4            0.2           0.8   \n",
      "22                 0.4                  0.4            0.0           1.0   \n",
      "23                 1.0                  1.0            0.0           1.0   \n",
      "24                 1.0                  0.8            0.2           0.8   \n",
      "25                 1.0                  0.2            0.8           0.2   \n",
      "26                 0.4                  0.0            0.4           0.6   \n",
      "27                 0.2                  0.4           -0.2           1.2   \n",
      "28                 1.0                  0.8            0.2           0.8   \n",
      "29                 0.2                  0.4           -0.2           1.2   \n",
      "\n",
      "     auc  effective_epsilon  \n",
      "0   0.72                inf  \n",
      "1   0.68                inf  \n",
      "2   0.60                inf  \n",
      "3   0.76                inf  \n",
      "4   0.66           1.386294  \n",
      "5   0.92                inf  \n",
      "6   0.88                inf  \n",
      "7   0.86           1.609438  \n",
      "8   0.94                inf  \n",
      "9   0.84                inf  \n",
      "10  1.00                inf  \n",
      "11  1.00                inf  \n",
      "12  1.00                inf  \n",
      "13  1.00                inf  \n",
      "14  1.00                inf  \n",
      "15  1.00                inf  \n",
      "16  1.00                inf  \n",
      "17  1.00                inf  \n",
      "18  1.00                inf  \n",
      "19  1.00                inf  \n",
      "20  0.74                inf  \n",
      "21  0.78                inf  \n",
      "22  0.58                inf  \n",
      "23  0.56           0.287682  \n",
      "24  0.80                inf  \n",
      "25  0.80                inf  \n",
      "26  0.92                inf  \n",
      "27  0.34           0.000000  \n",
      "28  0.78                inf  \n",
      "29  0.42                inf  \n",
      "[<tapas.report.attack_summary.MIAttackSummary object at 0x157c7eeb0>, <tapas.report.attack_summary.MIAttackSummary object at 0x158db7dc0>, <tapas.report.attack_summary.MIAttackSummary object at 0x157fec7f0>, <tapas.report.attack_summary.MIAttackSummary object at 0x158dbd430>, <tapas.report.attack_summary.MIAttackSummary object at 0x158c26f40>, <tapas.report.attack_summary.MIAttackSummary object at 0x158d9dfd0>, <tapas.report.attack_summary.MIAttackSummary object at 0x158c09b20>, <tapas.report.attack_summary.MIAttackSummary object at 0x158da41c0>, <tapas.report.attack_summary.MIAttackSummary object at 0x158c09760>, <tapas.report.attack_summary.MIAttackSummary object at 0x158d37fa0>, <tapas.report.attack_summary.MIAttackSummary object at 0x15e7c1520>, <tapas.report.attack_summary.MIAttackSummary object at 0x15e7c8790>, <tapas.report.attack_summary.MIAttackSummary object at 0x15e5bfcd0>, <tapas.report.attack_summary.MIAttackSummary object at 0x158d8e250>, <tapas.report.attack_summary.MIAttackSummary object at 0x15e19e4c0>, <tapas.report.attack_summary.MIAttackSummary object at 0x15e5b4460>, <tapas.report.attack_summary.MIAttackSummary object at 0x15e4213d0>, <tapas.report.attack_summary.MIAttackSummary object at 0x158d1a2b0>, <tapas.report.attack_summary.MIAttackSummary object at 0x158d1a070>, <tapas.report.attack_summary.MIAttackSummary object at 0x15e7ed580>, <tapas.report.attack_summary.MIAttackSummary object at 0x158ba03a0>, <tapas.report.attack_summary.MIAttackSummary object at 0x176892760>, <tapas.report.attack_summary.MIAttackSummary object at 0x15e925220>, <tapas.report.attack_summary.MIAttackSummary object at 0x17686b850>, <tapas.report.attack_summary.MIAttackSummary object at 0x176c4c640>, <tapas.report.attack_summary.MIAttackSummary object at 0x176c4c0a0>, <tapas.report.attack_summary.MIAttackSummary object at 0x176dc1fa0>, <tapas.report.attack_summary.MIAttackSummary object at 0x176d2eaf0>, <tapas.report.attack_summary.MIAttackSummary object at 0x15e6e44f0>, <tapas.report.attack_summary.MIAttackSummary object at 0x176c29a00>]\n"
     ]
    }
   ],
   "source": [
    "# Printing all the collected metrics and summaries \n",
    "print(all_metrics)\n",
    "print(all_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0431890",
   "metadata": {},
   "source": [
    "## Results and Plots\n",
    "\n",
    "### Description\n",
    "Contains utilities to generate results and produce plots/reports to compare generators of random targets and compare attacks for different target record types of random and outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27694802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates and saves the report of comparison of generators on random targets\n",
    "def generateReport_ComparisonOfGeneratorsOnRandomTargets():\n",
    "    num_attacks = all_metrics.shape[0]\n",
    "    random_indices = [num for i in range(0, num_attacks, num_targets*2) for num in range(i, i+num_targets)]\n",
    "    report = tapas.report.BinaryLabelAttackReport(all_metrics.iloc[random_indices])\n",
    "    report.metrics = [\"privacy_gain\", \"auc\", \"effective_epsilon\"]\n",
    "    report.compare(comparison_column='generator', fixed_pair_columns=['attack', 'dataset'], marker_column='target_id', filepath=\"../figures_cancer_synthetic_Dec24/cancer_compare_generators\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c581376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krishnadave/Library/Caches/pypoetry/virtualenvs/tapas-T4rVcRt4-py3.9/lib/python3.9/site-packages/numpy/lib/function_base.py:4527: RuntimeWarning: invalid value encountered in subtract\n",
      "  diff_b_a = subtract(b, a)\n"
     ]
    }
   ],
   "source": [
    "generateReport_ComparisonOfGeneratorsOnRandomTargets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d008c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates report that compares attack for different target record types of random and outlier\n",
    "def generateReport_CompareAttacksForDifferentTargetRecordTypes():\n",
    "    len_generators = 3\n",
    "    all_metrics['target_type'] = (['Random']*num_targets + ['Outlier']*num_targets) * len_generators\n",
    "    report = tapas.report.BinaryLabelAttackReport(all_metrics)\n",
    "    report.metrics = [\"privacy_gain\", \"auc\"]\n",
    "    report.compare(comparison_column='generator', fixed_pair_columns=['attack', 'dataset'], marker_column='target_type', filepath=\"../figures_cancer_synthetic_Dec24/Cancer_random_versus_outlier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f945e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "generateReport_CompareAttacksForDifferentTargetRecordTypes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
